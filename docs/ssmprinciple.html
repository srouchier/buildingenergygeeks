<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 10 Principle of SSMs | Building energy statistical modelling</title>
  <meta name="description" content="Handbook of statistical learning for building energy performance." />
  <meta name="generator" content="bookdown 0.26 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 10 Principle of SSMs | Building energy statistical modelling" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Handbook of statistical learning for building energy performance." />
  <meta name="github-repo" content="srouchier/buildingenergygeeks" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 10 Principle of SSMs | Building energy statistical modelling" />
  
  <meta name="twitter:description" content="Handbook of statistical learning for building energy performance." />
  

<meta name="author" content="Simon Rouchier" />


<meta name="date" content="2022-07-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="composite-time-series-models.html"/>
<link rel="next" href="a-simple-rc-model-python.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./index.html">Building energy statistical modelling</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Home page</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#motivation"><i class="fa fa-check"></i>Motivation</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#content-of-the-book"><i class="fa fa-check"></i>Content of the book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#programming-languages"><i class="fa fa-check"></i>Programming languages</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about"><i class="fa fa-check"></i>About</a></li>
</ul></li>
<li class="part"><span><b>I Theory and workflow</b></span></li>
<li class="chapter" data-level="1" data-path="scope.html"><a href="scope.html"><i class="fa fa-check"></i><b>1</b> Background on data analysis</a>
<ul>
<li class="chapter" data-level="1.1" data-path="scope.html"><a href="scope.html#the-energy-savings-potential-of-buildings"><i class="fa fa-check"></i><b>1.1</b> The energy savings potential of buildings</a></li>
<li class="chapter" data-level="1.2" data-path="scope.html"><a href="scope.html#from-data-to-energy-savings"><i class="fa fa-check"></i><b>1.2</b> From data to energy savings</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="scope.html"><a href="scope.html#formalisation-of-the-system"><i class="fa fa-check"></i><b>1.2.1</b> Formalisation of the system</a></li>
<li class="chapter" data-level="1.2.2" data-path="scope.html"><a href="scope.html#some-uses-of-data"><i class="fa fa-check"></i><b>1.2.2</b> Some uses of data</a></li>
<li class="chapter" data-level="1.2.3" data-path="scope.html"><a href="scope.html#model-calibration-as-the-key-to-data-analysis"><i class="fa fa-check"></i><b>1.2.3</b> Model calibration as the key to data analysis</a></li>
<li class="chapter" data-level="1.2.4" data-path="scope.html"><a href="scope.html#inverseproblems"><i class="fa fa-check"></i><b>1.2.4</b> The difficulty of inverse problems</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="scope.html"><a href="scope.html#categories"><i class="fa fa-check"></i><b>1.3</b> Categories of data-driven modelling approaches</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="scope.html"><a href="scope.html#either-physical-interpretability-or-prediction-accuracy"><i class="fa fa-check"></i><b>1.3.1</b> Either physical interpretability or prediction accuracy</a></li>
<li class="chapter" data-level="1.3.2" data-path="scope.html"><a href="scope.html#calibrated-simulation-white-box"><i class="fa fa-check"></i><b>1.3.2</b> Calibrated simulation (white-box)</a></li>
<li class="chapter" data-level="1.3.3" data-path="scope.html"><a href="scope.html#machine-learning-black-box"><i class="fa fa-check"></i><b>1.3.3</b> Machine learning (black-box)</a></li>
<li class="chapter" data-level="1.3.4" data-path="scope.html"><a href="scope.html#statistical-modelling-and-inference-grey-box"><i class="fa fa-check"></i><b>1.3.4</b> Statistical modelling and inference (grey-box)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="modelling.html"><a href="modelling.html"><i class="fa fa-check"></i><b>2</b> Building energy statistical modelling</a>
<ul>
<li class="chapter" data-level="2.1" data-path="modelling.html"><a href="modelling.html#modelling1"><i class="fa fa-check"></i><b>2.1</b> Building physics in a nutshell</a></li>
<li class="chapter" data-level="2.2" data-path="modelling.html"><a href="modelling.html#modelling2"><i class="fa fa-check"></i><b>2.2</b> Measurement and modelling boundaries</a></li>
<li class="chapter" data-level="2.3" data-path="modelling.html"><a href="modelling.html#modelling3"><i class="fa fa-check"></i><b>2.3</b> Categories of statistical models</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="workflow.html"><a href="workflow.html"><i class="fa fa-check"></i><b>3</b> A Bayesian data analysis workflow</a>
<ul>
<li class="chapter" data-level="3.1" data-path="workflow.html"><a href="workflow.html#bayesian"><i class="fa fa-check"></i><b>3.1</b> Bayesian inference summarised</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="workflow.html"><a href="workflow.html#motivation-for-a-bayesian-approach"><i class="fa fa-check"></i><b>3.1.1</b> Motivation for a Bayesian approach</a></li>
<li class="chapter" data-level="3.1.2" data-path="workflow.html"><a href="workflow.html#general-bayesian-principles"><i class="fa fa-check"></i><b>3.1.2</b> General Bayesian principles</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="workflow.html"><a href="workflow.html#workflow-for-one-model"><i class="fa fa-check"></i><b>3.2</b> Workflow for one model</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="workflow.html"><a href="workflow.html#overview"><i class="fa fa-check"></i><b>3.2.1</b> Overview</a></li>
<li class="chapter" data-level="3.2.2" data-path="workflow.html"><a href="workflow.html#step-1-model-specification"><i class="fa fa-check"></i><b>3.2.2</b> Step 1: model specification</a></li>
<li class="chapter" data-level="3.2.3" data-path="workflow.html"><a href="workflow.html#priorpredictivechecking"><i class="fa fa-check"></i><b>3.2.3</b> Prior predictive checking</a></li>
<li class="chapter" data-level="3.2.4" data-path="workflow.html"><a href="workflow.html#computation"><i class="fa fa-check"></i><b>3.2.4</b> Step 2: computation with Markov Chain Monte Carlo</a></li>
<li class="chapter" data-level="3.2.5" data-path="workflow.html"><a href="workflow.html#modelvalidation"><i class="fa fa-check"></i><b>3.2.5</b> Step 3: model checking and validation</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="workflow.html"><a href="workflow.html#modelselection"><i class="fa fa-check"></i><b>3.3</b> Model assessment and selection</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="workflow.html"><a href="workflow.html#model-selection-workflows"><i class="fa fa-check"></i><b>3.3.1</b> Model selection workflows</a></li>
<li class="chapter" data-level="3.3.2" data-path="workflow.html"><a href="workflow.html#sensitivity-analysis"><i class="fa fa-check"></i><b>3.3.2</b> Sensitivity analysis</a></li>
<li class="chapter" data-level="3.3.3" data-path="workflow.html"><a href="workflow.html#structural-identifiability"><i class="fa fa-check"></i><b>3.3.3</b> Structural identifiability</a></li>
<li class="chapter" data-level="3.3.4" data-path="workflow.html"><a href="workflow.html#inferencediagnostics"><i class="fa fa-check"></i><b>3.3.4</b> Practical identifiability</a></li>
<li class="chapter" data-level="3.3.5" data-path="workflow.html"><a href="workflow.html#modelcomparison"><i class="fa fa-check"></i><b>3.3.5</b> Model comparison criteria</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Temporally independent data</b></span></li>
<li class="chapter" data-level="4" data-path="ordinary-linear-regression.html"><a href="ordinary-linear-regression.html"><i class="fa fa-check"></i><b>4</b> Ordinary linear regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="ordinary-linear-regression.html"><a href="ordinary-linear-regression.html#introduction-to-olr"><i class="fa fa-check"></i><b>4.1</b> Introduction to OLR</a></li>
<li class="chapter" data-level="4.2" data-path="ordinary-linear-regression.html"><a href="ordinary-linear-regression.html#tutorial-olr-with-r"><i class="fa fa-check"></i><b>4.2</b> Tutorial: OLR with R</a></li>
<li class="chapter" data-level="4.3" data-path="ordinary-linear-regression.html"><a href="ordinary-linear-regression.html#simple-linear-regression-with-r"><i class="fa fa-check"></i><b>4.3</b> Simple linear regression with R</a></li>
<li class="chapter" data-level="4.4" data-path="ordinary-linear-regression.html"><a href="ordinary-linear-regression.html#bayesian-regression-with-stan"><i class="fa fa-check"></i><b>4.4</b> Bayesian regression with Stan</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="bayesianmv.html"><a href="bayesianmv.html"><i class="fa fa-check"></i><b>5</b> Bayesian M&amp;V</a>
<ul>
<li class="chapter" data-level="5.1" data-path="bayesianmv.html"><a href="bayesianmv.html#a-bayesian-workflow-for-mv"><i class="fa fa-check"></i><b>5.1</b> A Bayesian workflow for M&amp;V</a></li>
<li class="chapter" data-level="5.2" data-path="bayesianmv.html"><a href="bayesianmv.html#change-point-models"><i class="fa fa-check"></i><b>5.2</b> Change-point models</a></li>
<li class="chapter" data-level="5.3" data-path="bayesianmv.html"><a href="bayesianmv.html#ipmvp-option-c-example-rstan"><i class="fa fa-check"></i><b>5.3</b> IPMVP option C example (Rstan)</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="bayesianmv.html"><a href="bayesianmv.html#loading-and-displaying-the-data"><i class="fa fa-check"></i><b>5.3.1</b> Loading and displaying the data</a></li>
<li class="chapter" data-level="5.3.2" data-path="bayesianmv.html"><a href="bayesianmv.html#daily-averaged-data"><i class="fa fa-check"></i><b>5.3.2</b> Daily averaged data</a></li>
<li class="chapter" data-level="5.3.3" data-path="bayesianmv.html"><a href="bayesianmv.html#model-definition"><i class="fa fa-check"></i><b>5.3.3</b> Model definition</a></li>
<li class="chapter" data-level="5.3.4" data-path="bayesianmv.html"><a href="bayesianmv.html#model-specification-with-stan"><i class="fa fa-check"></i><b>5.3.4</b> Model specification with Stan</a></li>
<li class="chapter" data-level="5.3.5" data-path="bayesianmv.html"><a href="bayesianmv.html#model-fitting"><i class="fa fa-check"></i><b>5.3.5</b> Model fitting</a></li>
<li class="chapter" data-level="5.3.6" data-path="bayesianmv.html"><a href="bayesianmv.html#validation-and-results"><i class="fa fa-check"></i><b>5.3.6</b> Validation and results</a></li>
<li class="chapter" data-level="5.3.7" data-path="bayesianmv.html"><a href="bayesianmv.html#residuals"><i class="fa fa-check"></i><b>5.3.7</b> Residuals</a></li>
<li class="chapter" data-level="5.3.8" data-path="bayesianmv.html"><a href="bayesianmv.html#savings"><i class="fa fa-check"></i><b>5.3.8</b> Savings</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="finite-mixture-models.html"><a href="finite-mixture-models.html"><i class="fa fa-check"></i><b>6</b> Finite mixture models</a>
<ul>
<li class="chapter" data-level="6.1" data-path="finite-mixture-models.html"><a href="finite-mixture-models.html#principle"><i class="fa fa-check"></i><b>6.1</b> Principle</a></li>
<li class="chapter" data-level="6.2" data-path="finite-mixture-models.html"><a href="finite-mixture-models.html#tutorial-rstan"><i class="fa fa-check"></i><b>6.2</b> Tutorial (Rstan)</a></li>
</ul></li>
<li class="part"><span><b>III Time-series modelling</b></span></li>
<li class="chapter" data-level="7" data-path="armax.html"><a href="armax.html"><i class="fa fa-check"></i><b>7</b> Autoregressive models</a>
<ul>
<li class="chapter" data-level="7.1" data-path="armax.html"><a href="armax.html#principle-of-armax-models"><i class="fa fa-check"></i><b>7.1</b> Principle of ARMAX models</a></li>
<li class="chapter" data-level="7.2" data-path="armax.html"><a href="armax.html#tutorial-rstan-1"><i class="fa fa-check"></i><b>7.2</b> Tutorial (Rstan)</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="armax.html"><a href="armax.html#data-the-ashrae-machine-learning-competition"><i class="fa fa-check"></i><b>7.2.1</b> Data: the ASHRAE machine learning competition</a></li>
<li class="chapter" data-level="7.2.2" data-path="armax.html"><a href="armax.html#a-simple-arx-model"><i class="fa fa-check"></i><b>7.2.2</b> A simple ARX model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="hmm.html"><a href="hmm.html"><i class="fa fa-check"></i><b>8</b> Hidden Markov models</a>
<ul>
<li class="chapter" data-level="8.1" data-path="hmm.html"><a href="hmm.html#principles"><i class="fa fa-check"></i><b>8.1</b> Principles</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="hmm.html"><a href="hmm.html#the-forward-algorithm"><i class="fa fa-check"></i><b>8.1.1</b> The forward algorithm</a></li>
<li class="chapter" data-level="8.1.2" data-path="hmm.html"><a href="hmm.html#the-viterbi-algorithm"><i class="fa fa-check"></i><b>8.1.2</b> The Viterbi algorithm</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="hmm.html"><a href="hmm.html#tutorial-python"><i class="fa fa-check"></i><b>8.2</b> Tutorial (Python)</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="composite-time-series-models.html"><a href="composite-time-series-models.html"><i class="fa fa-check"></i><b>9</b> Composite time series models</a>
<ul>
<li class="chapter" data-level="9.1" data-path="composite-time-series-models.html"><a href="composite-time-series-models.html#markov-switching-models"><i class="fa fa-check"></i><b>9.1</b> Markov switching models</a></li>
<li class="chapter" data-level="9.2" data-path="composite-time-series-models.html"><a href="composite-time-series-models.html#hidden-markov-energy-signature"><i class="fa fa-check"></i><b>9.2</b> Hidden Markov energy signature</a></li>
</ul></li>
<li class="part"><span><b>IV State-space models</b></span></li>
<li class="chapter" data-level="10" data-path="ssmprinciple.html"><a href="ssmprinciple.html"><i class="fa fa-check"></i><b>10</b> Principle of SSMs</a>
<ul>
<li class="chapter" data-level="10.1" data-path="ssmprinciple.html"><a href="ssmprinciple.html#description"><i class="fa fa-check"></i><b>10.1</b> Description</a></li>
<li class="chapter" data-level="10.2" data-path="ssmprinciple.html"><a href="ssmprinciple.html#linearssm"><i class="fa fa-check"></i><b>10.2</b> Linear state-space models</a></li>
<li class="chapter" data-level="10.3" data-path="ssmprinciple.html"><a href="ssmprinciple.html#kalmanfilter"><i class="fa fa-check"></i><b>10.3</b> The Kalman filter</a></li>
<li class="chapter" data-level="10.4" data-path="ssmprinciple.html"><a href="ssmprinciple.html#non-linear-state-space-models"><i class="fa fa-check"></i><b>10.4</b> Non-linear state-space models</a></li>
<li class="chapter" data-level="10.5" data-path="ssmprinciple.html"><a href="ssmprinciple.html#switching-state-space-models"><i class="fa fa-check"></i><b>10.5</b> Switching state-space models</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="a-simple-rc-model-python.html"><a href="a-simple-rc-model-python.html"><i class="fa fa-check"></i><b>11</b> A simple RC model (Python)</a>
<ul>
<li class="chapter" data-level="11.1" data-path="a-simple-rc-model-python.html"><a href="a-simple-rc-model-python.html#case-study"><i class="fa fa-check"></i><b>11.1</b> Case study</a></li>
<li class="chapter" data-level="11.2" data-path="a-simple-rc-model-python.html"><a href="a-simple-rc-model-python.html#modelling-1"><i class="fa fa-check"></i><b>11.2</b> Modelling</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="a-simple-rc-model-python.html"><a href="a-simple-rc-model-python.html#rc-model"><i class="fa fa-check"></i><b>11.2.1</b> RC model</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="a-simple-rc-model-python.html"><a href="a-simple-rc-model-python.html#deterministic-formulation"><i class="fa fa-check"></i><b>11.3</b> Deterministic formulation</a></li>
<li class="chapter" data-level="11.4" data-path="a-simple-rc-model-python.html"><a href="a-simple-rc-model-python.html#stochastic-formulation"><i class="fa fa-check"></i><b>11.4</b> Stochastic formulation</a>
<ul>
<li class="chapter" data-level="11.4.1" data-path="a-simple-rc-model-python.html"><a href="a-simple-rc-model-python.html#specification"><i class="fa fa-check"></i><b>11.4.1</b> Specification</a></li>
<li class="chapter" data-level="11.4.2" data-path="a-simple-rc-model-python.html"><a href="a-simple-rc-model-python.html#training"><i class="fa fa-check"></i><b>11.4.2</b> Training</a></li>
<li class="chapter" data-level="11.4.3" data-path="a-simple-rc-model-python.html"><a href="a-simple-rc-model-python.html#diagnostics-and-residuals-analysis"><i class="fa fa-check"></i><b>11.4.3</b> Diagnostics and residuals analysis</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="the-pysip-library-python.html"><a href="the-pysip-library-python.html"><i class="fa fa-check"></i><b>12</b> The pySIP library (Python)</a></li>
<li class="part"><span><b>V Gaussian Process models</b></span></li>
<li class="chapter" data-level="13" data-path="gaussian-process-models.html"><a href="gaussian-process-models.html"><i class="fa fa-check"></i><b>13</b> Gaussian Process models</a>
<ul>
<li class="chapter" data-level="13.1" data-path="gaussian-process-models.html"><a href="gaussian-process-models.html#principle-1"><i class="fa fa-check"></i><b>13.1</b> Principle</a></li>
<li class="chapter" data-level="13.2" data-path="gaussian-process-models.html"><a href="gaussian-process-models.html#gaussian-processes-for-prediction-of-energy-use"><i class="fa fa-check"></i><b>13.2</b> Gaussian Processes for prediction of energy use</a></li>
<li class="chapter" data-level="13.3" data-path="gaussian-process-models.html"><a href="gaussian-process-models.html#gaussian-processes-for-time-series-data"><i class="fa fa-check"></i><b>13.3</b> Gaussian Processes for time series data</a></li>
<li class="chapter" data-level="13.4" data-path="gaussian-process-models.html"><a href="gaussian-process-models.html#latent-force-models"><i class="fa fa-check"></i><b>13.4</b> Latent Force Models</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Building energy statistical modelling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ssmprinciple" class="section level1 hasAnchor" number="10">
<h1><span class="header-section-number">Chapter 10</span> Principle of SSMs<a href="ssmprinciple.html#ssmprinciple" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Unless specified otherwise, the main source for the descriptions of this section is the book of Simo Särkkä: <a href="https://users.aalto.fi/~ssarkka/pub/cup_book_online_20131111.pdf">Bayesian filtering and smoothing</a></p>
<div id="description" class="section level2 hasAnchor" number="10.1">
<h2><span class="header-section-number">10.1</span> Description<a href="ssmprinciple.html#description" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>State-space models (SSM) are the continuous-state equivalent of hidden Markov models: the sequence of <span class="math inline">\(T\)</span> output variables <span class="math inline">\({y_t}\)</span> is generated by a parallel sequence of <span class="math inline">\(n_x\)</span> latent continuous state variables <span class="math inline">\(x_t \in \mathbb{R}^{n_x}\)</span>. Since the letter <span class="math inline">\(x\)</span> is used here to denote the state variable, we denote the explanatory inputs as <span class="math inline">\(u_t \in \mathbb{R}^{n_u}\)</span>. A Bayesian SSM is fully defined by four distributions: a prior distribution of the parameters <span class="math inline">\(\theta\)</span>; a prior distribution of the first hidden state <span class="math inline">\(\mathbf{x}_0\)</span>; a dynamic model which describes the system dynamics and its uncertainties in terms of the transition probability distribution; a measurement model which describes how the measurements depend on the current state:
<span class="math display" id="eq:ssm">\[\begin{align}
    \theta &amp; \sim p(\theta) \nonumber \\
    \mathbf{x}_0 &amp; \sim p(\mathbf{x}_0) \nonumber \\
    \mathbf{x}_t &amp; \sim p(\mathbf{x}_t | \mathbf{x}_{t-1}, \theta) \nonumber \\
    \mathbf{y}_t &amp; \sim p(\mathbf{y}_t | \mathbf{x}_t, \theta) \tag{10.1}
\end{align}\]</span></p>
<p>Given the measurements <span class="math inline">\(\mathbf{y}_{1:T}\)</span>, we are then interested in computing:</p>
<ul>
<li>Filtering distributions, i.e. the marginal distribution of each state given the current and previous measurements:
<span class="math display">\[ p(\mathbf{x}_t | \mathbf{y}_{1:t}, \theta), \quad t = 1, \dots ,T \]</span></li>
<li>Prediction distributions, the marginal distributions of future states:
<span class="math display">\[ p(\mathbf{x}_{t+k} | \mathbf{y}_{1:t}, \theta), \quad t = 1, \dots ,T, \quad k=1,2,\dots \]</span></li>
<li>Smoothing distributions, the marginal distributions of each state given all measurements:
<span class="math display">\[p(\mathbf{x}_t | \mathbf{y}_{1:T}, \theta), \quad t = 1, \dots ,T \]</span></li>
<li>Parameter estimation, i.e. the posterior density of parameters given all measurements:
<span class="math display">\[ p(\theta | \mathbf{y}_{1:T}) \]</span></li>
<li><em>On-line</em> parameter estimation, i.e. the marginal distribution of parameters given the current and previous measurements:
<span class="math display">\[ p(\theta | \mathbf{y}_{1:t}), \quad t = 1, \dots ,T \]</span></li>
</ul>
<p>Filtering, prediction and smoothing are variations of <em>state estimation</em>. Filtering and smoothing a linear Gaussian state-space model are respectively done with the Kalman filter (KF) and the Rauch-Tung-Striebel smoother (RTSS). These methods are not applicable to non-linear and non-Gaussian models, but other alternatives are: extended Kalman filter (EKF) and unscented Kalman filter (UKF), along with their smoother counterparts (ERTSS and URTSS); sequential Monte Carlo, or particle filters and smoothers; etc.</p>
<p>Parameter estimation is meant here as the computation of <span class="math inline">\(p(\theta | \mathbf{y}_{1:T})\)</span>, the marginal posterior of <span class="math inline">\(\theta\)</span> given all measurements. The way to do this is to perform filtering, which produces the likelihood function <span class="math inline">\(p(\mathbf{y}_{1:T} | \theta)\)</span> as a side product. The Maximum Likelihood estimate of <span class="math inline">\(\theta\)</span> is then obtainable by numerical maximization of the likelihood, otherwise the full posterior density can be approached by one of the Markov Chain Monte Carlo (MCMC) methods. As opposed to batch estimation, on-line estimation means computing <span class="math inline">\(p(\theta | \mathbf{y}_{1:t})\)</span> at every time step <span class="math inline">\(t\)</span>, i.e. the probability distribution of <span class="math inline">\(\theta\)</span> given the current and previous measurement.</p>
</div>
<div id="linearssm" class="section level2 hasAnchor" number="10.2">
<h2><span class="header-section-number">10.2</span> Linear state-space models<a href="ssmprinciple.html#linearssm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the linear Gaussian state-space formulation, both the dynamic and the measurement models are linear with respect to the state <span class="math inline">\(\mathbf{x}_t\)</span> and the inputs <span class="math inline">\(\mathbf{u}_t\)</span>:
<span class="math display" id="eq:lineardiscrete2" id="eq:lineardiscrete1">\[\begin{align}
        \mathbf{x}_t &amp; = \mathbf{A}_\theta \, \mathbf{x}_{t-1} + \mathbf{B}_\theta \, \mathbf{u}_t + \mathbf{w}_t \tag{10.2} \\
        \mathbf{y}_t &amp; = \mathbf{C}_\theta \, \mathbf{x}_t + \mathbf{D}_\theta \, \mathbf{u}_t + \mathbf{v}_t \tag{10.3}
\end{align}\]</span>
where the matrices <span class="math inline">\(\mathbf{A}_\theta\)</span>, <span class="math inline">\(\mathbf{B}_\theta\)</span>, <span class="math inline">\(\mathbf{C}_\theta\)</span> and <span class="math inline">\(\mathbf{D}_\theta\)</span> have coefficients as functions of <span class="math inline">\(\theta\)</span> and are not necessarily time-invariant. <span class="math inline">\(\mathbf{w}_t \sim N\left(0, \mathbf{Q}\right)\)</span> is the process noise and <span class="math inline">\(\mathbf{v}_t \sim N\left(0, \mathbf{R}\right)\)</span> is the measurement noise.</p>
<p>As a practical example of linear state-space models, the simplified resistor-capacitor (RC) model structures are a popular choice for either parameter estimation or system identification. When written as a set of stochastic differential equations, they allow accounting for modelling approximations (<span class="citation">Madsen and Holst (<a href="#ref-madsen1995estimation" role="doc-biblioref">1995</a>)</span>) and offer a more reproducible parameter estimation than deterministic models that overlook modelling errors (<span class="citation">Rouchier, Rabouille, and Oberlé (<a href="#ref-rouchier2018calibration" role="doc-biblioref">2018</a>)</span>). These models will be used in the next chapter for the estimation of thermal properties of building envelopes.</p>
<div class="figure"><span style="display:block;" id="fig:rcmodel"></span>
<img src="figures/rcmodel.png" alt="Second order RC model" width="65%" />
<p class="caption">
Figure 10.1: Second order RC model
</p>
</div>
<p>Consider the example of a simple building represented by a 2-resistor, 2-capacitor model structure (2R2C) as shown here. The equations of this model are:</p>
<p><span class="math display" id="eq:statespace2" id="eq:statespace1">\[\begin{align}
  C_i \, \mathrm{d} T_i &amp; = \dfrac{1}{R_i}\left(T_e-T_i\right)\mathrm{d}t + \Phi_h \, \mathrm{d}t + A_i \Phi_s \mathrm{d}t + \sigma_i \,\mathrm{d}\omega_i \tag{10.4} \\
  C_e \, \mathrm{d} T_e &amp; = \dfrac{1}{R_i}\left(T_i-T_e\right)\mathrm{d}t + \frac{1}{R_o}\left(T_o-T_e\right)\mathrm{d}t + A_e \Phi_s \mathrm{d}t + \sigma_e \, \mathrm{d}\omega_e \tag{10.5}
\end{align}\]</span></p>
<p>where <span class="math inline">\(T_i\)</span>, <span class="math inline">\(T_e\)</span> and <span class="math inline">\(T_o\)</span> are the indoor, envelope and outdoor temperatures. The envelope temperature is associated with the thermal mass of the opaque surfaces, and does not represent a specific coordinate within the envelope. The model has two states <span class="math inline">\(T_e\)</span> (unobserved) and <span class="math inline">\(T_i\)</span> (observed); <span class="math inline">\(\Phi_h\)</span> (W) is the indoor heating power; <span class="math inline">\(\Phi_s\)</span> (W/m<span class="math inline">\(^2\)</span>) is the global horizontal solar irradiance. <span class="math inline">\(R_i\)</span> (K/W) is the thermal resistance between the indoor air temperature and the envelope, <span class="math inline">\(R_e\)</span> the resistance between the envelope and the ambient air. <span class="math inline">\(C_i\)</span> and <span class="math inline">\(C_e\)</span> (J/K) are the heat capacitances of the interior and the envelope, respectively, and <span class="math inline">\(A_i\)</span> and <span class="math inline">\(A_e\)</span> (m<span class="math inline">\(^2\)</span>) are their solar gain coefficients. <span class="math inline">\(\{\omega_i\}\)</span> and <span class="math inline">\(\{\omega_e\}\)</span> are standard Wiener processes and <span class="math inline">\(\sigma_i^2\)</span> are <span class="math inline">\(\sigma_e^2\)</span> are their variances. This process noise is a way to account for modelling approximations, unrecognized inputs or noise-corrupted input measurements.</p>
<p>Eq. <a href="ssmprinciple.html#eq:statespace1">(10.4)</a> and <a href="ssmprinciple.html#eq:statespace2">(10.5)</a> can be written in matrix form:
<span class="math display" id="eq:statespace3">\[\begin{equation}
\mathrm{d} \begin{bmatrix} T_i \\ T_e \end{bmatrix} = \begin{pmatrix} -\frac{1}{R_i \, C_i} &amp; \frac{1}{R_i \, C_i} \\ \frac{1}{R_i \, C_e} &amp; -\frac{1}{R_i \, C_e}-\frac{1}{R_e \, C_e}\end{pmatrix} \begin{bmatrix} T_i \\ T_e \end{bmatrix}\, \mathrm{d}t + \begin{pmatrix} 0 &amp; \frac{1}{C_i} &amp; \frac{A_i}{C_i} \\ \frac{1}{R_e \, C_e} &amp; 0 &amp; \frac{A_e}{C_e} \end{pmatrix} \begin{bmatrix} T_o \\ \Phi_h \\ \Phi_s \end{bmatrix} \, \mathrm{d}t + \mathbf{\sigma} \, \mathrm{d}\omega \tag{10.6}
\end{equation}\]</span>
which is the dynamic model of the following stochastic state-space model, written in continuous-discrete form:
<span class="math display" id="eq:statespace5" id="eq:statespace4">\[\begin{align}
\mathrm{d}\mathbf{x}(t) &amp; = \mathbf{A}_\mathit{rc} \, \mathbf{x}(t) \, \mathrm{d}t + \mathbf{B}_\mathit{rc} \, \mathbf{u}(t)\,\mathrm{d}t + \mathbf{\sigma}_\theta \mathrm{d}\omega \tag{10.7}  \\
\mathbf{y}_t &amp; = \mathbf{C}_\theta \, \mathbf{x}_t + \mathbf{v}_t \tag{10.8}
\end{align}\]</span>
The state vector <span class="math inline">\(\mathbf{x}\)</span> includes the temperatures <span class="math inline">\(T_i\)</span> and <span class="math inline">\(T_e\)</span> calculated by the model, and <span class="math inline">\(\mathbf{u}=\left[T_o, \Phi_h, \Phi_s\right]\)</span> is the input vector including boundary conditions and excitations. The second equation, Eq. <a href="ssmprinciple.html#eq:statespace5">(10.8)</a>, is the observation equation. It indicates that the measured quantity <span class="math inline">\(y_t\)</span> may be different from the output of the state equation. In our case, the observed temperature is only the first component of the state vector, and is encumbered with some measurement error <span class="math inline">\(\mathbf{v}_t\)</span>. In this equation, time is noted as a subscript to indicate that observations come in a discrete sequence.</p>
<p>This model described by Eq. <a href="ssmprinciple.html#eq:statespace4">(10.7)</a> must be discretized in order to specify its evolution between discrete time coordinates. Supposing a sample interval length <span class="math inline">\(\Delta t\)</span> and assume that the inputs <span class="math inline">\(\mathbf{u}(t)\)</span> are constant during each interval. Eq. <a href="ssmprinciple.html#eq:statespace4">(10.7)</a> and <a href="ssmprinciple.html#eq:statespace5">(10.8)</a> can be discretized into the system of Eq. <a href="ssmprinciple.html#eq:lineardiscrete1">(10.2)</a> and <a href="ssmprinciple.html#eq:lineardiscrete2">(10.3)</a> through the following discretization equations:
<span class="math display" id="eq:discretization4" id="eq:discretization2" id="eq:discretization1">\[\begin{align}
  \mathbf{A}_\theta &amp; = \mathrm{exp}\left( \mathbf{A}_\mathit{rc} \, \Delta t  \right) \tag{10.9}\\
  \mathbf{B}_\theta &amp; = \mathbf{A}_\mathit{rc}^{-1} \, \left(\mathbf{A}_\theta-\mathbf{I}\right) \, \mathbf{B}_\mathit{rc} \tag{10.10} \\
  \mathbf{Q} &amp; = \int_0^{\Delta t} \mathrm{exp}\left( \mathbf{A}_\mathit{rc} \, \Delta t  \right) \, \mathbf{\sigma}_\theta \, \mathrm{exp}\left( \mathbf{A}_\mathit{rc}^T \, \Delta t  \right) \mathrm{d}t \tag{10.11}
\end{align}\]</span></p>
<p>The Kalman filter is the typical method for computing the marginal distribution of each state <span class="math inline">\(p(\mathbf{x}_t | \mathbf{y}_{1:t}, \theta)\)</span> and the likelihood function <span class="math inline">\(p(\mathbf{y}_{1:t}| \theta)\)</span> from which <span class="math inline">\(\theta\)</span> can be estimated.</p>
</div>
<div id="kalmanfilter" class="section level2 hasAnchor" number="10.3">
<h2><span class="header-section-number">10.3</span> The Kalman filter<a href="ssmprinciple.html#kalmanfilter" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Given a state transition probability <span class="math inline">\(p\left( \mathbf{x}_{t} | \theta, \mathbf{x}_{t-1}, \mathbf{u}_{t} \right)\)</span> (Eq. <a href="ssmprinciple.html#eq:lineardiscrete1">(10.2)</a>) and an observation probability <span class="math inline">\(p\left( \mathbf{y}_{t} | \mathbf{x}_{t}\right)\)</span> (Eq. <a href="ssmprinciple.html#eq:lineardiscrete2">(10.3)</a>), a Kalman filter produces <span class="math inline">\(p\left(\mathbf{x}_t|\mathbf{y}_{1:T}, \theta \right)\)</span>, the probability distribution function of each state <span class="math inline">\(\mathbf{x}_t\)</span> given measurements and parameter values, and the marginal likelihood function <span class="math inline">\(L_y(\theta)=p\left(\mathbf{y}_{1:T} | \theta \right)\)</span>.</p>
<p>Filtering produces <span class="math inline">\(p\left(\mathbf{x}_t|\mathbf{y}_{1:N}, \theta \right)\)</span>, the probability distribution function of each state <span class="math inline">\(\mathbf{x}_t\)</span> given measurements and parameter values. In the following, definitions adapted from the book of Shumway et al (<span class="citation">Shumway and Stoffer (<a href="#ref-shumway2000time" role="doc-biblioref">2000</a>)</span>) are used: <span class="math inline">\(\mathbf{x}_{t|s}\)</span> is the expected state at time <span class="math inline">\(t\)</span> given observations up to time <span class="math inline">\(s\)</span>. <span class="math inline">\(\mathbf{P}_{t|s}\)</span> is the variance of the state <span class="math inline">\(\mathbf{x}_{t}\)</span>, i.e. the mean-squared error.
<span class="math display">\[\begin{align}
  \mathbf{x}_{t|s} &amp; = \mathrm{E}\left(\mathbf{x}_t|\mathbf{y}_{1:s}, \theta \right) \\
  \mathbf{P}_{t|s} &amp; = \mathrm{Var}\left(\mathbf{x}_t|\mathbf{y}_{1:s, \theta}\right)= \mathrm{E}\left[(\mathbf{x}_t-\mathbf{x}_{t|s})(\mathbf{x}_t-\mathbf{x}_{t|s})^T|\mathbf{y}_{1:s}, \theta \right]
\end{align}\]</span></p>
<div class="figure"><span style="display:block;" id="fig:kalman"></span>
<img src="figures/kalman.png" alt="Schematic view of one iteration of the Kalman filter" width="50%" />
<p class="caption">
Figure 10.2: Schematic view of one iteration of the Kalman filter
</p>
</div>
<p>The Kalman filter algorithm is described here and illustrated by Fig. <a href="ssmprinciple.html#fig:kalman">10.2</a>:</p>
<ul>
<li>Set the initial states <span class="math inline">\(\mathbf{x}_{0|0}\)</span> and their covariance <span class="math inline">\(\mathbf{P}_{0|0}\)</span></li>
<li>for <span class="math inline">\(t=1...T\)</span>:
<ol style="list-style-type: decimal">
<li><strong>Prediction step</strong>: given the previous state <span class="math inline">\(\mathbf{x}_{t|t}\)</span> and its covariance <span class="math inline">\(\mathbf{P}_{t|t}\)</span>, the model estimates the one-step ahead prediction.
<span class="math display">\[\begin{align}
\mathbf{x}_{t+1|t} &amp; = \mathbf{A}_\theta \, \mathbf{x}_{t|t} + \mathbf{B}_\theta \, \mathbf{u}_{t+1}\\
\mathbf{P}_{t+1|t} &amp; = \mathbf{A}_\theta \, \mathbf{P}_{t|t} \, \mathbf{A}_\theta^T + \mathbf{Q}
\end{align}\]</span></li>
<li><strong>Innovations</strong> (prediction error) <span class="math inline">\(\varepsilon_{t+1}\)</span> and their covariances <span class="math inline">\(\Sigma_{t+1}\)</span> are then calculated, along with the Kalman gain <span class="math inline">\(\mathbf{K}_{t+1}\)</span>, by comparing <strong>measurements</strong> <span class="math inline">\(\mathbf{y}_{t+1}\)</span> with the one-step ahead prediction <span class="math inline">\(\mathbf{x}_{t+1|t}\)</span>:
<span class="math display">\[\begin{align}
  \varepsilon_{t+1} &amp; = \mathbf{y}_{t+1} - \mathbf{C}_\theta \, \mathbf{x}_{t+1|t}\\
  \Sigma_{t+1} &amp; = \mathbf{C}_\theta \, \mathbf{P}_{t+1|t} \, \mathbf{C}_\theta^T + \mathbf{R} \\
  \mathbf{K}_{t+1} &amp; = \mathbf{P}_{t+1|t} \, \mathbf{C}_\theta^T \, \Sigma_{t+1}^{-1}
\end{align}\]</span></li>
<li><strong>Updating step</strong>: the new states at time <span class="math inline">\(t+1\)</span> are updated, as a compromise between the one-step ahead prediction and the measurement.
<span class="math display">\[\begin{align}
  \mathbf{x}_{t+1|t+1} &amp; = \mathbf{x}_{t+1|t} + \mathbf{K}_{t+1} \, \mathbf{\varepsilon}_{t+1} \\
  \mathbf{P}_{t+1|t+1} &amp; = \left( \mathbf{I}- \mathbf{K}_{t+1} \, \mathbf{C}_\theta \right) \, \mathbf{P}_{t+1|t}
\end{align}\]</span></li>
</ol></li>
<li>The total (negative) log-likelihood can be calculated up to a normalizing constant:
<span class="math display">\[\begin{equation}
  -\ln L_y(\theta) = \frac{1}{2} \sum_{t=1}^{T} \ln \left|\Sigma_t(\theta)\right| + \frac{1}{2} \sum_{t=1}^{T} \varepsilon_t(\theta)^T \, \Sigma_t(\theta)^{-1} \, \varepsilon_t(\theta)
\end{equation}\]</span></li>
</ul>
<p>Roughly speaking, the Kalman filter applies Bayes’ rule at each time step: the updated state <span class="math inline">\(p(\mathbf{x}_t|\mathbf{y}_{1:t})=N(\mathbf{x}_{t|t}, \mathbf{P}_{t|t})\)</span> is a posterior distribution, obtained from a compromise between a prior output of the model <span class="math inline">\(p(\mathbf{x}_t|\mathbf{y}_{1:t-1})=N(\mathbf{x}_{t|t-1}, \mathbf{P}_{t|t-1})\)</span> and the evidence brought by measurements <span class="math inline">\(\mathbf{y}_t\)</span>. Their relative weight is expressed by the Kalman gain <span class="math inline">\(\mathbf{K}_t\)</span> that measures the relative confidence we put in both the model and the measurements.</p>
</div>
<div id="non-linear-state-space-models" class="section level2 hasAnchor" number="10.4">
<h2><span class="header-section-number">10.4</span> Non-linear state-space models<a href="ssmprinciple.html#non-linear-state-space-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>State-space models are not necessarily linear or Gaussian. In a more generic situation, the system of equations <a href="ssmprinciple.html#eq:ssm">(10.1)</a> can take the form:
<span class="math display" id="eq:nonlineardiscrete2" id="eq:nonlineardiscrete1">\[\begin{align}
        \mathbf{x}_t &amp; = F(\mathbf{x}_{t-1}, \mathbf{u}_t, \mathbf{w}_t) \tag{10.12} \\
        \mathbf{y}_t &amp; = G(\mathbf{x}_t, \mathbf{u}_t, \mathbf{v}_t) \tag{10.13}
\end{align}\]</span></p>
<p>The most common extensions of the Kalman filter to non-linear systems are the extended Kalman filter (EKF) and unscented Kalman filter (UKF). The EKF approximates the non-linear and non-Gaussian measurement and dynamic models by linearization at the nominal solution. The unscented Kalman filter (UKF) approximates the propagation of densities through the non-linearities of measurement and noise processes using the unscented transform. They are both Gaussian approximations.</p>
<p>Non-linearity in a building energy state-space model can arise in different situations:</p>
<ul>
<li>The states can include not only the temperatures simulated by an RC model. By using fictitious process equations to augment the state vector with model parameters, it is possible to perform on-line joint state-parameter estimation.</li>
<li>To encode time-varying uncertainty in some of the model inputs: states can be augmented with additional state variables describing these varying unknown parameters.</li>
</ul>
<p>For non-linear non-Gaussian state-space models, particle filters, or sequential Monte Carlo (SMC) methods, are now a popular alternative to the EKF because of their suitability for parallel implementation and for on-line inference (<span class="citation">Cappé, Godsill, and Moulines (<a href="#ref-cappe2007overview" role="doc-biblioref">2007</a>)</span>). SMC filters and smoothers represent the posterior distribution of states as a weighted set of Monte Carlo samples. They are applicable to joint state-parameter estimation and have already been applied to the characterization of building envelope properties (<span class="citation">Rouchier, Jiménez, and Castaño (<a href="#ref-rouchier2019sequential" role="doc-biblioref">2019</a>)</span>).</p>
<p>In a non-Bayesian framework, particle methods can approximate the likelihood function which may then be maximized for an off-line or on-line ML estimation of parameters (<span class="citation">Kantas et al. (<a href="#ref-kantas2015particle" role="doc-biblioref">2015</a>)</span>). In Bayesian joint state-parameter estimation, the target distribution <span class="math inline">\(p(\theta, x_{1:T} | y_{1:T})\)</span> can be approached in several ways: Particle MCMC methods (<span class="citation">Andrieu, Doucet, and Holenstein (<a href="#ref-andrieu2010particle" role="doc-biblioref">2010</a>)</span>) sample from <span class="math inline">\(\theta\)</span> with a MCMC method, within which particle filters are called to approximate the marginal state distributions; the SMC<span class="math inline">\(^2\)</span> algorithm (<span class="citation">Chopin, Jacob, and Papaspiliopoulos (<a href="#ref-chopin2013smc2" role="doc-biblioref">2013</a>)</span>) couples a SMC algorithm in the <span class="math inline">\(\theta\)</span>-dimension, which propagates and resamples many particle filters in the <span class="math inline">\(x\)</span>-dimension.</p>
</div>
<div id="switching-state-space-models" class="section level2 hasAnchor" number="10.5">
<h2><span class="header-section-number">10.5</span> Switching state-space models<a href="ssmprinciple.html#switching-state-space-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The last form of statistical model we will mention here are the state-space models with regime switching, or switching state-space models (SSSM), or switching dynamical systems. They are state-space models containing both categorical and continuous hidden variables, and thus a sort of coalition between hidden Markov models and state-space models.</p>
<div class="figure"><span style="display:block;" id="fig:tikzsdm"></span>
<img src="figures/tikz_sdm.png" alt="Switching dynamic model" width="35%" />
<p class="caption">
Figure 10.3: Switching dynamic model
</p>
</div>
<p>One of the forms of SSSMs is shown here, where the continuous state variable <span class="math inline">\(X\)</span> is conditioned on a categorical latent variable <span class="math inline">\(S\)</span> denoted the <em>switch</em>. Like in an HMM or MSM, the switch refers to one of several possible regimes the system can be in. The solutions of such models with a fixed number of modes of operation can be approximated by several methods</p>
<ul>
<li>Generalized pseudo-Bayesian methods, and the interacting multiple model (IMM) algorithm (<span class="citation">Blom and Bar-Shalom (<a href="#ref-blom1988interacting" role="doc-biblioref">1988</a>)</span>), based on forming a mixture of Kalman filters;</li>
<li>Expectation propagation (<span class="citation">Zoeter and Heskes (<a href="#ref-zoeter2011expectation" role="doc-biblioref">2011</a>)</span>);</li>
<li>Rao-Blackwellised particle filters (<span class="citation">Doucet et al. (<a href="#ref-Doucet:2000" role="doc-biblioref">2000</a>)</span>) use closed form integration (Kalman filters) for some of the state variables and Monte Carlo integration for others (<span class="citation">Särkkä (<a href="#ref-sarkka2013bayesian" role="doc-biblioref">2013</a>)</span>).</li>
</ul>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-andrieu2010particle" class="csl-entry">
Andrieu, Christophe, Arnaud Doucet, and Roman Holenstein. 2010. <span>“Particle Markov Chain Monte Carlo Methods.”</span> <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 72 (3): 269–342.
</div>
<div id="ref-blom1988interacting" class="csl-entry">
Blom, Henk AP, and Yaakov Bar-Shalom. 1988. <span>“The Interacting Multiple Model Algorithm for Systems with Markovian Switching Coefficients.”</span> <em>IEEE Transactions on Automatic Control</em> 33 (8): 780–83.
</div>
<div id="ref-cappe2007overview" class="csl-entry">
Cappé, Olivier, Simon J Godsill, and Eric Moulines. 2007. <span>“An Overview of Existing Methods and Recent Advances in Sequential Monte Carlo.”</span> <em>Proceedings of the IEEE</em> 95 (5): 899–924.
</div>
<div id="ref-chopin2013smc2" class="csl-entry">
Chopin, Nicolas, Pierre E Jacob, and Omiros Papaspiliopoulos. 2013. <span>“Smc2: An Efficient Algorithm for Sequential Analysis of State Space Models.”</span> <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 75 (3): 397–426.
</div>
<div id="ref-Doucet:2000" class="csl-entry">
Doucet, Arnaud, Nando de Freitas, Kevin Murphy, and Stuart Russell. 2000. <span>“Rao-Blackwellised Particle Filtering for Dynamic Bayesian Networks.”</span> In <em>Uncertainty in Artificial Intelligence (UAI)</em>, 176–83. San Francisco, CA.
</div>
<div id="ref-kantas2015particle" class="csl-entry">
Kantas, Nikolas, Arnaud Doucet, Sumeetpal S Singh, Jan Maciejowski, Nicolas Chopin, et al. 2015. <span>“On Particle Methods for Parameter Estimation in State-Space Models.”</span> <em>Statistical Science</em> 30 (3): 328–51.
</div>
<div id="ref-madsen1995estimation" class="csl-entry">
Madsen, Henrik, and Jan Holst. 1995. <span>“Estimation of Continuous-Time Models for the Heat Dynamics of a Building.”</span> <em>Energy and Buildings</em> 22 (1): 67–79.
</div>
<div id="ref-rouchier2019sequential" class="csl-entry">
Rouchier, Simon, Maria José Jiménez, and Sergio Castaño. 2019. <span>“Sequential Monte Carlo for on-Line Parameter Estimation of a Lumped Building Energy Model.”</span> <em>Energy and Buildings</em> 187: 86–94.
</div>
<div id="ref-rouchier2018calibration" class="csl-entry">
Rouchier, Simon, Mickaël Rabouille, and Pierre Oberlé. 2018. <span>“Calibration of Simplified Building Energy Models for Parameter Estimation and Forecasting: Stochastic Versus Deterministic Modelling.”</span> <em>Building and Environment</em> 134: 181–90.
</div>
<div id="ref-sarkka2013bayesian" class="csl-entry">
Särkkä, Simo. 2013. <em>Bayesian Filtering and Smoothing</em>. Cambridge University Press.
</div>
<div id="ref-shumway2000time" class="csl-entry">
Shumway, Robert H, and David S Stoffer. 2000. <em>Time Series Analysis and Its Applications</em>. Vol. 3. Springer.
</div>
<div id="ref-zoeter2011expectation" class="csl-entry">
Zoeter, Onno, and T Heskes. 2011. <span>“Expectation Propagation and Generalised EP Methods for Inference in Switching Linear Dynamical Systems.”</span>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="composite-time-series-models.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="a-simple-rc-model-python.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "lunr",
"options": null
},
"toc": {
"collapse": "subsection",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"info": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
