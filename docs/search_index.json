[["index.html", "Building energy statistical modelling Home page Motivation Content of the book Programming languages About", " Building energy statistical modelling Simon Rouchier 2022-07-06 Home page Motivation Data science offers promising prospects for improving the energy efficiency of buildings. Thanks to the availability of smart meters and sensor networks, along with increasingly accessible algorithms for data processing and analysis, statistical models may be trained to predict the energy use of HVAC systems or the indoor conditions. These trained models and their predictions then lead to various inferences: assessing the real impact of energy conservation measures; identifying HVAC faults or physical properties of the envelope in order to provide incentive for retrofitting; minimizing energy consumption through model predictive control; detecting and diagnosing faults; etc. The availability of measurements and computational power have given data mining methods an increasing popularity. The field of data analysis applied to building energy performance assessment however faces two main challenges to this day. Ironically, the first challenge is the abundance of data. Smart meters and building management systems deliver large amounts of information which can hide the few readings which are the most relevant to energy conservation. Automated monitoring and fault detection algorithms only do what they are told, and will hardly replace human intervention when it comes to understanding readings. The second challenge is the difficulty of data science. Without a principled methodology, it is very easy to draw erroneous conclusions, by incorrectly assuming that a model is properly trained. By lack of a background in statistics, building energy practitioners often lack the tools to ensure their inferences are correct. Content of the book The topic of this book is statistical modelling and inference applied to building energy performance assessment. It has two target audiences: building energy researchers and practitioners who need a gentle introduction to statistical modelling; statisticians who may be interested in applications to energy performance. The first part of the book covers the motivation and theoretical background. Chap. 1 is an overview of the possibilities of data analysis applied to building energy performance assessment, and of the main categories and challenges of data analysis methods. Chap. 2 quickly describes building physics and how they can be formulated as statistical models. Chap. 3 will describe the main steps of a Bayesian workflow for statistical modelling and inference, which aims at making sure that models are well defined and trained for a given application. Then, the rest of the book shows some applications. It is a series of R and Python notebooks classified into chapters, each focusing on a type of model. The notebooks are self-sufficient, either based on R or Python, and mention whether non-standard libraries or other software should be installed. Regression and mixture models Time series analysis State-space models for Gaussian Process models The target of the book is that the basics of Bayesian data analysis are explained to building energy practitioners who dont necessarily have a large background on statistics. The book does not cover: Data acquisition and pre-processing, although a crucial step of data analysis, will not be explained in detail for each problem. Big data. The typical size of our data files is a few MB, up to a few GB for the largest sets. This is far from what computer scientists consider big data. Machine learning. Even when the target of a particular problem is prediction rather than inference of physical properties, our statistical models will always have some degree of physical interpretability. Classification. Most of the methods shown are variations of regression problems, as our models will almost always have quantitative responses. There are however strong links with classification problems, especially when it comes to identifying occupant presence and behaviour. Programming languages This book is written and maintained with bookdown. I dont believe there is an obvious winner in the war of which language is better for data science. An analyst may use more than one for a variety of reasons. Part II tutorials are written in R Part III tutorials (time series) are written in R and Python Part IV tutorials (state space models) are written in Python The first language used in this book is R. Not because of its performance, but because it offers a comfortable environment for data analysis: the elegance of the tidyverse is unmatched, and Rstudio is simply my favourite scientific IDE by a long shot. Still, some tutorials are written in Python because I have more experience with it, and because it is the default language for most researchers in building energy performance. Python is always a safe choice for performance and versatility, and is currently very strong in machine learning (scikit-learn, TensorFlow, Keras, Pytorch, Pyro) About I am Simon Rouchier, lecturer at the Université Savoie Mont Blanc, Chambéry, France. This website is the beta version of a future book. It possibly still has some typos and inconsistencies: I welcome all feedback by email or through the books Github repo. Ce(tte) uvre est mise à disposition selon les termes de la Licence Creative Commons Attribution - Pas dUtilisation Commerciale - Partage dans les Mêmes Conditions 4.0 International. "],["scope.html", "Chapter 1 Background on data analysis 1.1 The energy savings potential of buildings 1.2 From data to energy savings 1.3 Categories of data-driven modelling approaches", " Chapter 1 Background on data analysis 1.1 The energy savings potential of buildings The energy consumption of buildings accounts for 40% of the global primary energy use and up to 33% of carbon emissions in some countries, mainly from the operation of heating, ventilation and air conditioning (HVAC) systems. The gap in energy performance between older, poorly insulated constructions, and newer net-zero energy or passive buildings with efficient energy control strategies, shows the magnitude of the improvement that could be brought to the energy efficiency of the building stock. It is commonly known that the share of new buildings in the overall construction sector is very low. Most buildings are several decades old and often have poor energy performance, especially compared to recent standards for new constructions. The largest potential for energy savings in the building sector therefore lies in the renovation of the existing building stock, or its proper energy management. Data science offers promising prospects for improving the energy efficiency of buildings. Thanks to the availability of smart meters and sensor networks, along with increasingly accessible algorithms for data processing and analysis, statistical models may be trained to predict the energy use of HVAC systems or the indoor conditions. These trained models and their predictions then lead to various inferences: assessing the real impact of energy conservation measures; identifying HVAC faults or physical properties of the envelope in order to provide incentive for retrofitting; minimizing energy consumption through model predictive control; detecting and diagnosing faults; etc. The availability of measurements and computational power have given data mining methods an increasing popularity. The field of data analysis applied to building energy performance assessment however faces two main challenges to this day. Ironically, the first challenge is the abundance of data. Smart meters and building management systems deliver large amounts of information which can hide the few readings which are the most relevant to energy conservation. Automated monitoring and fault detection algorithms only do what they are told, and will hardly replace human intervention when it comes to understanding readings. The second challenge is the difficulty of data science. Without a principled methodology, it is very easy to draw erroneous conclusions, by incorrectly assuming that a model is properly trained. By lack of a background in statistics, building energy practitioners often lack the tools to ensure their inferences are correct. 1.2 From data to energy savings 1.2.1 Formalisation of the system Before proposing a few examples on how data acquisition may support energy conservation measures, let us first formalise the framework, in which the following parts of this book will describe building energy performance. Figure 1.1: Formalisation of the system into observable variables and non-measurable influences The first level of this formalisation are the two conditions imposed on the building: weather and occupancy. What these two terms have in common is the fact that the buildings designer does not get to choose or influence them: every analysis that we will conduct will be conditional on these imposed conditions. The weather imposes the outdoor boundary conditions of the building envelope. It can be described as a set of measurable quantities (outdoor air temperature, humidity, direct and diffuse solar irradiance, wind speed and direction), some of which are predictable to some extent. The occupancy is more difficult to extensively describe with a finite set of variables. This term is used here to encompass all actions and choices of the occupants regarding their own comfort and how they operate the building: leaving and entering the building, setting indoor temperature set points, opening and closing windows, operating appliances that consume energy These actions cannot be simply measured and summarised into a few descriptive variables, as can be the weather. The second level of the formalisation is the building itself. Building energy simulation usually separates the description of the HVAC systems and the envelope. Both can be characterised in terms of energy performance by a finite set of intrinsic performance indicators: heat transfer coefficient of the envelope, boiler efficiency, window transmissivity, pipe network heat loss These quantities cannot be directly observed, and define the intrinsic energy performance of the building: their values should not depend on the current state of the two previously mentioned conditions. The third level of the formalisation includes all extensive and intensive variables that can be measured by any kind of meter or sensor, inside or near the building. These variables are the consequences of the two conditions (weather and occupancy) coupled with the buildings intrinsic performance. They include readings from energy meters, variables which describe the indoor air (temperature, relative humidity, CO\\(_2\\) concentration) and various signals that describe the state of operation of HVAC systems or envelope components. The indoor air temperature, for instance, is influenced by the weather (outdoor air temperature, solar irradiance), the occupants (who chose the set point) and the intrinsic building energy performance (heat transfer coefficient and inertia of the envelope, efficiency of the temperature control loop). These variables, along with weather variables, constitute the data from which we will perform inferences and predictions. 1.2.2 Some uses of data We now use this formalisation to demonstrate a few examples of how recorded data may be used to motivate energy conservation measures or verify their efficiency. Fig. 1.2 uses the same color code as Fig. 1.1: blue denotes observable variables, orange denotes non-measurable information. Figure 1.2: How data can be used for various inferences and predictions Forecasting energy use The ability to predict the energy use of buildings is a useful tool for energy management, from the scale of a single dwelling to the scale of energy distribution in a smart grid. On a side note, we can mention a difference between the terms of prediction and forecasting. In modelling studies, prediction is a general term that means computing the outcome of any simulation model, while forecasting specifically denotes estimating the future values of a time series, on a time horizon where observations are not available. Two factors may facilitate the ability to forecast a time series. The first of these advantageous characteristics is a repetitive trend in an energy consumption profile (see Fig. 1.3). Large office buildings, collective housing and retail facilities tend to display a predictable daily profile for energy uses that have a low dependency on environmental factors, such as lighting and electrical appliances. Some energy uses mostly depend on user behaviour, whose stochastic behaviour tend to be smoothed out at larger scales of observation. Figure 1.3: Repetitive consumption profiles are easy to forecast by extrapolating daily and seasonal fluctuations Another factor that facilitates the prediction of energy use is a high dependency on an environmental factor that is itself easy to get forecasts of. In a building with a controlled set-point indoor temperature, a strong correlation may be observed between the outdoor temperature and the heating power in winter (see Fig. 1.4). The outdoor temperature is then considered a significant explanatory variable: its forecasts will allow forecasting the heating power with a satisfactory confidence. Figure 1.4: A correlation between outdoor temperature and heating power can be used to predict future demand These two examples illustrate two categories of leverages in time series analysis and forecasting. When forecasting a particular series, we can either make use of its own characteristics (periodicity, seasonality, autocorrelation), or identify dependencies with other measurable data. Measurement and verification (M&amp;V) The ability to predict energy demand can also be valued in the context of Measurement and Verification (M&amp;V). M&amp;V is the process of assessing savings caused by an Energy Conservation Measure (ECM). Savings are determined by comparing measured consumption or demand before and after implementation of a program, making suitable adjustments for changes in conditions. The International Performance Measurement and Verification Protocol (IPMVP) formalizes this process, and presents several options to conduct it. Figure 1.5: The IPMVP provides guidelines on how to perform M&amp;V An example of adjustment is, when estimating the energy savings delivered by an ECM, to substract its new energy consumption from the consumption that would have occurred if the building had stayed in the same situation in the weather conditions of the reporting period (adjusted baseline consumption). This requires a prediction model that can extrapolate the initial behaviour of the building by accounting for variable weather conditions. Other possible adjustments include changes in occupancy schedules. This is necessary to assess whether measured energy savings are caused by the ECM itself, or by changes in these influences. The IPMVP presents several options, depending on whether the operation concerns an entire facility or a portion, and defines the notion of measurement boundary as the set of measurements that are relevant to determine savings. In order to verify the savings from a single equipment, and a measurement boundary can be drawn around it, the approach used is retrofit-isolation: IPMVP options A and B. If the purpose of reporting is to verify total facility energy performance, the approach is the whole-facility option C. All options must account for the presence of interactive effects: energy impacts created by the ECM that cannot be measured within the measurement boundary. Any option that requires adjustment on measured independent variables implies the use of a prediction model, even a simple one. In the IPMVP option D, savings are determined through simulation of the energy consumption rather than direct measurements. The simulation model is calibrated so that it predicts the energy and load that matches the actual metered data. Under the correct assumptions, and with the right methodology (which we propose in this book!), calibrated simulation is potentially a very powerful M&amp;V methodology, as it may disaggregate energy uses and estimate interactive effects outside of the measurement boundary. M&amp;V is a crucial tool in the establishment of Energy Performance Contracts (EPC), which can be established on the basis of designed performance (building energy simulation), with an eventual uncertainty analysis and/or sensitivity analysis, or on the basis of measured energy consumption. Intrinsic performance assessment The third hereby presented application of data analysis is the estimation of quantitative indicators of the intrinsic energy performance of a building. This is different from the M&amp;V process as it does not necessarily imply the comparison between before/after situations. One typical example is the Heat Loss Coefficient (HLC), or the Heat Transfer Coefficient, which characterize heat transmission through the envelope, eventually including air infiltration. The co-heating test is one of the well-known methods for HLC assessment: it measures the heating power required to maintain a steady indoor temperature, and obtains HLC by averaging these measurements over a sufficiently long period. Figure 1.6: The co-heating test records the heating power, indoor and outdoor temperature, in order to estimate the heat loss coefficient of the envelope What is referred here as intrinsic performance assessment may also be called characterization, or parameter estimation, since it primarily works by estimating the values of static parameters of a simulation model. Such parameters may include the HLC, but also the efficiency of a system, an air infiltration rate Hence, it can be part of an energy audit to help characterize the state of a building and its eventual flaws before the design of an ECM. Fault detection and diagnostics One of the characteristics of smart buildings is the ability to monitor energy usage with the aim of identifying abnormal consumption behaviour and notifying the building manager to implement appropriate procedures (Araya et al. (2017)). Fault detection and diagnostics (FDD) is the process of using building operational data to detect the occurence of faults and identify their root causes (Granderson et al. (2020)). This process can be done manually, or by an algorithm delevoped to perform it automatically. Figure 1.7: Fault detection is a double challenge: (1) automatically detecting a difference between measurements and normal behaviour; (2) identifying its possible cause. Many supervised statistical learning methods for building energy load forecasting and anomaly detection have been developed in the last years. FDD is a challenging topic because it needs to reconcile two targets: efficient pattern detection among possibly large amounts of data, and physical interpretability of detected anomalies based on a detailed description of the building and its components. 1.2.3 Model calibration as the key to data analysis All the applications described above can be summarized by the same description: data are recorded and interpreted to draw conclusions about quantities that are either not directly observable (such as estimating a heat loss coefficient), or not yet observed (such as forecasting energy use). In all cases, the missing link between the data and the conclusion is a numerical model. Figure 1.8: The overall process of collecting and analyzing data As was described by the formalisation proposed above, the energy consumption of HVAC systems and other appliances, along with other measurable indoor variables, are the consequences of two sets of conditions (weather and occupancy) and of the intrinsic energy performance of the envelope and systems. Disaggregating each of these influences, and accessing intrinsic energy performance indicators, is a major challenge. For instance, measurement and verification protocols attempt to demonstrate whether energy savings may be attributed to an energy conservation measure, or is partially caused by a change in weather or occupancy behaviour. After identifying the phenomena that we wish to predict, or the performance indicators that we wish to estimate, monitoring equipment is implemented for data acquisition. Measurements are an insight of the real behaviour of a building, and are the basis for training the models that will reproduce it. The required types of monitoring depend on the characterisation target and on the specific energy uses of the building under study. In all cases, measurements can only provide a very fractional view of all phenomena that drive the energy performance. Heating and cooling energy consumption, for instance, is the outcome of several concurring heat transfer phenomena: transmission through the envelope and between thermal zones; convection through ventilation and air infiltration; temperature stratification inside each room; long-wave radiative heat exchange between walls Heating and cooling are also not the only energy consumptions that an operator may wish to be able to predict. Unless separate energy meters are implemented on each system and appliance, measurements of energy consumption are often aggregated values from which separate uses are difficult to isolate. Other important characteristics of the monitoring equipment are: the type and accuracy of sensors used for a given measurement, the acquisition time step, the spatial granularity of observation. Raw data only describes a fraction of the overall system, and does not allow disaggregating intrinsic energy performance indicators from influences of the weather and of the occupants. The solution to this problem is to define a numerical model as the missing link between the complexity of the real building and the conciseness of the data. The model is a numerical description of the building, where the non-observable performance indicators are given a specified value. The conditions imposed by the weather and the occupants are quantified in the equations of the model, which in turn provides values for energy consumption or indoor variables as output. Model specification is all but trivial, especially because of the variety of model types offered by building energy simulation. Selecting an appropriate model structure is essential to the learning procedure. The complexity of the model is a compromise between realism and parcimony: it should at least describe all the most significant processes occuring in the system, and should not allow any redundancy in the input-output relationship. Among several models, equally capable of reproducing a dataset, the best choice is usually the most simple one (Hastie, Tibshirani, and Friedman (2009)). The model is first defined after our knowledge of the state of the building. The next step is its calibration using the measured data. Calibrating a model means finding the settings or set of parameters with which its output best matches a series of observations, called a training dataset. This data usually originates from measurements (in either experimental test cells or real buildings), but may also have been produced by a complex reference model that we wish to approximate by a simplified one. There are two main outcomes of model training, which were already shown by two categories of data analysis applications: The first outcome are inferences about the processes that generated the data. If the model structure was appropriately chosen for this purpose, its parameters are related to the intrinsic energy performance indicators of the building. The second outcome is the ability acquired by the model to reproduce measurements, and therefore forecast the future values of some of the observed variables. Model calibration is therefore the key to all applications of data analysis that we mentioned earlier. It is however a more complicated problem than it seems and requires careful choices at every step of the entire procedure. 1.2.4 The difficulty of inverse problems Inverse techniques are a suite of methods which promise to provide better experiments and improved understanding of physical processes. Inverse problem theory can be summed up as the science of training models using measurements. The target of such a training is either to learn physical properties of a system by indirect measurements, or setting up a predictive model that can reproduce past observations. In the last couple of decades, building physics researchers have benefited from elements of statistical learning and time series analysis to improve their ability to construct knowledge from data. What is referred to here as inverse problems are actually a very broad field that encompasses any study where data is gathered and mined for information. Inverse heat transfer theory (Beck, Blackwell, and Clair Jr (1985)) was developed as a way to quantify heat exchange and thermal properties, and has translated well into building physics. Many engineers and researchers however lack the tools for a critical analysis of their results. This caution is particularly important as the dimensionality of the problem (i.e. the number of unknown parameters) increases. When data are available and a model is written to get a better understanding of it, it is very tempting to simply run an optimisation algorithm and assume that the calibrated model has become a sensible representation of reality. If the parameter estimation problem has a relatively low complexity (i.e. few parameters and sufficient measurements), it can be solved without difficulty. In these cases, authors often do not carry a thorough analysis of results, their reliability and ranges of uncertainty. However, it is highly interesting to attempt extracting the most possible information from given data, or to lower the experimental cost required by a given estimation target. System identification then becomes a more demanding task, which cannot be done without proof of reliability of its results. One should not overlook the mathematical challenges of inverse problems which, when added to measurement uncertainty and modelling approximations, can easily result in erroneous inferences. Figure 1.9: Inverse problems in a nutshell Following the formalisation of building energy monitoring shown above, we propose a formalisation of a typical inverse problem for building physics (Rouchier (2018)), without considering any statistical aspects for now. The general principle of solving a system identification problem is to describe an observed phenomenon by a model allowing its simulation. Measurements \\(\\mathbf{z}=(\\mathbf{u},\\mathbf{y})\\) are carried in an experimental setup: a building is probed for the quantities from which we wish to estimate its energy performance (indoor temperature, meter readings, climate, etc.) A model is defined as a mapping between some of the measurements set as input \\(\\mathbf{u}\\) (boundary conditions, weather data) and some as output \\(\\mathbf{y}\\). A numerical model is a mathematical formulation of the outputs \\(\\hat y(u, \\theta)\\), parameterised by a finite set of variables \\(\\theta\\). The most intuitive way to calibrate a model is to minimize an indicator such as the sum of squared residuals with an optimisation algorithm, in order to find the value of \\(\\theta\\) that makes the model most closely match the data. Ideally, the model is unbiased: it accurately describes the behaviour of the system, so that there exists a true value \\(\\theta^*\\) of the parameter vector for which the output \\(\\hat y\\) reproduces the undisturbed value of observed variables. \\[\\begin{equation} \\mathbf{y}_k = \\mathbf{\\hat y}_k(u, \\theta^*) + \\varepsilon_k \\tag{1.1} \\end{equation}\\] where \\(\\varepsilon\\) denotes measurement error, i.e. the difference between the real process \\(y^*\\) and its observed value \\(y\\). The most convenient assumption is that of additive noise, i.e. \\(\\varepsilon_k\\) is a sequence of independent and identically distributed random variables. In practice, \\(\\theta^*\\) will never be reached exactly, but rather approached by an estimator \\(\\hat \\theta\\), because the entire process of estimating it from measurements is disturbed by an array of approximations (Maillet (2010)) Experimental errors. The numerical data \\((u,y)\\) available for model calibration differs from the hypothetical outcome of the ideal, undisturbed physical system \\((u^*,y^*)\\). Sensors may be intrusive, produce noisy measurements, may be poorly calibrated, have a finite precision and resolution Numerical errors. The hypothesis of an unbiased model (Eq. (1.1)) states that there exists a parameter value \\(\\theta^*\\) for which the model output is separated from the observations \\(y\\) only by a zero mean, Gaussian distributed measurement noise. It means that the model perfectly reproduces the physical reality, and the only perceptible error is due to the imperfection of sensors. This is exceedingly optimistic, especially in building energy simulation. Measurement and modelling approximations are problematic because inverse problems are typically ill-posed (Beck, Blackwell, and Clair Jr (1985)): their solution is highly sensitive to noise in the measured data and approximation errors. A global optimum of the inverse problem may then be found with unrealistic physical values for \\(\\theta\\) as a consequence of seemingly moderate errors made when setting up the problem. Figure 1.10: Errors and uncertainties on parameter estimation, caused by measurement and modelling errors We divided source of errors into experimental and numerical errors. The Guide to the expression of Uncertainty in Measurement (GUM)(JCGM (2008)) then separates errors into a random component and a systematic component: systematic errors are errors which retain a non-zero mean if the measurement was repeated an infinite number of times under repeatability conditions. Systematic and random errors, whether they concern the measurement or the modelling procedures, will affect the estimation of a parameter \\(\\theta\\) in terms of accuracy and precision. The figure above illustrates accuracy and precision in the case of estimating a parameter value \\(\\theta\\), but the exact same terminology can be used if the purpose of the trained model is to predict the future values of a variable \\(y\\). The GUM defines uncertainty (of measurement) as the dispersion of the values that could reasonably be attributed to a measured quantity. Similarly, parameter estimates or model predictions come with an uncertainty, which quantifies their possible range of values caused by the random errors in the measurement and modelling processes. Precision is an indicator of low uncertainty, and can be conveyed by confidence intervals. On the other hand, accuracy is a measure of bias. It is the difference between the true value of the target variable and the mean of our estimation. Biased estimates and predictions are the outcome of errors that have not been explicitely taken into account in the inverse problem. We tend to prefer low bias and high uncertainty, than high bias and low uncertainty: indeed, a high uncertainty suggests that the data were not sufficient to provide confident inferences, which incites caution when communicating the results. On the contrary, the bias cannot be simply estimated and is not visible. The worst case scenario is obtaining a bias higher than the uncertainty, which means that the true reference value is not even contained in our confidence interval. By including possible systematic errors in the model formulation, we wish to turn bias into uncertainty. In order to ensure, as much as possible, that the parameters and predictions returned by the model calibration procedure are unbiased and physically interpretable, a complete workflow will be described in the next chapter of this book. This workflow sums up the important steps that should be followed before and after applying the training algorithm itself, and the various tests to be performed to prevent biased conclusions. Statistical modelling will give us the tools to perform such a careful analysis of data. 1.3 Categories of data-driven modelling approaches 1.3.1 Either physical interpretability or prediction accuracy The main subject of this book is to propose a workflow for the analysis of building energy data, that attempts to make the most out of the available data while avoiding the inherent pitfalls of inverse problems. This workflow is described and applied in the parts of the book that follow. Before presenting it, it is however perhaps necessary to clarify some aspects of vocabulary. The previous sections have used expressions that seemed interchangeable, or at least overlapping in their definitions: model calibration; data-driven modelling; statistical learning and inference; inverse problems These terms can describe the same process, or a part of it: collecting data and interpreting them with a numerical or a statistical model, in order to draw conclusions that will support energy conservation measures. There is so much literature on data-driven approaches for forecasting building energy consumption and demand, that several reviews are made every year, and a review of these reviews could be done. One noticeable trend is to classify models into white-box, grey-box and black-box (Deb and Schlueter (2021)), according to their physical interpretability. A classification of data analysis methods and terms is proposed here: models categories from white-box to black-box are shown on a scale of two criteria: physical interpretability and forecasting accuracy. They are then roughly separated into three types of approaches: model calibration (mostly for white-box models), machine learning (black-box) and statistical learning with (grey-box) probabilistic models). Figure 1.11: Data analysis methods can be split into three categories. The first criterion by which methods can be classified is their requirements. These applications shown above essentially have at least one of the following two requirements: Applications that require the ability to accurately forecast the energy use, or any other variable: energy management, optimised predictive control. These applications do not need the trained predictive model to have physically interpretable parameters, or even parameters at all. Applications that involve learning the value of one or more interpretable physical values that describe physical properties of a building. These applications, such as the co-heating test, may be denote performance assessment or characterisation. Some applications require both prediction accuracy and physical interpretability to some extent: measurement and verification, commissioning, fault detection The requirement of data analysis will determine the type of model that will be trained to replicate the data. The type of model is then closely related to the way that inferences will be drawn from it. We can loosely classify data analysis workflows into the three following categories: 1.3.2 Calibrated simulation (white-box) Model calibration usually denotes fitting numerical models which are based on a more or less detailed physical description of the building, complete with a description of HVAC systems and controls, usually without a statistical representation of variables. The advantage of these models is their interpretability: each parameter has a direct meaning, which can be related to thermophysical properties of the envelope or systems. The IPMVP Option D evaluates energy savings by directly adding or removing energy conservation measures in a calibrated numerical model, and therefore requires such a detailed description of the building. Detailed building energy models can be trained to replicate data by manual adjustement of parameters, or by more automated methods (Reddy (2006)). The ASHRAE Guideline 14 specifies what is an acceptable level of accuracy or uncertainty for a calibrated simulation with very permissive criteria: typically, models are declared to be calibrated if they produce Mean Bias Errors (MBE) within 10% and CV(RMSE) within 30% when using hourly data, or 5% MBE and 15% CV(RMSE) with monthly data. Calibrating detailed models however comes with conditions and limitations. A sufficiently detailed model, with enough degrees of freedom, will have no difficulty satisfying the above criteria, but may do so without necessarily assigning their true physical value to each parameter. This inverse problem may have identifiability issues, i.e. the existence of infinitely many combinations of different parameters which result in the same model output. A preliminary sensitivity analysis may be conducted in order to only select the most significant parameters as free for calibration, while fixing the rest. Still, deterministic models greatly underestimate the bias and uncertainty of their own predictions, and calibrated simulation can easily satisfy ASHRAEs validation criterion without representing the true state of a building. 1.3.3 Machine learning (black-box) On the other side of the spectrum, machine learning (ML) is a purely data-driven approach. ML models are not based on physical considerations, but are designed to replicate observed patterns with maximum flexibility and adaptability. The most popular choices of ML methods are Artificial Neural Networks, Support Vector Machines, Boosting and Random Forests. One of the main references on the field is The Elements of Statistical Learning by (Hastie, Tibshirani, and Friedman (2009)). As mentioned earlier, there is enough literature on data-driven building energy modelling to motivate a review of reviews (Amasyali and El-Gohary (2018)), even when only focusing on the machine learning (black-box) side. Because of their lack of physical interpretability, it is difficult for trained ML models to provide insight into thermophysical properties of building components. For this reason, their applications are complementary to the energy model calibration approach mentioned in the previous section. However, they are designed for prediction: they are well suited for forecasting energy demand. ML also comes with standard practices of validation and model order selection, in order to find a good bias-variance tradeoff and ensure accurate predictions. This book will not venture very far into machine learning territory. We will however use Gaussian Process models at some point, by integrating them into other statistical models rather than by themselves. 1.3.4 Statistical modelling and inference (grey-box) Calibrated simulation and machine learning both have advantages and limitations, as they are either appropriate for parameter interpretability or prediction accuracy. This book will focus on the third option: probabilistic modelling and statistical inference. Statistical inference can either follow a frequentist or a Bayesian paradigm: both will be introduced and demonstrated in our applications. Statistical models represent the data-generating process (the building) as a set of statistical assumptions and stochastic processes, rather than deterministic relationships between variables. The formulation of these stochastic processes can be based on physical considerations, like a typical building energy model, except that they explicitely include possible errors and uncertainty. As a result, parameter estimates and predictions are inferred with a certain uncertainty as well, which translates the confidence that our model is able to produce about them. Model checking criteria then allow us to anticipate possible bias: results produced by a thoroughly validated statistical inference procedure are more reliable than deterministic calibrated simulation. Probabilistic modelling starts with the definition of an sampling distribution \\(p\\left(y|\\theta\\right)\\), which is the distribution of the observed data \\(y\\) conditional on the model parameters \\(\\theta\\). When viewed as a function of \\(\\theta\\) for fixed \\(y\\), this distribution is called the likelihood function. Finding the value of \\(\\theta\\) that maximizes the likelihood function is called Maximum Likelihood Estimation, one of the main cases of frequentist inference. Bayesian inference adds a prior probability distribution \\(p(\\theta)\\) to the problem. The prior distribution describes any knowledge we may already have regarding the model parameters, before accounting for the measured data. According to (Gelman et al. (2013)): Bayesian inference is the process of fitting a probability model to a set of data and summarizing the result by a probability distribution on the parameters of the model and on unobserved quantities such as predictions for new observations. Therefore, another specificity of Bayesian inference compared to frequentist inference is the fact that all variables of the problem are described as probability distributions, rather than point estimates. This book is focused on statistical modelling and inference applied to building energy performance assessment. The next chapter will now describe how building physics can be formulated with statistical models, and present a few possible structures for these models. Then, we will propose a full workflow for statistical inference, either frequentist or Bayesian, which aims at making sure that models are well defined and trained for a given application. References "],["modelling.html", "Chapter 2 Building energy statistical modelling 2.1 Building physics in a nutshell 2.2 Measurement and modelling boundaries 2.3 Categories of statistical models", " Chapter 2 Building energy statistical modelling The positioning of this book is statistical modelling and inference, applied to building energy performance assessment. In the previous chapter, this approach was presented as a compromise between physical interpretability of parameter estimates, and flexibility of predictive models. The first step into this approach, before taking data into consideration, is the probabilistic modelling of the energy balance of buildings. Rather than using Building Energy Simulation (BES) software, our approach is generally to start from simple models and gradually increase complexity if required. This means writing each equation individually, and formulating uncertainty into the probabilistic framework. 2.1 Building physics in a nutshell BES decomposes a building into separate thermal zones, each of which is assumed to have a uniform air temperature. A single-family house may be split into one or two zones, while larger housings and office buildings have more, usually denoted by their orientation and usage. The temporal evolution of temperature, humidity and other variables of comfort or indoor air quality, are calculated in each zone as consequences of: influence of the weather; exchange between zones; HVAC settings; occupancy. BES software can reach quite a high level of detail when modelling the phenomena that influence these indoor variables: long-wave radiative heat exchange between all walls; position of the sunspot; influence of furniture on indoor humidity; CFD modelling of air flow Since we will give our building energy models a statistical formulation, these models should stay as simple as possible, while staying close to the main phenomena that govern the heat and energy balance of an observed building. A schematic outlook of heat gains and losses in a heated space is shown here, which we will summarise by three main equations (Fig. 2.1 below assumes the example of a heated building in which gas is the main fuel used by the heating appliances. A similar reasoning can be done in other conditions, such as an air conditioned building in summer). Figure 2.1: Decomposition of heat gains and losses in a heated zone The first equation of our simplified building energy modelling framework is the temporal evolution of the indoor temperature in a thermal zone. It is shown on the right side of Fig. 2.1 by the imbalance between all heat gains (\\(\\Phi_\\mathit{in}\\), red lines) and all heat losses (\\(\\Phi_\\mathit{out}\\), blue lines). \\[\\begin{equation} C \\frac{\\partial T}{\\partial t} = \\Phi_\\mathit{in} - \\Phi_\\mathit{out} \\tag{2.1} \\end{equation}\\] Where \\(C\\) is an effective heat capacity of the thermal zone. The second equation is the breakdown of \\(\\Phi_\\mathit{in}\\), which includes all heat gains of the room other than transmission and ventilation exchanges. This part is shown by red lines on Fig. \\(\\ref{fig:sankey}\\) and will depend on the specificities of each building. A common list of usual heat inputs can be the following. \\[\\begin{equation} \\Phi_\\mathit{in} = \\Phi_h + \\Phi_\\mathit{sol} + \\Phi_\\mathit{int} \\tag{2.2} \\end{equation}\\] \\(\\Phi_h\\): the energy consumption dedicated to space heating. This value is usually not directly measured, but is only a part of a meter reading (e.g. gas) which includes production and distribution losses, and often also cover the production of domestic hot water (DHW). \\(\\Phi_\\mathit{sol}\\): the solar heat gains. They are the outcome of direct and diffuse solar radiation, which are measured outside. The fraction of the total outdoor solar irradiance which is converted into indoor heat input depends on the orientation of the room relative to the position of the sun, the shadings, the type of glazing, etc. \\(\\Phi_\\mathit{int}\\): the sum of other internal heat gains, due to the presence of inhabitants, water heat gains, electrical appliances, lighting, etc. This variable is difficult to measure but tends to show a daily or weekly pattern, which makes it predictable to some extent. The third main equation is the breakdown of \\(\\Phi_\\mathit{out}\\), which includes all heat exchange between the zone under consideration and its surroundings: \\[\\begin{equation} \\Phi_\\mathit{out} = H_\\mathit{tr}^e \\left(T-T_e\\right) + H_\\mathit{tr}^s \\left(T-T_s\\right) + \\sum_j H_\\mathit{tr}^\\mathit{adj,j} \\left(T-T_\\mathit{adj,j}\\right) + \\Phi_\\mathit{inf} + \\Phi_v \\tag{2.3} \\end{equation}\\] The first term \\(H_\\mathit{tr}^e \\left(T_i-T_e\\right)\\) denotes heat loss by direct transmission from the heated room at temperature \\(T\\) to the outside at temperature \\(T_e\\). The \\(H_\\mathit{tr}^e\\) coefficient includes the heat transmissivity of opaque walls, glazing and thermal bridges. The second term \\(H_\\mathit{tr}^g \\left(T_i-T_g\\right)\\) denotes heat loss towards the ground at temperature \\(T_g\\). The third term encompasses heat exchange with all adjacent rooms. This mainly concerns unheated spaces, which may have a significant temperature difference with the thermal zone under consideration. \\(\\Phi_\\mathit{inf}\\) et \\(\\Phi_\\mathit{ven}\\) respectively denote heat loss from air infiltration or mechanical ventilation. These terms are written here in the direction of heat leaving the room, hence the name of the variable \\(\\Phi_\\mathit{out}\\). This is of course just a notation: if the outdoor air temperature or an adjacent room temperature are higher than the zones temperature \\(T\\), some of these terms may very well switch signs, implying that heat is entering the zone. These three first equations (2.1) to (2.3) will be the basis for all our modelling of heat transfer. Fig. 2.1 also illustrates some challenges of statistical inference for building energy performance assessment. An important question is the difficulty to directly observe the terms of Eq. (2.2) that influence the indoor heat balance. On the one hand, some hypotheses are required to formulate the solar heat gains \\(\\Phi_\\mathit{sol}\\) from outdoor measurements of solar irradiance. On the other hand, the internal heat gains \\(\\Phi_\\mathit{int}\\) of occupied buildings are the sum of influences that are hard to measure. Even the heating power \\(\\Phi_h\\) is usually not directly available. Still in the example of a building equipped with a hydronic heating system fueled by gas, a typical situation is having a common meter for all of gas consumption. \\[\\begin{equation} e_\\mathit{gas}(t) = e_\\mathit{dhw}(t) + e_\\mathit{sh}(t) + e_\\mathit{loss}(t) \\tag{2.4} \\end{equation}\\] The consumption intended for domestic hot water production \\(e_\\mathit{dhw}\\) and space heating \\(e_\\mathit{sh}\\) either need to be metered separately, or to be disaggregated from a single meter \\(e_\\mathit{gas}\\). Then, the energy consumption for space heating \\(e_\\mathit{sh}\\) translates to the heating power \\(\\Phi_h\\) (see Eq. (2.2)) through assumptions regarding the heating system. If the target of a study is the performance assessment of the buiding envelope from measurements of heating power and temperatures, then different strategies will be required if \\(\\Phi_h\\) is somehow directly measured, than if only a general meter for \\(e_\\mathit{gas}\\) is available. The proper formulation of Eq. (2.2) therefore requires assumptions to translate outdoor solar irradiance measurements into \\(\\Phi_\\mathit{sol}\\), and assumptions to translate energy meter readings into \\(\\Phi_h\\). The second challenge we mention here is the prediction of electricity consumption, and eventually its impact on the indoor heat balance. Supposing that hourly or daily measurements of electricity consumption are available, one can be interested in either: identifying repetitive patterns which makes this consumption predictable for purposes of energy distribution management; estimating the fraction of this energy use that contributes to indoor heat gains \\(\\Phi_\\mathit{int}\\). The former question is closely related to the detection and data-driven modelling of occupancy, which is a ML problem that may be based on a variety of sensors and methods. The second question is even more challenging, as it requires not only an estimation of the use of each appliance, but also of their heat loss percentage. The third challenge displayed on Fig. 2.1 is the decomposition of the heat loss of the envelope. The first three terms on the right side of Eq.(2.3) may be aggregated in order to define two global indicators of the heat performance of the envelope: the total Heat Transfer Coefficient (HTC) and the transmission coefficient \\(H_\\mathit{tr}\\). \\[\\begin{align} \\Phi_\\mathit{out} &amp; = \\underbrace{H_\\mathit{tr} \\left(T-T_e\\right) + \\Phi_\\mathit{inf}}_{\\mathrm{HTC} \\left(T-T_e\\right)} + \\Phi_v \\\\ \\mathrm{HTC} &amp; = H_\\mathit{tr} + \\Phi_\\mathit{inf}/\\left(T-T_e\\right) \\tag{2.5} \\end{align}\\] The \\(H_\\mathit{tr}\\) coefficient describes all heat transmission through the envelope, and the HTC also includes the effect of air infiltration. Controlled mechanical ventilation is not included in these coefficients, but may as well be considered as part of the heat gains in Eq. (2.2). One of the essential questions of this book will be the characterisation of HTC and \\(H_\\mathit{tr}\\), using short-term or long-term measurements that may be recorded without disturbing the normal operation of the building. 2.2 Measurement and modelling boundaries The first step into setting up a probability model is the choice of its boundaries, i.e. which of the measured data is the dependent variable, and which are the explanatory variables. The dependent variable \\(y\\), or model output, is a variable that we wish for a fitted model to be able to predict (this book does not cover situations with several dependent variables in a single model). The explanatory variables, or independent variables, are the model inputs by which we try to explain the evolutions of the dependent variable. Explanatory variables are denoted \\(x\\) in most regression models, or \\(u\\) in more complex hierarchical models where \\(x\\) may denote a latent variable instead. Some models have latent variables, which are unobserved and affect the dependent variable. The IPMVP defines measurement boundaries as notional boundaries drawn around equipment, systems or facilities to segregate those which are relevant to saving determination from those which are not. All Energy Consumption and Demand of equipment or systems within the boundary must be measured or estimated. [] Any energy effects occurring beyond the selected measurement boundary are called interactive effects. The magnitude of any interactive effects needs to be estimated or evaluated to determine savings associated with the ECMs. The same definition of boundaries work for simulations: a model must be defined so that its inputs and outputs are the measured independent and dependent variables, and all energy effects occurring within these boundaries are either fixed, or part of the list of parameters \\(\\theta\\) that will be estimated by calibration. Figure 2.2: Building energy models simulate the interactions between envelope, ambiance, HVAC systems and their controls, described by a finite set of parameters. The explanatory variables are related to the two conditions mentioned earlier: weather and occupants. The dependent variables are separate energy consumptions. Typical modelling boundaries of BES resemble Fig. 2.2. The time-varying inputs provided by the user are weather files and occupancy profiles. Most of the time, the latter come from standard scenarios rather than measurement. Occupancy is understood by BES as a finite set of actions and influences: presence, temperature set-points, use of appliances. The model returns predictions of energy use, usually with a higher level of disaggregation (consumption of each system) than is easily available by measurement. Other models can have the indoor temperature as output (Fig. 2.3): for instance, heat transfer simulation models used for assessing the performance of the envelope, or for tuning model predictive control strategies. Figure 2.3: Assessing the performance of the envelope may involve models with the indoor temperature as dependent variable. This example illustrates a problem: the boiler that produces space heating and domestic hot water can be beyond the measurement boundary, and only the global consumption is observed. If the target of a study is to characterise some parameter \\(\\theta\\), or evaluate the evolution of a latent variable, rather than train a predictive model, then the same dataset \\(\\mathcal{D}\\) can be mapped into input and output variables in different ways. Regardless of this choice, the principles of the above definition of measurement boundaries should be applied to modelling: any effects that are believed to influence the dependent variable should be either measured (explanatory variables), given assumed values (interactive effects), or estimated by fitting. 2.3 Categories of statistical models Once the modelling boundaries are set, the next step is choosing the model structure itself, i.e. the equations that relate dependent variables to independent variables and eventual latent variables. The next few sections will describe the different categories of models that will be implemented later in the applications, also summarized by Fig. 2.4. Before getting to these descriptions, a summary of some notations and vocabulary they have in common might be helpful. Figure 2.4: This book separates statistical models in categories according to two questions: whether the time resolution of data is high enough so that temporal dependencies should be accounted for; whether there is a direct relationship between dependent and independent variables or an indirect relationship through latent variables. Gaussian Process models will be presented separately because they can apply to most situations. A regression model directly relates the dependent variable with one or several explanatory variables \\(x\\) and its parameters \\(\\theta\\). Regression concerns dependent variables with continuous values, as opposed to classification which concerns categorical or discrete dependent variables. \\[\\begin{equation} y \\sim f\\left(x, \\theta\\right) \\tag{2.6} \\end{equation}\\] In this form, regression models assumes the independence of all elements of \\(y\\) with each other: each measurement is unaffected by its previous value. These models are therefore to be used with low frequency or aggregated data for long-term predictions or summaries. Some problems can be modelled hierarchically, with observable outcomes modeled conditionally on certain parameters \\(\\theta\\), which themselves are given a probabilistic specification in terms of further parameters \\(\\phi\\), known as hyperparameters. \\[\\begin{equation} \\theta \\sim f\\left(\\phi\\right) \\tag{2.7} \\end{equation}\\] The hyperparameter \\(\\phi\\) can then be given a hyperprior distribution \\(p\\left(\\phi\\right)\\). As written by (Gelman et al. (2013)), simple nonhierarchical models are usually inappropriate for hierarchical data: with few parameters, they generally cannot fit large datasets accurately, whereas with many parameters, they tend to overfit such data []. In contrast, hierarchical models can have enough parameters to fit the data well, while using a population distribution to structure some dependence into the parameters, thereby avoiding problems of overfitting. Hierarchical thinking gives flexibility to simple model structures, and will be useful to explain data from a group of buildings, or from a single building monitored over several operating conditions. Data often comes at high enough frequency so that consecutive measurements of the outcome variable cannot be considered independent from each other. Time-series models offer many ways to express this dependency Shumway and Stoffer (2000) \\[\\begin{equation} y_t \\sim f\\left(y_{t-1}, y_{t-2},..., x, \\theta\\right) \\tag{2.8} \\end{equation}\\] This type of model is called autoregressive because of the similarity of this formulation with the regression model of Eq. (2.6): the dependent variable at time \\(t\\) is a regression function of its previous values. The simplest autoregressive models lack explanatory variables \\(x\\), and only formulate the dependent variable as a regression function of its previous values. They can be used to identify trends and repetitive cycles in a single variable and predict its future values. The last criterion we are considering for classifying models by categories, is the presence of latent variables. A latent variable model is a hierarchical model which relates the observed dependent variable to a set of unobservable latent variables. For each outcome \\(y_n\\) there is a latent variable \\(z_n\\) in \\(\\left\\{1,...,K\\right\\}\\) with a categorical distribution parameterized by some parameter \\(\\lambda\\). \\[\\begin{align} y_n \\sim f(z_n) \\\\ z_n \\sim \\mathrm{categorical}(\\lambda) \\tag{2.9} \\end{align}\\] Finite mixture models can be parameterized as latent variable models, although the description we will make of them does not explicitely display latent variables. The models which will play the largest role in the next few chapters of this book are time series models with latent variables, or state-space models (SSM). \\[\\begin{align} y_t \\sim f\\left(z_t, \\theta\\right) \\\\ z_t \\sim f\\left(z_{t-1}, \\theta\\right) \\tag{2.10} \\end{align}\\] An SSM is a type of Dynamic Bayesian Network (DBN)(Murphy (2002)) where an underlying hidden state \\(z_t\\), generates the observations \\(y_t\\). The state evolves in time as a function of observable inputs. State-space models expand the classical time-series modelling approaches by allowing more complex assumptions on the evolution of the system and its uncertainty. A Hidden Markov Model (HMM) is a type of DBN whose hidden state takes discrete values. We will use the term of state-space model for models whose hidden states are continuous, although some authors call them Kalman filter models (KFM). DBNs with both categorical and continuous hidden states are called switching dynamic systems or switching Kalman filter models, and are the highest complexity we will consider in this book. References "],["workflow.html", "Chapter 3 A Bayesian data analysis workflow 3.1 Bayesian inference summarised 3.2 Workflow for one model 3.3 Model assessment and selection", " Chapter 3 A Bayesian data analysis workflow 3.1 Bayesian inference summarised 3.1.1 Motivation for a Bayesian approach Bayesian statistics are mentioned in the Annex B of the ASHRAE Guideline 14, after it has been observed that standard approaches make it difficult to estimate the savings uncertainty when complex models are required in a measurement and verification worflow: Savings uncertainty can only be determined exactly when energy use is a linear function of some independent variable(s). For more complicated models of energy use, such as changepoint models, and for data with serially autocorrelated errors, approximate formulas must be used. These approximations provide reasonable accuracy when compared with simulated data, but in general it is difficult to determine their accuracy in any given situation. One alternative method for determining savings uncertainty to any desired degree of accuracy is to use a Bayesian approach. Still on the topic of measurement and verification, and the estimation of savings uncertainty, several advantages and drawbacks of Bayesian approaches are described by (Carstens, Xia, and Yadavalli (2018)). Advantages include: Because Bayesian models are probabilistic, uncertainty is automatically and exactly quantified. Confidence intervals can be interpreted in the way most people understand them: degrees of belief about the value of the parameter. Bayesian models are more universal and flexible than standard methods. Models are also modular and can be designed to suit the problem. For example, it is no different to create terms for serial correlation, or heteroscedasticity (non-constant variance) than it is to specify an ordinary linear model. The Bayesian approach allows for the incorporation of prior information where appropriate. When the savings need to be calculated for normalised conditions, for example, a typical meteorological year, rather than the conditions during the post-retrofit monitoring period, it is not possible to quantify uncertainty using current methods. However, (Shonder and Im (2012)) have shown that it can be naturally and easily quantified using the Bayesian approach. The first two points above are the most relevant to a data analyst: any arbitrary model structure can be defined to explain the data, and the exact same set of formulas can then be used to obtain any uncertainty after the models have been fitted. 3.1.2 General Bayesian principles A Bayesian model is defined by two components: An observational model \\(p\\left(y|\\theta\\right)\\), or likelihood function, which describes the relationship between the data \\(y\\) and the model parameters \\(\\theta\\). A prior model \\(p(\\theta)\\) which encodes eventual assumptions regarding model parameters, independently of the observed data. Specifying prior densities is not mandatory. The target of Bayesian inference is the estimation of the posterior density \\(p\\left(\\theta|y\\right)\\), i.e. the probability distribution of the parameters conditioned on the observed data. As a consequence of Bayes rule, the posterior is proportional to the product of the two previous densities: \\[\\begin{equation} p(\\theta|y) \\propto p(y|\\theta) p(\\theta) \\tag{3.1} \\end{equation}\\] This formula can be interpreted as follows: the posterior density is a compromise between assumptions and evidence brought by data. The prior can be strong or weak, to reflect for a more or less confident prior knowledge. The posterior will stray away from the prior as more data is introduced. Figure 3.1: Example of estimating a set point temperature after assuming a Normal prior distribution centred around 20°C. The dashed line is the point estimate which would have been obtained if only the data had been considered. The posterior distribution can be seen as a refinement of the prior, given the evidence of the data. In general, information from the posterior distribution is represented by summary statistics such as the mean, variance or credible intervals, which can be used to inform decisions and are easier to interpret than the full posterior distribution. Most of these summary statistics take the form of posterior expectation values of certain functions, \\(f(\\theta)\\), \\[\\begin{equation} \\label{posterior_expectation} \\mathbb{E}[f(\\theta)] = \\int p(\\theta|y) f(\\theta) \\mathrm{d} \\theta \\tag{3.2} \\end{equation}\\] More sophisticated questions are answered with expectation values of custom functions \\(f(\\theta)\\). One common example is the posterior predictive distribution: in many applications, one is not only interested in estimating parameter values, but also the predictions \\(\\tilde{y}\\) of the observable during a new period. The distribution of \\(\\tilde{y}\\) conditioned on the observed data \\(y\\) is called the posterior predictive distribution: \\[\\begin{equation} p\\left(\\tilde{y}|y\\right) = \\int p\\left(\\tilde{y}|\\theta\\right) p\\left(\\theta|y\\right) \\mathrm{d}\\theta \\tag{3.3} \\end{equation}\\] The posterior predictive distribution is an average of the model predictions over the posterior distribution of \\(\\theta\\). This formula is equivalent to the concept of using a trained model for prediction. Apart from the possibility to define prior distributions, the main specificity of Bayesian analysis is the fact that all variables are encoded as probability densities. The two main results, the parameter posterior \\(p(\\theta|y)\\) and the posterior prediction \\(p\\left(\\tilde{y}|y\\right)\\), are not only point estimates but complete distributions which include a full description of their uncertainty. 3.2 Workflow for one model 3.2.1 Overview As was mentioned in Sec. 1.2.4, inverse problems are all but trivial. It is possible that the available data is simply insufficient to bring useful inferences, but that we still try to train an unsuitable model with it. Statistical analysts need the right tools to guide model selection and training, and to warn them when there is a risk of biased inferences and predictions. This chapter is an attempt to summarize the essential points of a Bayesian workflow from a building energy perspective. Frequentist inference is also mentioned, but as a particular case of Bayesian inference. There is a very rich literature on the proper workflow for statistical inference, including the most cited book in this report (Gelman et al. (2013)) and extensive online tutorials. Gelman divides the process of Bayesian data analysis into three steps: Setting up a full probability model; Conditioning on observed data (learning); Evaluating the fit of the model and the implications of the resulting posterior (checking and validation). Figure 3.2: A workflow for the proper specification and training of one model. Most of the workflow is similar for frequentist and Bayesian inference. Fig. 3.2 gives an overview of these three steps, which will be detailed in the present section. An additional step of preliminary analyses may be included to Gelmans formulation of the model definition: sensitivity analysis and identifiability analysis are two categories of methods which may prevent over-parameterisation and ill-posedness of the inverse problem. Their outcome may incite to reformulate the probability model. This process concerns the training of a single model. An analyst however rarely attempts to analyze data with a single model. A simple model will provide biased inferences and predictions if its structure is too simple to represent the real system. In a complex model with many degrees of freedom, the unicity of the solution to the inverse problem is not guaranteed, leading to non-robust inferences and overfitting. All statistical learning lectures therefore come with guidelines for model checking, assessment and selection: this will be explained in Sec. 3.3. 3.2.2 Step 1: model specification The first step into building our model is a conceptual analysis of the system and the available data. The first question is to decide what we want to learn from the data, and is related to the choice of measurement and modelling boundaries mentioned in Sec. 2.2, and the choice of model structure mentioned in Sec. 2.3: which of the measurements is the dependent variable \\(y\\), which are the relevant explanatory variables \\(x\\), and how will the model parameters \\(\\theta\\) be defined. In Sec. 1.3, we have roughly separated the analyses in two categories: prediction and inference. If the main goal is prediction (of energy use, occupation, ambient variables) then the choice of the dependent variable is straightforward, but there is a lot of freedom in the choice of explanatory variables and model structure \\(p(y|\\theta)\\). If the main goal is inference (of some energy performance index such as HTC) then the choice of modelling boundaries \\(x\\) and \\(y\\) is not trivial, but the parameterisation of the model will be constrained so that \\(\\theta\\) can be related to the inference goal. In both situations, the model definition is greatly impacted by the time resolution and length of the dataset. Higher time resolutions (under an hour) enable the choice of dynamical models, which can encode more inferential information but imply a more complex development. Longer datasets (several months) enable the aggregation of data over longer resolutions and observations covering different weather conditions. The next step is the development of the model: the translation of the conceptual narrative of the system into formal mathematical descriptions. The target is to formulate the entire system into probabilities that our fitting method can work with. In the case of simple regression models, the observational model may be summarized by a single likelihood function \\(p(y|\\theta)\\), eventually conditioned on explanatory variables. If the practitioner wishes to use a regression model to explain the relationship between the parameters and the data, doing so in a Bayesian framework is very similar to the usual (frequentist) framework. As an example, a Bayesian model for linear regression with three parameters \\((\\theta_0,\\theta_1,\\theta_2)\\) and two explanatory variables \\((X_1,X_2)\\) may read: \\[\\begin{align} p(y|\\theta,X) &amp; = N\\left(\\theta_0 + \\theta_1 X_1 + \\theta_2 X_2, \\sigma\\right) \\tag{3.4} \\\\ p(\\theta_i) &amp; = \\Gamma(\\alpha_i, \\beta_i) \\tag{3.5} \\end{align}\\] This means that \\(y\\) follows a Normal distribution whose expectation is a linear function of \\(\\theta\\) and \\(X\\), with standard deviation \\(\\sigma\\) (the measurement error). The second equation is the prior model: in this example, each parameter is assigned a Gamma prior distribution parameterised by a shape \\(\\alpha\\) and a scale \\(\\beta\\). Other model structures can be formulated similarly: change-point models, polynomials, models with categorical variables Bayesian modelling however allows for much more flexibility: Other distributions than the Normal distribution can be used in the observational model; Hierarchical modelling is possible: parameters can be assigned a prior distribution with parameters which have their own (hyper)prior distribution; Heteroscedasticity can be encoded by assuming a relationship between the error term and explanatory variables, etc. More complex models with latent variables have separate expressions for the respective conditional probabilities of the observations \\(y\\), latent variables \\(z\\) and parameters \\(\\theta\\). In this case, there is a likelihood function \\(p(y,z|\\theta)\\) and a marginal likelihood function \\(p(y|\\theta)\\) so that: \\[\\begin{equation} p(y|\\theta) = \\int p(y,z|\\theta) \\mathrm{d}z \\tag{3.6} \\end{equation}\\] Other applications, such as the IPMVP option D, rely on the use of calibrated building energy simulation (BES) models. These models are described by a much larger number of parameters and equations that the simple regression models typically used for other IPMVP options. In this context, it is not feasible to fully describe BES models in the form of a simple likelihood function \\(p(y|\\theta)\\). In order to apply Bayesian uncertainty analysis to a BES model, it is possible to first approximate it with a Gaussian process (GP) model emulator. This process is denoted Bayesian calibration and was based on the seminal work of Kennedy and OHagan (Kennedy and OHagan (2001)). As opposed to the manual adjustment of building energy model parameters, Bayesian calibration explicitly quantifies uncertainties in calibration parameters, discrepancies between model predictions and observed values, as well as observation errors (Chong and Menberg (2018)). 3.2.3 Prior predictive checking The full probability model is the formalization of many assumptions regarding the data-generating process. In theory, the model can be formulated only based on domain expertise, regardless of the data. In practice, a model which is inconsistent with the data has little chance to yield informative inferences after training. The prior predictive distribution, or marginal distribution of observations \\(p(y)\\), is a way to check for the consistency of our expertise. \\[\\begin{equation} p\\left(y\\right) = \\int p\\left(y|\\theta\\right) p\\left(\\theta\\right) \\mathrm{d}\\theta \\tag{3.7} \\end{equation}\\] Basically, computing this distribution is equivalent to running a few simulations of a numerical model before its training, with some assumed values of the parameters. In Bayesian terms, we first draw a finite number of parameter vectors \\(\\tilde{\\theta}^{(m)}\\) from the prior distribution, and use each of them to compute a model output \\(\\tilde{y}^{(m)}\\): \\[\\begin{align} \\tilde{\\theta}^{(m)} &amp; \\sim p(\\theta) \\tag{3.8} \\\\ \\tilde{y}^{(m)} &amp; \\sim p(y|\\tilde{\\theta}^{(m)}) \\tag{3.9} \\end{align}\\] This set of model outputs approximates the prior probability distribution. If large inconsistencies can be spotted between this distribution and measurements, we can adjust some assumptions regarding the prior definition or the structure of the observational model. An example of prior predictive checking is shown on the top right of Fig. 3.2, which suggests a model structures which does not contradict the observations. 3.2.4 Step 2: computation with Markov Chain Monte Carlo Except in a few convenient situations, the posterior distribution is not analytically tractable. In practice, rather than finding an exact solution for it, it is estimated by approximate methods. The most popular option for approximate posterior inference are Markov Chain Monte Carlo (MCMC) sampling methods. When it is not possible or not computationally efficient to sample directly from the posterior distribution, Markov Chain simulation is used to stochastically explore the typical set, i.e. the regions of parameter space which have a significant contribution to the desired expectations. Markov chains used in MCMC methods are designed so that their stationary distribution is the posterior distribution. If the chain is long enough, the state history of the chain provides samples from the typical set \\(\\left(\\theta^{(1)},...,\\theta^{(S)}\\right)\\) \\[\\begin{equation} \\theta^{(s)} \\sim p(\\theta | y) \\tag{3.10} \\end{equation}\\] where each draw \\(\\theta^{(s)}\\) contains a value for each of the parameters of the model. Commonly used MCMC algorithms, such as Metropolis-Hastings or Gibbs sampler, are inefficient in high-dimension because of their random walk behavior. As the dimension increases, the typical set becomes narrower and good guesses become rarer; the Markov chain may get stuck for a potentially long time. Hamiltonian Monte Carlo (HMC) algorithm alleviates this issue by exploiting information about the geometry of the typical set (Betancourt (2017)). The HMC algorithm supresses the random walk behavior by borrowing an idea from physics. Metaphorically, the vector of parameters represents the position of a frictionless particle which follows a physical path determined by the curvature of the posterior distribution. Furthermore, the gradient of the logarithm of the posterior distribution guides the Markov chain along regions of high probability mass which provides an efficient exploration of the typical set. HMC is a state-of-the-art algorithm for Bayesian inference and is made available by software libraries such as the STAN language. Applications of HMC to the calibration of building energy models include whole building simulation (Chong and Menberg (2018)) and state-space models (Lundström and Akander (2020)). Posterior expectation values can be accurately estimated by Monte Carlo estimator. Based on exact independent random samples from the posterior distribution \\(\\left(\\theta^{(1)},...,\\theta^{(S)}\\right) \\sim p(\\theta | y)\\), the expectation of any function \\(f(\\theta)\\) can be estimated with \\[\\begin{equation} \\mathbb{E}[f(\\theta)] \\approx \\frac{1}{N} \\sum_{n=1}^{N} f(\\theta)^{(n)} \\tag{3.11} \\end{equation}\\] Markov Chain Monte Carlo estimators converge to the true expectation values as the number of draws approaches infinity. In practice, diagnostics must be applied to check that the estimator follows the central limit theorem, which ensures that the estimator is unbiased after a finite number of draws. For that purpose it is first recommended to compute the (split-)\\(\\hat{R}\\) statistic, or Gelman-Rubin statistic, with multiple chains initialized at different initial positions and split into two halves (Gelman et al. (2013)). The \\(\\hat{R}\\) statistic measures for each scalar parameter, \\(\\theta\\), the ratio of samples variance within each chain \\(W\\) to the sample variance of all combined chains \\(B\\), \\[\\begin{equation} \\hat{R} = \\sqrt{\\frac{1}{W} \\left(\\frac{N-1}{N}W + \\frac{1}{N}B \\right)} \\tag{3.12} \\end{equation}\\] where \\(N\\) is the number of samples. If the chains have not converged, \\(W\\) will underestimate the variance, since the individual chains have not had time to range all over the stationary distribution, and \\(B\\) will overestimate the variance, since the starting positions were chosen to be overdispersed. Another important convergence diagnostics tool is the effective sample size (ESS), defined as: \\[\\begin{equation} \\text{ESS} = \\frac{N}{1 + 2 \\sum_{l=1}^{\\infty} \\rho_l} \\tag{3.13} \\end{equation}\\] with \\(\\rho_l\\) the lag-\\(l\\) autocorrelation of a function \\(f\\) over the history of the Markov chain. The effective sample size is an estimate of the number of independent samples from the posterior distribution. The diagnostic tools introduced in this section provide a principled workflow for reliable Bayesian inferences. They are readily available in most Bayesian computation libraries. Based on the recent improvements to the \\(\\hat{R}\\) statistic (Vehtari et al. (2021)), it is recommended to use the samples only if \\(\\hat{R} &lt; 1.01\\) and \\(\\text{ESS} &gt; 400\\). 3.2.5 Step 3: model checking and validation After model specification and learning, the third step of the workflow is model checking and validation. It should be conducted before any conclusions are drawn, and before the prediction accuracy of the model is estimated. The posterior predictive distribution The basic way of checking the fit of a model to data is to draw simulated values from the trained model and compare them to the observed data. In a non-Bayesian framework, we would pick the most likely point estimate of parameters, such as the maximum likelihood estimate, use it to compute the model output \\(\\hat{y}\\), and conduct residual analysis. In a Bayesian framework, posterior predictive checking allows some more possibilities. The posterior predictive distribution is the distribution of the observable \\(\\tilde{y}\\) (the model output) conditioned on the observed data \\(y\\): \\[\\begin{equation} p\\left(\\tilde{y}|y\\right) = \\int p\\left(\\tilde{y}|\\theta\\right) p\\left(\\theta | y\\right) \\mathrm{d}\\theta \\tag{3.14} \\end{equation}\\] This definition is very similar to the prior predictive distribution given in Sec. 3.2.3, except that the prior \\(p(\\theta)\\) has been replaced by the posterior \\(p(\\theta|y)\\). Similarly, it is simple to compute if the posterior has been approximated by an MCMC procedure: we first draw a finite number of parameter vectors \\(\\theta^{(m)}\\) from the posterior distribution, and use each of them to compute a model output \\(\\tilde{y}^{(m)}\\): \\[\\begin{align} \\theta^{(m)} &amp; \\sim p(\\theta|y) \\tag{3.15}\\\\ \\tilde{y}^{(m)} &amp; \\sim p(y|\\theta^{(m)}) \\tag{3.16} \\end{align}\\] This set of model outputs approximates the posterior probability distribution. The following methods of model checking may apply to either a frequentist or a Bayesian framework. If the fitting returns a point estimate of parameters \\(\\hat{\\theta}\\), then a single profile of model output \\(\\hat{y}\\) can be calculated from it, either to be compared with the training data set \\(y_\\mathit{train}\\) or to a separate test data set \\(y_\\mathit{test}\\). If the fitting returns a posterior distribution \\(p(y|\\theta)\\), the same comparisons may be applied to any of the \\(\\tilde{y}^{(m)}\\) samples. Measures of model adequacy After fitting, either ordinary linear regression or more sophisticated ones, some metrics may assess the predictive accuracy of the model. The R-squared (\\(R^2\\)) index is the proportion of the variance of the dependent variable that is explained by the regression model (closest to 1 is better) \\[\\begin{equation} R^2 = 1-\\frac{\\sum_{i=1}^N\\left(y_i - \\hat{y}_i\\right)^2}{\\sum_{i=1}^N\\left(y_i - \\bar{y}_i\\right)^2} \\tag{3.17} \\end{equation}\\] The root-mean-square-error (RMSE) simply measures the differences between model predictions \\(\\hat{y}\\) and observations \\(y\\) (lower is better) \\[\\begin{equation} \\mathrm{CV(RMSE)} = \\frac{1}{\\bar{y}} \\sqrt{\\frac{\\sum_{i=1}^N\\left(\\hat{y}_i-y_i\\right)^2}{N}} \\tag{3.18} \\end{equation}\\] Coverage Width-based Criterion (CWC) (Chong, Augenbroe, and Yan (2021)), an indicator for probabilistic forecasts which measures the quality of the predictions based on both their accuracy and precision. The \\(R^2\\) and CV-RMSE indices are too often treated as validation metrics. If they are calculated using a test data set, they can indeed estimate the model predictive ability outside of the training data set. They however do not ensure that the model correctly captures the data generating process: this is what residual analysis is for. Residual analysis The hypothesis of an unbiased model assumes that the difference between the model output and the observed temperature is a sequence of independent, identically distributed variables following a Gaussian distribution with zero mean and constant covariance. In the example of a linear regression model, this condition may read: \\[\\begin{equation} r_i = y_i - \\left( \\hat{\\theta}_0 + \\hat{\\theta}_1 X_{i,1} + \\hat{\\theta}_2 X_{i,2} \\right) \\sim N(0,\\sigma) \\tag{3.19} \\end{equation}\\] where \\(r_i\\) are the prediction residuals. Residual analysis is the process of checking the validity of their four hypotheses (independence, identical distribution, zero mean, constant variance), and the main step of model validation. It allows identifying problems that may arise after fitting a regression model (James et al. (2013)), among which: correlation of error terms, outliers, high leverage points, colinearity Figure 3.3: Example of residual plots after an ordinary linear regression in R: fitted vs residuals, Q-Q plot, scale location and residuals vs leverage Residual analysis can be performed by an array of tests and graphs, some of which are shown on Fig. 3.3. A simple plot of the residuals versus the model output should not display any trend. The same goes for a plot of residuals vs any of the explanatory variables. Should a trend be visible, the model structure is probably insufficient to explain the data. A quantile-quantile (Q-Q) plot (upper right) is a way to check if the distribution of residuals is approximately Gaussian The scale-location plot (lower left) is a way to check the hypothesis of homoskedasticity, i.e. constant variance The residuals vs leverage plot (lower right) allows identifying eventual outliers and high leverage points. Figure 3.4: Autocorrelation function (top) and cumulated periodogram (bottom) of an insufficient model (left) and a sufficient model (right) Most importantly, the correlation among the error terms should be checked. The autocorrelation function (ACF) checks the independence of residuals and may reveal lag dependencies which suggest influences that the model does not properly take into account. This is particularly important for time series models, and therefore well explained the time series litterature (Shumway and Stoffer (2000)). Alternatively, the Durbin-Watson test quantitatively checks for autocorrelation in regression models. If there is correlation among the error terms, then the estimated standard errors will tend to underestimate to true standard errors. As a result, confidence and prediction intervals will be narrower than they should be.(James et al. (2013)) If residuals display unequal variances or correlations, then the inferences and predictions of the fitted model should not be used. The model should be modified and re-trained according to the practitioners expertise and diagnostics of the analysis: additional explanatory variables can be included if possible. Posterior predictive checking Posterior predictive checking uses global summaries to check the joint posterior predictive distribution \\(p\\left(\\tilde{y}|y\\right)\\). Lack of fit of the data with respect to the posterior predictive distribution can be measured by the tail-area probability, or \\(p\\)-value, of a test quantity, and computed using posterior simulations of \\((\\theta, \\tilde{y})\\). The reader is referred to the book of Gelman et al for this Bayesian treatment of model checking. 3.3 Model assessment and selection Once a model has passed the validation criteria, we may assume that its structure is sufficient to explain the main mechanics of the data generating process. However, this does not ensure that this model is the most appropriate one to draw inferences from, or to use for future predictions. While validation metrics impose a lower bound on the necessary model complexity, there are upper bounds to it imposed by: the practical identifiability of parameters; the risk of overfitting and wrong predictions. 3.3.1 Model selection workflows The single-model training and validation workflow shown on Fig. 3.2 is embedded in a larger process for the selection of the appropriate model complexity and structure. Sec. 3.2.5 has shown the validation metrics that models should pass before their results are used. These conditions impose a lower bound on the acceptable model complexity. Then, the following sections will discuss that the model complexity should have an upper bound as well. Figure 3.5: A workflow of gradually increasing model complexity: models of gradually increasing complexity must pass validation checks. The ones that do are compared in terms of predictive accuracy. Two alternatives are shown on Fig. 3.5 and 3.6 for model selection among several possibilities of structures and complexities. The first one is the most common and intuitive: fitting models of gradually increasing complexity, keeping the ones that pass our validation checks, and comparing them in terms of predictive performance criteria. Inferences from simpler models may serve as starting values for more complex ones. This forward stepwise selection is suited to find a simple a robust model for prediction. Figure 3.6: A workflow of gradually decreasing model complexity. The second alternative, on Fig. 3.6, is backward selection: starting from a high number of predictors and fitting models of decreasing complexity. This approach is more suitable if the target is simply the estimation of parameter values: we try to find the upper bound of the information that can be learned from the data, and decrease our expectations in case of practical non-identifiability. 3.3.2 Sensitivity analysis It is possible to filter out parameters that are unlikely to be learned from the data by running a preliminary sensitivity analysis. Parameter estimation algorithms are often computationally expensive and their cost quickly rises with the number of parameters of the model. This is a motivation for excluding parameters with little influence on the output, especially if we expect their posterior distributions to be close to their prior. Sensitivity analysis is the main mathematical tool for the purpose of identifying the physical phenomena that can be really tested on the available experimental data. It measures the effects of parameter variations on the behaviour of a system and allows two things: ranking parameters by their significance so that non-influencial parameters may be filtered out, and identifying correlations between parameters which may prevent their estimation. Many local and global sensitivity analysis methods are applicable, providing first-order and total-order sensitivity indices from which correlations can be assessed: differential sensitivity analysis calculates the sensitivity of the model output to each parameter locally. Sampling-based methods (variance-based and one-at-a-time methods) allow a global sensitivity analysis but are more computationally intensive. Variance-based methods (Iooss and Lemaître (2015)) decompose the total variance of the output into a sum of the partial variances representing the marginal effect of each input parameter independently. The sensitivity indices are the ratio of each variance to the total variance of the output. To calculate the indices, the variance of the output is obtained thorough sampling of the input space: although significantly costly, Monte-Carlo sampling methods enable to calculate first order and total indices. The state-of-the art RBD-FAST method (Goffart and Woloszyn (2021)) greatly mitigates the computational cost of variance-based methods. As it does not require a specific sampling scheme, it can be coupled to uncertainty analysis for no additional cost. RBD-FAST is one of the available methods in the SAlib python library. 3.3.3 Structural identifiability Identifiability is the property of unicity and existence of a solution to an inverse problem. The usual definition of identifiability originates from (Bellman and Åström (1970)). This notion was originally predominantly developed to help understanding complex biological systems, each of which is modelled by a specific set of differential equations with unobservable parameters. The question of identifiability is whether the input-output relation of the system may be explained by a unique parameter combination \\(\\theta\\). \\[\\begin{equation} y(\\theta) = y(\\tilde{\\theta}) \\Rightarrow \\theta = \\tilde{\\theta} \\tag{3.20} \\end{equation}\\] Two conditions are required for the parameter estimates to be identifiable: the model structure must allow for parameters to be theoretically distinguishible from one another, with no redundancy; the data must be informative so that parameter uncertainty is not prohibitively high after identification. These conditions are respectively denoted structural and practical identifiability. Figure 3.7: The RC thermal model of a house with no indoor heat input may be non-identifiable: only the product of resistance and capacitance is identifiable, resulting in infinitely many possible solutions for individual parameters. Structural identifiability* is a purely mathematical property of the model structure and is an absolute necessary condition before applying data to a model. In a non-identifiable model, an infinity of posible parameter combinations may yield identical likelihoods. The Ph.D. thesis of Sarah Juricic (Juricic (2020)) reviewed methods for verifying structural identifiability of linear and non-linear systems, and proposed conditions for building thermal models to be identifiable. Practical identifiability relates the parameter estimation possibilities to the experimental design (type and amount of measurements), the richness of available data and its accuracy, in addition to accounting for the type of model used. A parameter within a model is identifiable in practice if the data brings enough information to estimate it with finite confidence intervals (Rouchier (2018)). This property is related to sensitivity indices to some extent: a parameter which is deemed non-influencial by a sensitivity analysis has little impact on the model output variance, and has therefore little chance to be learned effectively from data. Sensitivity analysis is however not a sufficient condition for practical identifiability. Inference diagnostics, performed after learning, are the only way to check how much information the data has brought to each model parameter. 3.3.4 Practical identifiability The main result we often expect from a fitted model is the value of its parameters, or some metrics computed from them. It is important to check whether all individual parameters are statistically significant, identify interactions between them, and assess how much the model has learned from data. Practical identifiability is a question of sufficiency of the data relatively to the model complexity. In a frequentist framework Frequentist inference returns a point estimate of parameters \\(\\hat{\\theta}\\) and their covariance matrix \\(\\mathrm{cov}(\\hat{\\theta})\\), assuming that the error is Gaussian. From this, statistical \\(t\\)-tests may be run to check for the significance of individual parameters, and point out strong correlations which would incite to reformulate the model. However, confidence intervals obtained from \\(\\mathrm{cov}(\\hat{\\theta})\\) are finite and may not point out practical non identifiability. Likelihood-based confidence intervals (Raue et al. (2009)) are a more informative way to display structural and practical non-identifiability. A comprehensive application of this theory in a building physics application was proposed recently by Deconinck and Roels (Deconinck and Roels (2017)) to measure the identifiability of parameters of several RC models describing the thermal characteristics of a building component. In a Bayesian framework Bayesian inference returns a more complete description of the multivariate posterior distribution. This distribution is not only described by a mean and covariance matrix, but by a finite number of points which are not restricted to a multivariate Normal density. Figure 3.8: Pairplot of the multivariate posterior distribution approximated by MCMC, of a four-parameter model Although a Bayesian analysis with proper prior knowledge is always feasible, if there is no learning from the data, i.e. from the likelihood, then the prior and posterior distributions will be identical. In this sense, identifiability in a Bayesian framework is close to the notion of identifiability in a frequentist approach as it is a matter of learning from the likelihood. Considering \\(\\theta=(\\theta_1, \\theta_2)\\) a set of parameters divided into two subsets, the subset \\(\\theta_2\\) is not identified by the data if the observation does not increase our prior knowledge about \\(\\theta_2\\) given \\(\\theta_1\\): \\[\\begin{equation} p(\\theta_2|\\theta_1, y) \\approx p(\\theta_2 |\\theta_1) \\tag{3.21} \\end{equation}\\] To this purpose, Xie and Carlin(Xie and Carlin (2006)) propose a metric based on the Kullback-Leibler (KL) divergence, a quantity largely used for measuring the difference between two distributions. It measures how much is left to learn given data \\(y\\) and is defined as: \\[\\begin{equation} D_{\\theta_1,y} = KL\\left(p(\\theta_2|\\theta_1),p(\\theta_2|y)\\right) = \\int_{-\\infty}^{\\infty}p(\\theta_2|\\theta_1) \\mathrm{log}\\frac{p(\\theta_2|\\theta_1)}{p(\\theta_2|y)} \\mathrm{d}\\theta_2 \\tag{3.22} \\end{equation}\\] This metric can be estimated with an MCMC approach, however requiring a second complete sampling of the posterior with the identifiable parameters fixed. Figure 3.9: Illustration of the Kullback Leibler divergence for three posterior distributions. The less identifiable a parameter, the closer the posterior is to the prior, and the lower the KL divergence. 3.3.5 Model comparison criteria A model should be complex enough to capture the data-generating process and pass the validation tests, but avoid overfitting. The statistical learning litterature abundantly warns readers against the risks of overfitting: a model with too many degrees of freedom will fit the training data very well, but will poorly extrapolate to new data, because it will reproduce specific patterns caused by local errors. Figure 3.10: The bias-variance tradeoff Fig. 3.10 illustrates the bias-variance tradeoff formulated in the ISL (James et al. (2013)): In order to minimize the expected test error, we need to select a statistical learning method that simultaneously achieves low variance and low bias. Variance refers to the amount by which \\(\\hat{y}\\) would change if we estimated it using a different training data set. Bias refers to the error that is introduced by approximating a real-life problem. Model selection criteria are designed to help comparing several models, not just based on their fit with training data, but on an estimation of their prediction accuracy with new data. These criteria often reward models that offer a good compromise between simplicity and accuracy. They include: The likelihood ratio method consists in assessing whether increasing the complexity of a model results in a significant improvement of likelihood which justifies this increased complexity (Bacher and Madsen (2011)). It is a computationally inexpensive method, since it only relies on the value of the total likelihood function, but can only apply to compare nested models. Cross-validation methods capture out-of-sample prediction error by fitting the model to training data and evaluating this predictive accuracy on a holdout set. They can be computationally expensive but avoid the problem of overfitting. Information criteria estimate the expected out-of-sample prediction error from an adjustment of the within-sample error (Gelman, Hwang, and Vehtari (2014)). A fully Bayesian criterion is the Widely Applicable Information Criterion (WAIC) (Watanabe and Opper (2010)), asymptotically equal to the Bayesian leave-one-out cross validation criterion. The Pareto-smoothed importance sampling leave-one-out (PSIS-LOO) cross validation (Vehtari, Gelman, and Bagry (2016)) can be considered the state-of-the-art Bayesian criterion of model comparison and selection. The method does not require nested models, has a fast computation time and is asymptotically equivalent to WAIC. The expected log pointwise predictive density for a new dataset (elpd) is a measure of predictive accuracy for the \\(n\\) data points of a given dataset, taken one at a time. The Bayesian LOO estimate of out-of-sample predictive fit is: \\[\\begin{equation} \\mathrm{elpd}_\\mathrm{loo} = \\sum_{i=1}^n \\mathrm{log}p(y_i|y_{-i}) \\tag{3.23} \\end{equation}\\] where \\[\\begin{equation} p(y_i|y_{-i}) = \\int p(y_i|\\theta)p(\\theta|y_{-i})\\mathrm{d}\\theta \\tag{3.24} \\end{equation}\\] is the leave-one-out predictive density given the data without the \\(i\\)th data point. Exact LOO cross-validation is costly, since it requires fitting the model as many times as there are observations, each time leaving out one observation. Instead, PSIS-LOO (Vehtari, Gelman, and Bagry (2016)) approximates the LOO estimate, using the pointwise log-likelihood values computed from samples of the posterior. LOO cross-validation in general should be interpreted as a measure of prediction accuracy in the same conditions a model was trained. A better view of a models adaptability to new situations would likely be offered by a clear separation of the training and test datasets. Nevertheless, LOO is appropriate for model selection in several applications, including those of the present study: short-term prediction accuracy for model predictive control, and performance assessment of the building envelope. The effective number of parameters \\(p_\\mathrm{eff}\\) is another measure of the complexity of a model. It can be used to compare the complexity of a non-parametric model such as a Gaussian Process, with that of a parametric model, and is an additional tool for interpretation of a model selection procedure. References "],["ordinary-linear-regression.html", "Chapter 4 Ordinary linear regression 4.1 Introduction to OLR 4.2 Tutorial: OLR with R 4.3 Simple linear regression with R 4.4 Bayesian regression with Stan", " Chapter 4 Ordinary linear regression 4.1 Introduction to OLR Linear regression models are usually the first example shown in most statistical learning lectures. They are a popular introduction to statistical modelling because of their simplicity, while their structure is flexible and applicable to quite a large range of physical systems. We consider an output variable \\(y\\), and a set of explanatory variables \\(x=(x_1,...,x_k)\\), and assume that a series of \\(n\\) values of \\(y_i\\) and \\(x_{i1},...x_{ik}\\) have been recorded. The ordinary linear regression model states that the distribution of \\(y\\) given the \\(n\\times k\\) matrix of predictors \\(X\\) is normal with a mean that is a linear function of \\(X\\): \\[\\begin{equation} E(y_i|\\theta, X) = \\theta_0 + \\theta_1 x_{i1} + ... + \\theta_k x_{ik} \\tag{4.1} \\end{equation}\\] The parameter \\(\\theta\\) is a vector of \\(k\\) coefficients which distribution is to be determined. Ordinary linear regression assumes a normal linear model in which observation errors are independent and have equal variance \\(\\sigma^2\\). Under these assumptions, along with a uniform prior distribution on \\(\\theta\\), the posterior distribution for \\(\\theta\\) conditional on \\(\\sigma\\) can be explicitely formulated: \\[\\begin{align} \\theta | \\sigma, y &amp; \\sim N\\left( \\hat{\\theta} , V_\\theta \\sigma^2\\right) \\tag{4.2} \\\\ \\hat{\\theta} &amp; = (X^T \\, X)^{-1} X^T \\, y \\tag{4.3}\\\\ V_\\theta &amp; = (X^T \\, X)^{-1} \\tag{4.4} \\end{align}\\] along with the marginal distribution of \\(\\sigma^2\\): \\[\\begin{align} \\sigma^2|y &amp; \\sim \\mathrm{Inv-}\\chi^2(n-k, s^2 ) \\tag{4.5} \\\\ s^2 &amp; = \\frac{1}{n-k}(y-X\\hat{\\theta})^T (y-X\\hat{\\theta}) \\tag{4.6} \\end{align}\\] In the words of Gelman et al. (2013) : in the normal linear model framework, the first key statistical modelling issue is defining the variables \\(x\\) and \\(y\\), possibly using transformations, so that the conditional expectation of \\(y\\) is reasonably linear as a function of the columns of \\(X\\) with approximately normal errors. The second main issue, related to a Bayesian analysis framework, is a proper specification of the prior distribution on the model parameters. Despite their simplicity, linear regression models can be very useful as a first insight into the heat balance of a building: they allow a quick assessment of which types of measurements have an impact on the global balance and guide the choice of more detailed models. Moreover, if a large enough amount of data is available, the estimates of some coefficients such as the HTC often turn out to be quite reliable. The ordinary linear regression model is enough to explain the variability of the data if the regression errors \\(y_i - E(y_i|\\theta, X)\\) are independent, identically distributed along a normal distribution with constant variance \\(\\sigma^2\\). If that is not the case, the model can be extended in several ways. The expected value \\(E(y_i|\\theta, X)\\) may be non-linear or include non-linear transformations of the explanatory variables. Unequal variances and correlated errors can be included by allowing a data covariance matrix \\(\\Sigma_y\\) that is not necessarily proportional to the identity matrix: \\(y \\sim N(X\\theta, \\Sigma_y)\\). A non-normal probability distribution can be used. These transformations invalidate the analytical solutions shown by Eq. (4.3) to (4.6), but we will see that Bayesian inference can treat them seamlessly. 4.2 Tutorial: OLR with R The data used in this example was published by the Oak Ridge National Laboratory, Building Technologies Research and Integration Center (USA). It contains end use breakdowns of energy use and various indoor environmental conditions collected at the Campbell Creek Research House #3, at a 15 minute time stamp. The data availability ranges from 10/1/2013 to 9/30/2014 and was made available on OpenEI. For this notebook, the original data set was reduced by removing many columns and averaging measurements over daily time steps. This tutorial uses R, and more specifically functions from the tidyverse, a great data science environment. The reader is referred to the book R for data science to learn about each of them. In the following block: read_csv, from readr, reads csv files The %&gt;% operator is the pipe from magrittr transform(), from dplyr, modifies a variable in a table ymd, from lubridate, reads a string into a date with a specific format. lubridate is not included in the tidyverse and has to be imported separately. library(tidyverse) library(lubridate) df &lt;- read_csv(&quot;data/linearregression.csv&quot;) %&gt;% transform(TIMESTAMP = ymd(TIMESTAMP)) head(df) ## TIMESTAMP e_hp e_dhw e_fan e_other ti tg ts ## 1 2013-11-01 22.76562 36.42188 9.866156 227.1703 24.08979 19.50503 12.687269 ## 2 2013-11-02 22.94271 32.01042 9.985406 232.4833 22.71608 18.40336 8.639988 ## 3 2013-11-03 23.07250 28.19500 9.977400 226.7301 21.18138 17.36883 10.676611 ## 4 2013-11-04 49.60208 58.77083 10.032833 228.3963 20.66780 16.77118 11.524884 ## 5 2013-11-05 23.09896 57.51042 10.008042 301.6326 20.97978 16.91042 11.532292 ## 6 2013-11-06 23.00521 55.95312 10.292667 280.0146 21.40096 17.36753 13.797685 ## te i_sol wind_speed ## 1 17.138426 125.65963 0.9309792 ## 2 10.260706 90.86609 0.6845729 ## 3 7.348556 113.59297 0.9137600 ## 4 8.462442 116.15323 0.3336250 ## 5 10.836343 108.98073 0.3369687 ## 6 14.630382 87.00867 0.3273958 Starting from out main equations of simplified building energy modelling (see Chap. 2) we assume steady-state conditions: \\(\\partial T / \\partial t = 0\\). This should be a reasonable assumption, because the time step resolution of the data is daily. In winter, the house is heated by a heat pump. Considering the available data, here is the full model by which we describe the heat balance of the house: \\[\\begin{equation} \\Phi_{hp} + \\Phi_s + \\Phi_v + \\Phi_{inf} = H \\, (T_i-T_e) + H_g \\, (T_i-T_g) \\tag{4.7} \\end{equation}\\] On the left side are the heat sources \\(\\Phi\\) (W), some of which may be negative: \\(\\Phi_{hp} \\propto e_{hp}\\) is the heating power provided by the heat pump to the indoor space. It is proportional to the energy reading \\(e_{hp}\\) (Wh), which we will use as output variable, and to the time step size and the COP of the heat pump, supposed constant. \\(\\Phi_s \\propto I_{sol}\\) are the solar gains, supposed proportional to the measured outdoor solar irradiance \\(I_{sol}\\) (W/m\\(^2\\)) and an unknown constant solar aperture coefficient \\(A_s\\) (m\\(^2\\)). \\(\\Phi_v = \\dot{m} \\, c_p \\, (T_s-T_i)\\) is the ventilation heat input, with a ventilation supply rate \\(\\dot{m}\\) and supply temperature \\(T_s\\), which is measured (the house has a mechanical ventilation system with heat recovery) \\(\\Phi_{inf} \\propto V_{ws} (T_e-T_i)\\) is the heat input from air infiltration. We suppose it is proportional to the wind speed \\(V_{ws}\\) and the outdoor-indoor temperature difference. On the right side are two terms of heat loss through the envelope: \\(H \\, (T_i-T_e)\\) is the direct heat loss from the heated space at temperature \\(T_i\\) to the outdoor at \\(T_e\\) \\(H_g \\, (T_i-T_g)\\) is the heat loss through the partition wall between the heated space and an unheated garage at \\(T_g\\). Linear regression should allow us to identify the coefficients of each term, supposing that they have enough variability and influence on the output \\(\\Phi_{hp}\\). The outcome of the regression method will let us judge if this hypothesis is appropriate. 4.3 Simple linear regression with R Before fitting the full model shown above, let us try one with a single explanatory variable, which we assume has the most influence on the energy use of the heat pump: the heat transmission through the envelope. \\[\\begin{equation} e_{hp} = \\theta_1 (T_i-T_e) \\tag{4.8} \\end{equation}\\] where the \\(\\theta_1\\) parameter includes the heat loss coefficient \\(H\\), the COP of the heat pump and the time step size. Since the COP is unknown, we wont be able to estimate \\(H\\). This is fine, as the point of the exercise is mostly to identify influential features. \\(\\theta_0\\) is a constant intercept. First, we need to add \\(T_i-T_e\\) as a new column of the dataframe. Then we use this column as the only explanatory variable in Rs linear regression function. An intercept is included by default: the + 0 part of the expression is used here to remove it. df &lt;- df %&gt;% mutate(tite = ti - te) lm1.fit = lm(e_hp ~ tite + 0, data=df) summary(lm1.fit) ## ## Call: ## lm(formula = e_hp ~ tite + 0, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -166.91 -86.02 -49.12 -8.52 439.43 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## tite 13.2377 0.5113 25.89 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 109.7 on 150 degrees of freedom ## Multiple R-squared: 0.8171, Adjusted R-squared: 0.8159 ## F-statistic: 670.2 on 1 and 150 DF, p-value: &lt; 2.2e-16 The table displays the results of the linear regression fitting by ordinary least squares. Some indicators are useful to judge if the model sufficiently explains the output data, or if some input features are redundant. The t-statistic and p-value indicate whether an input has a significant influence on the input: P&gt;|t| should be close to zero, meaning that the null hypothesis should be rejected. In this case, the only input is relevant. R-squared measures the goodness of fit of the regression. 0.817 is a rather low value, which hints that the output should be explained by additional features in the model. Other values like AIC, BIC or the Durbin-Watson statistic can be calculated. DW indicates whether there is autocorrelation of the residuals. The output variable is not well explained solely by a linear function of \\((T_i-T_e)\\), and the model should be improved. We can confirm this with a graph of the confidence interval and prediction interval of the fitted linear model: conf.int = as.data.frame( predict(lm1.fit, interval=&quot;confidence&quot;) ) pred.int = as.data.frame( predict(lm1.fit, interval=&quot;prediction&quot;) ) ggplot() + geom_point(data = df, aes(tite, e_hp)) + geom_line(aes(x=df$tite, y=pred.int$fit)) + geom_ribbon(aes(x=df$tite, ymin=pred.int$lwr, ymax=pred.int$upr), alpha=0.2, fill=&#39;blue&#39;) + geom_ribbon(aes(x=df$tite, ymin=conf.int$lwr, ymax=conf.int$upr), alpha=0.2, fill=&#39;red&#39;) I argued in Sec. 3.2.5 that residual analysis was the most appropriate way to correctly validate a model, but it wont be necessary here since the fit is simply not good. Now we can try a more complete linear regression model, which matches the full model described earlier \\[\\begin{equation} e_{hp} = \\theta_1 (T_i-T_e) + \\theta_2 (T_i-T_g) + \\theta_3 I_{sol} + \\theta_4 (T_i-T_s) + \\theta_5 V_{ws}(T_i-T_e) \\tag{4.9} \\end{equation}\\] This model has five input variables. There are some more variables that need to be added to the dataframe to account for: The heat loss towards the unheated garage at temperature \\(T_g\\) The ventilation heat supply \\(\\Phi_v \\propto (T_s-T_i)\\) The air infiltration heat loss \\(\\Phi_{inf} \\propto V_{ws} (T_e-T_i)\\) df &lt;- df %&gt;% mutate(titg = ti - tg, tits = ti - ts, vtite = wind_speed * (ti-te)) lm2.fit = lm(e_hp ~ tite + titg + i_sol + tits + vtite + 0, data=df) summary(lm2.fit) ## ## Call: ## lm(formula = e_hp ~ tite + titg + i_sol + tits + vtite + 0, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -211.35 -80.19 -28.12 22.40 323.54 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## tite 22.134160 2.190009 10.107 &lt;2e-16 *** ## titg 1.673180 5.849859 0.286 0.7753 ## i_sol -0.373744 0.198848 -1.880 0.0622 . ## tits -8.479184 3.621594 -2.341 0.0206 * ## vtite 0.002483 0.735571 0.003 0.9973 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 95.34 on 146 degrees of freedom ## Multiple R-squared: 0.8656, Adjusted R-squared: 0.861 ## F-statistic: 188 on 5 and 146 DF, p-value: &lt; 2.2e-16 The R-squared has improved: this model seems to be a better choice than the first one. Two input variables however have a very high \\(p\\)-value: \\((T_i-T_g)\\) and \\(V_{ws}(T_i-T_e)\\). This suggests that the heat transfer between the heated space and the garage, and the wind, have little impact on the energy consumption of the heat pump. We can simplify the model by removing these two features: lm3.fit = lm(e_hp ~ tite + i_sol + tits + 0, data=df) summary(lm3.fit) ## ## Call: ## lm(formula = e_hp ~ tite + i_sol + tits + 0, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -209.27 -81.17 -28.52 26.60 323.60 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## tite 22.4237 1.7835 12.573 &lt; 2e-16 *** ## i_sol -0.3878 0.1891 -2.050 0.04209 * ## tits -7.7897 2.6010 -2.995 0.00322 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 94.72 on 148 degrees of freedom ## Multiple R-squared: 0.8655, Adjusted R-squared: 0.8628 ## F-statistic: 317.5 on 3 and 148 DF, p-value: &lt; 2.2e-16 The R-squared was not really impacted by the removal of two features, suggesting that they were indeed not influential. We can display a quick residual analysis, to see if the model sufficiently explains the variability of the data: par(mfrow=c(2,2)) plot(lm3.fit) This linear regression model is not perfect but it looks like a decent compromise of simplicity and fitness. In particular, the residuals vs fitted graph shows highly non-normal residuals at high values of the dependent variable. 4.4 Bayesian regression with Stan We will now conduct linear regression in a Bayesian framework, as explained in Sec. 3.1. We use the Stan probabilistic programming language, which allows full Bayesian statistical inference. A Stan model is a block of text which can either be written in a separate file, or in the same script as the current code. A model defined in its own file can then be called within either language: R, Python, Julia library(rstan) We previously selected a linear model with three predictors (T_i-T_e), I_{sol} and (T_i-T_s). The model can be written in probability form: each of the data points e_{hp,n} is normally distributed with a constant noise standard deviation \\(\\sigma\\): \\[\\begin{equation} e_{hp,n} \\sim N( \\theta_1 (T_i-T_e)_n + \\theta_2 I_{sol,n} + \\theta_3 (T_i-T_s)_n, \\sigma) \\tag{4.10} \\end{equation}\\] Of course, the Stan documentation has an example of linear regression model. The following block defines a model with any number of predictors K, and no intercept. lr_model= &quot; data { int&lt;lower=0&gt; N; // number of data items int&lt;lower=0&gt; K; // number of predictors matrix[N, K] x; // predictor matrix vector[N] y; // outcome vector } parameters { vector[K] theta; // coefficients for predictors real&lt;lower=0&gt; sigma; // error scale } model { y ~ normal(x * theta, sigma); // likelihood } &quot; Then, a list called model_data is created, which maps each part of the data to its appropriate variable into the STAN model. This list must contain all variables defined in the data block of the model. model_data &lt;- list( N = nrow(df), K = 3, x = df %&gt;% select(tite, i_sol, tits), y = df$e_hp ) Now that the model has been specified and the data has been mapped to its variables, the syntax for model fitting is below. fit1 &lt;- stan( model_code = lr_model, # Stan program data = model_data, # named list of data chains = 4, # number of Markov chains warmup = 1000, # number of warmup iterations per chain iter = 4000, # total number of iterations per chain cores = 2, # number of cores (could use one per chain) ) Fitting may result in a number of warnings, telling us that some problems may have occurred: divergent transitions, large R-hat values, low Effective Sample Size Obtaining a fit without these warnings takes some practice but is essential for an unbiased interpretation of the inferred variables and predictions. A guide to Stans warnings and how to address them is available here. Stan returns an object (called fit1 above) from which the distributions of outputs and parameters of the fitted model can be accessed As a first validation step, it is useful to take a look at the values of the parameters that have been estimated by the algorithm. Below, we use three diagnostics tools: The print method shows the table of parameters, much like we could display after an ordinary linear regression traceplot shows the traces of the selected parameters. If the fitting has converged, the traces approximate the posterior distributions pairs shows the pairwise relationships between parameters. Strong interactions between some parameters are an indication that the model should be re-parameterised. print(fit1) ## Inference for Stan model: f938627ed76aed8e5509a93cc0ab5f66. ## 4 chains, each with iter=4000; warmup=1000; thin=1; ## post-warmup draws per chain=3000, total post-warmup draws=12000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff ## theta[1] 22.43 0.03 1.80 18.91 21.22 22.44 23.66 25.92 4634 ## theta[2] -0.39 0.00 0.19 -0.76 -0.51 -0.39 -0.26 -0.01 4929 ## theta[3] -7.80 0.04 2.62 -12.94 -9.57 -7.80 -6.02 -2.73 4168 ## sigma 95.47 0.07 5.55 85.39 91.58 95.22 99.08 107.02 6069 ## lp__ -758.63 0.02 1.42 -762.19 -759.31 -758.31 -757.60 -756.87 4143 ## Rhat ## theta[1] 1 ## theta[2] 1 ## theta[3] 1 ## sigma 1 ## lp__ 1 ## ## Samples were drawn using NUTS(diag_e) at Wed Jul 06 14:27:24 2022. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). traceplot(fit1) pairs(fit1) The n_eff and Rhat indices show that convergence is fine (see Sec. 3.2.4). We are therefore allowed to carry on and interpret the results. There is strong interaction between some parameters. The numerical results are almost identical to the non-Bayesian model. This is not surprising as we used exactly the same model with no prior distribution on any parameter. References "],["bayesianmv.html", "Chapter 5 Bayesian M&amp;V 5.1 A Bayesian workflow for M&amp;V 5.2 Change-point models 5.3 IPMVP option C example (Rstan)", " Chapter 5 Bayesian M&amp;V 5.1 A Bayesian workflow for M&amp;V This tutorial applies the principles of Bayesian data analysis described in Sec. 3.1 to a Measurement and Verification (M&amp;V) problem. In a nutshell, M&amp;V aims at reliably assessing the energy savings that were actually gained from energy conservation measures (ECM). One of the main M&amp;V workflows, formalised by the IPMVP, is the reporting period basis, or avoided consumption method: a numerical model is trained during a baseline observation period (before ECMs are applied); the trained model is used to predict energy consumption during the reporting period (after energy conservation measures); predictions are compared with the measured consumption of the reporting period, in order to estimate adjusted energy savings This method therefore requires data to be recorded before and after implementation of the ECM, for a sufficiently long time. Fig. 5.1 shows the main steps of this method, when following a Bayesian approach. We assume that the measurement boundaries have been defined and that data have been recorded during the baseline and reporting period respectively. Figure 5.1: Estimation of savings with uncertainty in an avoided consumption workflow. The step of model validation is not displayed As with standard approaches, choose a model structure to describe the data with, and formulate it as a likelihood function. Formulate eventual expert knowledge assumptions in the form of prior probability distributions. Run a MCMC (or other) algorithm to obtain a set of samples \\(\\left(\\theta^{(1)},...,\\theta^{(S)}\\right)\\) which approximates the posterior distribution of parameters conditioned on the baseline data \\(p(\\theta|y_\\mathit{base})\\). Validate the inference by checking convergence diagnostics: \\(\\hat{R}\\), ESS, etc. Validate the model by computing its predictions during the baseline period \\(p(\\tilde{y}_\\mathit{base}|y_\\mathit{base})\\). This can be done by taking all (or a representative set of) samples individually, and running a model simulation \\(\\tilde{y}_\\mathit{base}^{(s)} \\sim p(y_\\mathit{base}|\\theta=\\theta^{(s)})\\) for each. This set of simulations generates the posterior predictive distribution of the baseline period, from which any statistic can be derived (mean, median, prediction intervals for any quantile, etc.). The measures of model validation (\\(R^2\\), net determination bias, t-statistic) can then be computed either from the mean, or from all samples in order to obtain their own probability densities. Compute the reporting period predictions in the same discrete way: each sample \\(\\theta^{(s)}\\) generates a profile \\(\\tilde{y}_\\mathit{repo}^{(s)} \\sim p(y_\\mathit{repo}|\\theta=\\theta^{(s)})\\), and this set of simulations generates the posterior predictive distribution of the reporting period. Since each reporting period prediction \\(\\tilde{y}_\\mathit{repo}^{(s)}\\) can be compared with the measured reporting period consumption \\(y_\\mathit{repo}\\), we can obtain \\(S\\) values for the energy savings, which distribution approximate the posterior probability of savings. 5.2 Change-point models Some systems are dependent on a variable, but only above or below a certain value. For example, cooling energy use may be proportional to ambient temperature, yet only above a certain threshold. When ambient temperature decreases to below the threshold, the cooling energy use does not continue to decrease, because the fan energy remains constant. In cases like these, simple regression can be improved by using a change-point linear regression. Change point models often have a better fit than a simple regression, especially when modeling energy usage for a facility. The energy signature of a building decomposes the total energy consumption (or power \\(\\Phi\\)) into three terms: heating, cooling, and other uses. Heating and cooling are then assumed to be linearly dependent on the outdoor air temperature \\(T_e\\), and only turned on conditionally on two threshold temperatures \\(T_{b1}\\) and \\(T_{b2}\\), respectively. \\[\\begin{align} E(\\Phi|\\theta, X) &amp; = \\Phi_0 + \\mathrm{HTC}_1 \\, \\left(T_{b1} - T_e\\right) &amp; \\mathrm{if} \\quad T_e \\leq T_{b1} \\tag{5.1} \\\\ E(\\Phi|\\theta, X) &amp; = \\Phi_0 &amp; \\mathrm{if} \\quad T_{b1} \\leq T_e \\leq T_{b2} \\tag{5.2} \\\\ E(\\Phi|\\theta, X) &amp; = \\Phi_0 + \\mathrm{HTC}_2 \\, \\left(T_e - T_{b2}\\right) &amp; \\mathrm{if} \\quad T_{b2} \\leq T_e \\tag{5.3} \\end{align}\\] Data points should be averaged over long enough (at least daily) sampling times, so that the steady-state assumption formulated above can hold. \\(\\Phi_0\\) is the average baseline consumption during each sampling period, of all energy uses besides heating and cooling. Heating is turned on if the outdoor air temperature drops below a basis temperature \\(T_{b1}\\), and the heating power \\(\\Phi_h = \\mathrm{HTC}_1 \\, \\left(T_{b1} - T_e\\right)\\) is assumed proportional to a heat transfer coefficeint (HTC) value. The same reasoning is used to formulate cooling, with a summer HTC value that may be different from the first one. This model is therefore a piecewise linear regression model, where the switching points \\(T_{b1}\\) and \\(T_{b2}\\) are usually to be identified along with the other parameters. The appeal of the energy signature model is that the only data it requires are energy meter readings and outdoor air temperature, with a large sampling time. 5.3 IPMVP option C example (Rstan) library(rstan) library(tidyverse) library(lubridate) The data used in this example is the hourly energy consumption and outdoor air temperature data for 11 commercial buildings (office/retail), publicly available here: https://openei.org/datasets/dataset/consumption-outdoor-air-temperature-11-commercial-buildings We will be using two data files, respectively labeled Building 6 (Office Pre), and Building 6 (Office Post). 5.3.1 Loading and displaying the data The following block loads two separate data files: building60preoffice.csv is the baseline period file, saved into the df.base variable building60postoffice.csv is the reporting period file, saved into the df.repo variable The Date column of both files is converted into a DateTime type into a new column. Then, the baseline dataset is displayed for a first exploratory look at the data. # Baseline data: one year df.base &lt;- read_csv(&quot;data/building60preoffice.csv&quot;) %&gt;% mutate(DateTime = mdy_hm(Date), Date = as_date(DateTime)) # Post-retrofit data: one year df.repo &lt;- read_csv(&quot;data/building62postoffice.csv&quot;) %&gt;% mutate(DateTime = mdy_hm(Date), Date = as_date(DateTime)) # Plot the original data head(df.base) ## # A tibble: 6 x 4 ## Date OAT `Building 6 kW` DateTime ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt; ## 1 2009-01-02 41.6 23.3 2009-01-02 00:00:00 ## 2 2009-01-02 40.9 23.1 2009-01-02 01:00:00 ## 3 2009-01-02 39.5 23.7 2009-01-02 02:00:00 ## 4 2009-01-02 36.3 29.1 2009-01-02 03:00:00 ## 5 2009-01-02 32.8 35.6 2009-01-02 04:00:00 ## 6 2009-01-02 32.5 45.5 2009-01-02 05:00:00 ggplot(data = df.base) + geom_line(mapping = aes(x=DateTime, y=`Building 6 kW`)) A few interesting observations: The data has a hourly time step size. Every hour, the outdoor air temperature (OAT in °F) and energy use (kW) are available. The energy use is higher in summer and in winter than in-between. This suggests that this consumption data includes both heating and cooling appliances. Week-ends are clearly visible with a lower consumption than in the working days of the week. 5.3.2 Daily averaged data Averaging the data over daily time steps should allow to overlook the dependence between consecutive measurements. In turn, this allows using a model which will be much simpler than time series models, but will only be capable of low frequency predictions. The following block creates new datasets from the original ones: Measurements are daily averaged Temperatures are switched to °C for international suitability. A categorical variable is added to indicate week ends. Then, we plot the daily energy use \\(E\\) (kWh) versus the outdoor temperature \\(T\\) (°C) for both values of the week.end categorical variable. daily.average &lt;- function(df) { df %&gt;% group_by(Date) %&gt;% summarise(OAT = mean(OAT), E = sum(`Building 6 kW`), .groups = &#39;drop&#39; ) %&gt;% mutate(wday = wday(Date), week.end = wday==1 | wday==7, T = (OAT-32) * 5/9) } df.base.daily &lt;- daily.average(df.base) df.repo.daily &lt;- daily.average(df.repo) ggplot(data = df.base.daily) + geom_point(mapping = aes(x=T, y=E, color=week.end)) 5.3.3 Model definition After looking at the data, we can suggest using a change-point model which will include the effects of heating and cooling, and separate week ends from working days. The expected daily energy use \\(E\\) (in kWh per day) is a function of the outdoor temperature \\(T\\) and of a number of parameters: For the week-ends: \\(p(E|T) \\sim \\mathcal{N}\\left[\\alpha_1 + \\beta_{1,h}(\\tau_{1,h}-T)^+ + \\beta_{1,c}(T-\\tau_{1,c})^+, \\sigma\\right]\\) For the working days: \\(p(E|T) \\sim \\mathcal{N}\\left[\\alpha_2 + \\beta_{2,h}(\\tau_{2,h}-T)^+ + \\beta_{2,c}(T-\\tau_{2,c})^+,\\sigma\\right]\\) Where the 1 and 2 subscripts indicate week-ends and working day, respectively, and the \\(h\\) and \\(c\\) subscripts indicate heating and cooling modes. The \\(+\\) superscript indicates that a term is only applied if above zero. The two equations above mean that we expect the energy use \\(E\\) to be a normal distribution centered around a change-point model, with a constant standard deviation \\(\\sigma\\). Some particularities of Bayesian statistics are: this normal distribution can be replaced by any other probability distribution; the error term \\(\\sigma\\) can be formulated as a function of some inputs; etc. This model has 11 possible parameters, which makes it significantly more complex than an ordinary linear regression. We could simplify it by assuming that the working days and week ends mode share the same temperature thresholds for heating (\\(\\tau_{1,h}=\\tau_{2,h}\\)) or for cooling (\\(\\tau_{1,c}=\\tau_{2,c}\\)). The following method would also be exactly the same if we decided to complexify the model, for instance by assuming non-linear profiles on each side of the change points, or if we had more categorical variables. 5.3.4 Model specification with Stan In this example, we use the Stan probabilistic programming language, which allows full Bayesian statistical inference. A Stan model is a block of text which can either be written in a separate file, or in the same script as the current code. Specifying a model in Stan takes a certain learning curve, but it unlocks the full flexibility of Bayesian analysis. changepoint &lt;- &quot; functions { // This chunk is the formula for the changepoint model which will be used several times in this program real power_mean(int w, real t, vector alpha, vector beta_h, vector tau_h, vector beta_c, vector tau_c) { real a = w ? alpha[1] : alpha[2]; // condition on the type of day real heat = w ? beta_h[1] * fmax(tau_h[1]-t, 0) : beta_h[2] * fmax(tau_h[2]-t, 0) ; real cool = w ? beta_c[1] * fmax(t-tau_c[1], 0) : beta_c[2] * fmax(t-tau_c[2], 0) ; return (a + heat + cool); } } data { // This block declares all data which will be passed to the Stan model. int&lt;lower=0&gt; N_base; // number of data items in the baseline period vector[N_base] t_base; // temperature (baseline) int w_base[N_base]; // categorical variable for the week ends (baseline) vector[N_base] y_base; // outcome energy vector (baseline) int&lt;lower=0&gt; N_repo; // number of data items in the reporting period vector[N_repo] t_repo; // temperature (reporting) int w_repo[N_repo]; // categorical variable for the week ends (reporting) vector[N_repo] y_repo; // outcome energy vector (reporting) } parameters { // This block declares the parameters of the model. There are 10 parameters plus the error scale sigma vector[2] alpha; // baseline consumption (work days and week ends) vector[2] beta_h; // slopes for heating vector[2] tau_h; // threshold temperatures for heating vector[2] beta_c; // slopes for cooling vector[2] tau_c; // threshold temperatures for cooling real&lt;lower=0&gt; sigma; // error scale } model { // Assigning prior distributions on some parameters alpha ~ normal([400, 800], [150, 150]); tau_h ~ normal(8, 5); tau_c ~ normal(18, 5); beta_h ~ normal(40, 15); beta_c ~ normal(40, 15); // Observational model for (n in 1:N_base) { y_base[n] ~ normal(power_mean(w_base[n], t_base[n], alpha, beta_h, tau_h, beta_c, tau_c), sigma); } } generated quantities { vector[N_base] y_base_pred; vector[N_repo] y_repo_pred; real savings = 0; for (n in 1:N_base) { y_base_pred[n] = normal_rng(power_mean(w_base[n], t_base[n], alpha, beta_h, tau_h, beta_c, tau_c), sigma); } for (n in 1:N_repo) { y_repo_pred[n] = normal_rng(power_mean(w_repo[n], t_repo[n], alpha, beta_h, tau_h, beta_c, tau_c), sigma); savings += y_repo_pred[n] - y_repo[n]; } } &quot; Then, a list called model_data is created, which maps each part of the data to its appropriate variable into the Stan model. model_data &lt;- list( N_base = nrow(df.base.daily), t_base = df.base.daily$T, w_base = as.numeric(df.base.daily$week.end), y_base = df.base.daily$E, N_repo = nrow(df.repo.daily), t_repo = df.repo.daily$T, w_repo = as.numeric(df.repo.daily$week.end), y_repo = df.repo.daily$E ) 5.3.5 Model fitting Now that the model has been specified and the data has been mapped to its variables, the syntax for model fitting is below. One disadvantage of Bayesian inference is that the MCMC algorithm takes much longer to converge than a typical least-squares model fitting method. Running the code below might take a minute because we are only using 365 data points, but the Bayesian approach might become problematic for larger data files. # Fitting the model fit1 &lt;- stan( model_code = changepoint, # Stan program data = model_data, # named list of data chains = 2, # number of Markov chains warmup = 1000, # number of warmup iterations per chain iter = 4000, # total number of iterations per chain cores = 2, # number of cores (could use one per chain) refresh = 0, # progress not shown ) ## Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable. ## Running the chains for more iterations may help. See ## https://mc-stan.org/misc/warnings.html#tail-ess Fitting may result in a number of warnings, telling us that some problems may have occurred: divergent transitions, large R-hat values, low Effective Sample Size Obtaining a fit without these warnings takes some practice but is essential for an unbiased interpretation of the inferred variables and predictions. A guide to Stans warnings and how to address them is available here. The first step into solving these warnings is to re-run the algorithm with different controls: iter, max_treedepth, etc. If problems persist, it is possible that the model is too complex for the information that the data is able to provide and should be simplified, or that stronger priors should be proposed. A lot of problems can be solved with some prior information. In our specific case, this is especially useful for the variables in the equation for the week-ends, since there are not a lot of data points. 5.3.6 Validation and results Stan returns an object (called fit1 above) from which the distributions of outputs and parameters of the fitted model can be accessed The MCMC algorithm produces a chain of samples \\(\\theta^{(m)}\\) for the parameters, which approximate their posterior distributions. In this case, each parameter of the model is represented by a chain of 6,000 draws: from these draws, we can extract any statistics we need: mean, median, quantiles, \\(t\\)-score and \\(p\\)-values, etc. 5.3.6.1 Parameters As a first validation step, it is useful to take a look at the values of the parameters that have been estimated by the algorithm. Below, we use three diagnostics tools: The print method shows the table of parameters, much like we could display after an ordinary linear regression traceplot shows the traces of the selected parameters. If the fitting has converged, the traces approximate the posterior distributions pairs shows the pairwise relationships between parameters. Strong interactions between some parameters are an indication that the model should be re-parameterised. print(fit1, pars = c(&quot;alpha&quot;, &quot;beta_h&quot;, &quot;tau_h&quot;, &quot;beta_c&quot;, &quot;tau_c&quot;, &quot;sigma&quot;, &quot;savings&quot;)) ## Inference for Stan model: 00b679580a50f560b8d7e4a217a14cb6. ## 2 chains, each with iter=4000; warmup=1000; thin=1; ## post-warmup draws per chain=3000, total post-warmup draws=6000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% ## alpha[1] 448.31 0.96 37.21 355.54 429.62 452.24 473.38 507.88 ## alpha[2] 825.45 4.21 33.75 795.41 820.88 829.88 838.26 853.06 ## beta_h[1] 35.84 0.05 3.56 29.31 33.44 35.72 38.20 43.00 ## beta_h[2] 33.36 0.05 2.87 27.88 31.38 33.34 35.30 38.96 ## tau_h[1] 9.88 0.04 1.49 7.37 8.76 9.83 10.77 13.21 ## tau_h[2] 6.66 0.14 1.28 5.18 5.96 6.47 7.05 8.92 ## beta_c[1] 14.58 0.34 8.36 6.11 10.35 12.76 15.70 43.63 ## beta_c[2] 29.20 0.06 3.29 23.16 26.85 29.04 31.45 35.81 ## tau_c[1] 16.02 0.15 4.31 7.87 13.59 15.80 17.90 27.13 ## tau_c[2] 15.59 0.18 1.63 12.95 15.05 15.81 16.51 17.50 ## sigma 103.01 0.06 3.94 95.68 100.28 102.89 105.55 111.16 ## savings 69146.29 36.25 2830.25 63683.73 67168.09 69206.84 71091.00 74665.46 ## n_eff Rhat ## alpha[1] 1496 1.00 ## alpha[2] 64 1.03 ## beta_h[1] 4383 1.00 ## beta_h[2] 3701 1.00 ## tau_h[1] 1693 1.00 ## tau_h[2] 88 1.02 ## beta_c[1] 600 1.01 ## beta_c[2] 3351 1.00 ## tau_c[1] 835 1.00 ## tau_c[2] 83 1.02 ## sigma 4709 1.00 ## savings 6096 1.00 ## ## Samples were drawn using NUTS(diag_e) at Wed Jul 06 14:29:01 2022. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). traceplot(fit1, pars = c(&quot;alpha&quot;, &quot;beta_h&quot;, &quot;tau_h&quot;, &quot;beta_c&quot;, &quot;tau_c&quot;, &quot;sigma&quot;, &quot;lp__&quot;)) pairs(fit1, pars = c(&quot;alpha&quot;, &quot;beta_h&quot;, &quot;savings&quot;)) 5.3.6.2 Predictions Our main goal here is to compare the energy use measured during the reporting period \\(y_\\mathit{repo}\\) with the predictions of the fitted model. Since it is a probabilistic model, its outcome is actually a probability distribution \\(p\\left(y_\\mathit{repo}|x_\\mathit{repo}, x_\\mathit{base}, y_\\mathit{base}\\right)\\), based on the observed values of the model inputs \\(x\\) during the baseline and reporting periods, and on the observed energy use during the baseline period \\(y_\\mathit{base}\\). This so-called posterior predictive distribution \\(p\\left(y_\\mathit{repo}|...\\right)\\) is already directly available, because a value of \\(y_\\mathit{repo}\\) (for each time step) was directly calculated by the Stan model for each value \\(\\theta^{(m)}\\) (see Stan users guide for more details). \\[\\begin{equation} p\\left(y_\\mathit{repo}|...\\right) \\approx \\frac{1}{M} \\sum_{m=1}^M p\\left(y_\\mathit{repo}|x_\\mathit{repo},\\theta^{(m)}\\right) \\end{equation}\\] First, let us look at the posterior predictive distribution during the baseline period, in order to validate the model compared to its training data: # Extracting full predictive distributions from the stanfit object la &lt;- rstan::extract(fit1, permuted = TRUE) y_base_pred &lt;- la$y_base_pred # Quantiles y_base_quan &lt;- apply(y_base_pred, 2, quantile, probs=c(0.025, 0.5, 0.975)) # Data frame df.base.post &lt;- data.frame(Date = df.base.daily$Date, T = df.base.daily$T, y = df.base.daily$E, w = df.base.daily$week.end, pred_low = y_base_quan[1, ], pred_med = y_base_quan[2, ], pred_up = y_base_quan[3, ]) # Plot ggplot(data = df.base.post) + geom_point(mapping = aes(x=T, y=y, color=w)) + geom_line(data = . %&gt;% filter(!df.base.post$w), mapping = aes(x=T, y=pred_med), color=&#39;red&#39;) + geom_ribbon(data = . %&gt;% filter(!df.base.post$w), mapping = aes(x=T, ymin=pred_low, ymax=pred_up), fill=&#39;red&#39;, alpha=0.1) + geom_line(data = . %&gt;% filter(df.base.post$w), mapping = aes(x=T, y=pred_med), color=&#39;blue&#39;) + geom_ribbon(data = . %&gt;% filter(df.base.post$w), mapping = aes(x=T, ymin=pred_low, ymax=pred_up), fill=&#39;blue&#39;, alpha=0.1) The colored bands show a 95% prediction interval for the working days and the week ends, respectively. The points are the measurements of the baseline period. 5.3.7 Residuals An important validation step is to check for autocorrelation in the residuals of the fitted model, on the baseline data that was used for fitting. Autocorrelation is often a sign of insufficient model complexity, or that the form of the model error term has not been appropriately chosen. The two graphs below show: Residuals vs Date, in order to display eventual autocorrelation residuals vs Temperature ggplot(data = df.base.post) + geom_point(mapping = aes(x=Date, y=pred_med-y)) + geom_ribbon(mapping = aes(x=Date, ymin=pred_low-y, ymax=pred_up-y), alpha=0.2) ggplot(data = df.base.post) + geom_point(mapping = aes(x=T, y=pred_med-y)) + geom_ribbon(mapping = aes(x=T, ymin=pred_low-y, ymax=pred_up-y), alpha=0.2) The second graph is fine, but it seems that these is a trend in the residuals in the first few months and last few months of the year, suggesting that the model doesnt quite capture the winter energy consumption very well. 5.3.8 Savings Our Stan model already calculates the expected output \\(y\\) of the reporting period, for each sample \\(\\theta_i\\) of the posterior distribution. We can therefore display a probability distribution for each of the data points of the reporting period, and compare it with the measured data in the same period. The following graph compares the energy use measured during the reporting period (points) with the probability distributions of energy use predicted by the model during the same period. # Extracting full predictive distributions from the stanfit object y_repo_pred &lt;- la$y_repo_pred # Quantiles y_repo_quan &lt;- apply(y_repo_pred, 2, quantile, probs=c(0.025, 0.5, 0.975)) # Data frame df.repo.post &lt;- data.frame(Date = df.repo.daily$Date, T = df.repo.daily$T, y = df.repo.daily$E, w = df.repo.daily$week.end, pred_low = y_repo_quan[1, ], pred_med = y_repo_quan[2, ], pred_up = y_repo_quan[3, ]) # Plot ggplot(data = df.repo.post) + geom_point(mapping = aes(x=T, y=y, color=w)) + geom_line(data = . %&gt;% filter(!df.repo.post$w), mapping = aes(x=T, y=pred_med), color=&#39;red&#39;) + geom_ribbon(data = . %&gt;% filter(!df.repo.post$w), mapping = aes(x=T, ymin=pred_low, ymax=pred_up), fill=&#39;red&#39;, alpha=0.1) + geom_line(data = . %&gt;% filter(df.repo.post$w), mapping = aes(x=T, y=pred_med), color=&#39;blue&#39;) + geom_ribbon(data = . %&gt;% filter(df.repo.post$w), mapping = aes(x=T, ymin=pred_low, ymax=pred_up), fill=&#39;blue&#39;, alpha=0.1) The savings, i.e. the difference between the measured energy use during the reporting period and their prediction by the model, have been included in the Stan model definition. Similarly to the prediction, the savings are therefore available as a probability distribution: we have a full description of any confidence interval we may wish for. The table of results shown after model fitting shows that The mean estimated savings are 69,069 kWh The 95% confidence interval spans between 63,550 and 74,880 kWh We can also choose to display any quantile of the posterior distribution of savings: plot(fit1, pars = c(&quot;savings&quot;)) ## ci_level: 0.8 (80% intervals) ## outer_level: 0.95 (95% intervals) "],["finite-mixture-models.html", "Chapter 6 Finite mixture models 6.1 Principle 6.2 Tutorial (Rstan)", " Chapter 6 Finite mixture models 6.1 Principle The energy signature models only offer a coarse disaggregation of energy use into three components: heating, cooling, and baseline consumption. Furthermore, they rely on very long sampling times and cannot predict sub-daily consumption profiles. Finite Mixture Models (FMM) are one way to take the disaggregation of the baseline energy consumption further. Their most common specific case are the Gaussian Mixture Models (GMM). Finite mixture models assume that the outcome \\(y\\) is drawn from one of several distributions, the identity of which is controlled by a categorical mixing distribution. For instance, the mixture of \\(K\\) normal distributions \\(f\\) with locations \\(\\mu_k\\) and scales \\(\\sigma_k\\) reads: \\[\\begin{equation} p(y_i|\\lambda, \\mu, \\sigma) = \\sum_{k=1}^K \\lambda_k f(y_i|\\mu_k,\\sigma_k) \\tag{6.1} \\end{equation}\\] where \\(\\lambda_k\\) is the (positive) mixing proportion of the \\(k\\)th component and \\(\\sum_{k=1}^K \\lambda_k = 1\\). The FMM distributes the observed values into a finite number of distributions with probability \\(\\lambda_k\\). The optimal number of components is not always a trivial choice: studies involving GMM often rely on some model selection index, such as the Bayesian Information Criterion (BIC), to guide the choice of the appropriate value for \\(K\\). The dependency of observations \\(y\\) on explanatory variables \\(x\\) can be included in the FMM, by formulating its parameters \\(\\left\\{ \\lambda_k(x), \\mu_k(x), \\sigma_k(x) \\right\\}\\) as dependent on the given value \\(x\\) of these regressors. Furthermore, in order to include the effects of different power consumption demand behaviours, the mixture probabilities \\(\\lambda_k\\) can be modelled as dependent on a categorical variable \\(z\\). Finite Mixture Models thus offer a very high flexibility for attempting to disaggregate and predict energy uses, while including the possible effects of continuous or discrete explanatory variables. 6.2 Tutorial (Rstan) This example uses a data file provided in the books repository. The tutorial below is written in R and uses Stan. Unsurprisingly, the Stan users guide also has a chapter on finite mixtures library(rstan) library(tidyverse) df &lt;- read_csv(&quot;data/mixture.csv&quot;) summary(df) ## mean_consumption ratio_house ratio_apartment ## Min. : 0.770 Min. :0.0000 Min. :0.0000 ## 1st Qu.: 3.753 1st Qu.:0.2006 1st Qu.:0.1181 ## Median : 5.146 Median :0.6346 Median :0.3654 ## Mean : 5.499 Mean :0.5523 Mean :0.4477 ## 3rd Qu.: 6.939 3rd Qu.:0.8819 3rd Qu.:0.7994 ## Max. :41.805 Max. :1.0000 Max. :1.0000 nrow(df) ## [1] 20744 This data file is an excerpt of an energy consumption census in France. Each row represents an area of about 2,000 residents. The available data are the mean residential energy consumption in each area, and the ratios of houses and apartments. On average, we expect a house to have a higher energy consumption than an apartment: it is larger, has more residents, and more envelope surface with heat loss. Therefore, we can expect areas with more houses to have a higher mean consumption than areas with more apartments. Let us look at a pairplot of the three features. library(GGally) ggpairs(df, columns=c(&quot;mean_consumption&quot;, &quot;ratio_house&quot;, &quot;ratio_apartment&quot;)) There is indeed some correlation between the ratio of houses in each area and the mean consumption. The density of ratio_house is strongly bimodal, and the density of mean_consumption looks like it could be split into two distributions as well. We can now try to translate our assumptions into a simple mixture of two distributions. Equation (6.1) can be formulated as such: \\[\\begin{equation} p(y_t | \\lambda, \\mu, \\sigma) = \\lambda_t f\\left(y_t | \\mu_1, \\sigma_1 \\right) + (1-\\lambda_t) f\\left(y_t | \\mu_2, \\sigma_2 \\right) \\tag{6.2} \\end{equation}\\] where, for each data point \\(t\\), \\(y_t\\) is the dependent variable mean_consumption. \\(\\lambda_t\\) is the explanatory variable ratio_house. \\(f\\) is a type of continuous probability distribution. It can be Normal, Gamma, LogNormal, etc. \\(\\mu\\) and \\(\\sigma\\) are the parameters of the distribution \\(f\\) that we will choose. The indices \\(1\\) and \\(2\\) denote each of the two mixture components. This is a Stan mixture model with any number K of components: mixture &lt;- &quot; data { // This block declares all data which will be passed to the Stan model. int&lt;lower=0&gt; N; // number of data items in the training dataset int&lt;lower=0&gt; K; // number of components real y[N]; // outcome energy vector real l[N, K]; // ratios in the training dataset } parameters { // This block declares the parameters of the model. vector[K] mu; vector[K] sigma; } model { for (n in 1:N) { vector[K] lps; for (k in 1:K) { lps[k] = log(l[n, k]) + lognormal_lpdf(y[n] | mu[k], sigma[k]); } target += log_sum_exp(lps); } } &quot; We can separate the data into a training set and a test set like so. The following block allocates 75% of the data to the training set: # set.seed(12345) # this is optional but ensures that results are reproducible train_ind &lt;- sample(seq_len(nrow(df)), size = floor(0.75 * nrow(df))) train &lt;- df[train_ind, ] test &lt;- df[-train_ind, ] The next step maps the data to the Stan model and runs the MCMC algorithm. model_data &lt;- list( N = nrow(train), N_test = nrow(test), K = 2, y = train$mean_consumption, l = train %&gt;% select(ratio_house, ratio_apartment) ) # Fittage fit1 &lt;- stan( model_code = mixture, # Stan program data = model_data, # named list of data chains = 2, # number of Markov chains. 4 is better, 2 is faster warmup = 1000, # number of warmup iterations per chain iter = 4000, # total number of iterations per chain cores = 2, # number of cores (could use one per chain) ) Let us now display the results of the fitting: print(fit1, pars=c(&quot;mu&quot;, &quot;sigma&quot;, &quot;lp__&quot;)) ## Inference for Stan model: 3b97c47b3b8f3bc85f0a82dca510237c. ## 2 chains, each with iter=4000; warmup=1000; thin=1; ## post-warmup draws per chain=3000, total post-warmup draws=6000. ## ## mean se_mean sd 2.5% 25% 50% 75% ## mu[1] 1.90 0.00 0.00 1.90 1.90 1.90 1.91 ## mu[2] 1.26 0.00 0.00 1.26 1.26 1.26 1.27 ## sigma[1] 0.27 0.00 0.00 0.27 0.27 0.27 0.27 ## sigma[2] 0.29 0.00 0.00 0.28 0.29 0.29 0.29 ## lp__ -29304.80 0.03 1.43 -29308.46 -29305.46 -29304.50 -29303.75 ## 97.5% n_eff Rhat ## mu[1] 1.91 5070 1 ## mu[2] 1.27 4316 1 ## sigma[1] 0.27 7358 1 ## sigma[2] 0.29 6617 1 ## lp__ -29303.01 3215 1 ## ## Samples were drawn using NUTS(diag_e) at Wed Jul 06 14:36:51 2022. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). traceplot(fit1, pars=c(&quot;mu&quot;, &quot;sigma&quot;, &quot;lp__&quot;)) pairs(fit1, pars=c(&quot;mu&quot;, &quot;sigma&quot;, &quot;lp__&quot;)) It looks like we can be satisfied with the MCMC convergence: n_eff is high enough and Rhat close to 1 for all parameters, and all chains seem stationary. The last step is to predict values of the mean consumption of each area in the test data set. We calculate this prediction from the ratios of houses and apartments, and from the mean estimated values of the distributions in the mixture model. # Extracting distribution parameters from the fit object la &lt;- rstan::extract(fit1, permuted = TRUE) mu &lt;- colMeans(la$mu) sigma &lt;- colMeans(la$sigma) # Predict the consumption of the test data from the ratios test$y &lt;- test$ratio_house * rlnorm(nrow(test), mu[1], sigma[1]) + test$ratio_apartment * rlnorm(nrow(test), mu[2], sigma[2]) # Plot to compare measured and predicted consumption on the test data ggplot(data=test) + geom_histogram(mapping=aes(x=mean_consumption), bins=50, color=&#39;blue&#39;, alpha=0.3) + geom_histogram(mapping=aes(x=y), bins=50, color=&#39;red&#39;, alpha=0.3) "],["armax.html", "Chapter 7 Autoregressive models 7.1 Principle of ARMAX models 7.2 Tutorial (Rstan)", " Chapter 7 Autoregressive models 7.1 Principle of ARMAX models Most of the data recorded by monitoring buildings are time series: sequences taken at successive points and indexed in time. The previous chapter dealt with data that were aggregated with low enough frequency that successive values of the outcome variable could be considered independent from each other, and only dependent on explanatory variables. This aggregation however comes with a significant loss of information, since all dynamic effects are smoothed out. Dynamic models, including time-series models, allow the dependent variable to also be influenced by its own past values. Autoregressive models are based on the idea that the current value of the series, \\(y_t\\), can be explained as a function of \\(p\\) past values, \\(y_{t-1}\\), \\(y_{t-2}\\), , \\(y_{t-p}\\) where \\(p\\) determines the number of steps into the past needed to forecast the current value. The simplest time-series models are linear AutoRegressive (AR) models. The AR(p) model is of the form: \\[\\begin{equation} y_t = \\alpha + \\beta_1 y_{t-1} + \\beta_2 y_{t_2} + ... + \\beta_p y_{t_p} + \\epsilon_t \\tag{7.1} \\end{equation}\\] where \\(\\epsilon_t \\sim N(0, \\sigma^2)\\) is independent identically distributed (iid) noise with mean \\(0\\) and variance \\(\\sigma^2\\). This notation is equivalent to writing: \\[\\begin{equation} y_t \\sim N(\\alpha + \\beta_1 y_{t-1} + \\beta_2 y_{t_2} + ... , \\sigma^2) \\tag{7.2} \\end{equation}\\] but the time-series literature usually separates the noise into a dedicated variable \\(w_t\\) in order to formulate assumptions for it. Figure 7.1: AR(2) model: an observation is conditioned on its two previous instances AR models can be extended into many forms, including the AutoRegressive (AR) Moving Average (MA) model with eXogeneous (X) variables, or ARMAX. In addition to being related to its own \\(p\\) previous values AR(p), the output can be predicted by additional inputs (X) and the white noise \\(\\epsilon_t\\) can be replaced by a moving average of order \\(q\\) MA(q): \\[\\begin{equation} E(y_t|\\theta, X) = \\sum_{i=1}^p \\beta_i y_{t-i} + \\sum_{j=0}^q \\gamma_j \\epsilon_{t-j} + \\sum_{k=1}^K \\left[ \\sum_{i=0}^p \\theta_{k,i} x_{t-i,k} \\right] \\tag{7.3} \\end{equation}\\] On the right side of this equation, the second term, MA(q), is a linear combination of the successive values of the prevous errors. The third term includes the influence of the current value and up to \\(p\\) previous values of \\(K\\) explanatory variables. A more general notation could assume a different order \\(p\\) for each separate input. ARX and ARMAX models have among others been applied to modelling the heat dynamics of buildings and building components by the collaborative work of the IEA EBC Annex 58 (Madsen et al. (2015)). Issues when applying AR(MA)X-models are not only the selection and validation of the model, but also the extraction of physical information from the model parameters, as each individual parameter lacks a direct physical meaning. An important step in ARX-modelling is to select suitable orders of the input and output polynomials. This can be done by stepwise increasing the model order until most significant autocorrelation and cross correlation are removed. We refer to the books of Madsen (Madsen (2007)) or Shumway et al. (Shumway and Stoffer (2000)), for a more extensive description of all types of AR models. Further structures include Integrated models for non-stationary data (ARIMA), Seasonal components for longer-term cyclic data (SARMA, SARIMAX), and the autoregressive conditionally heteroscedastic (ARCH) model for non-constant conditional noise variance. 7.2 Tutorial (Rstan) Auto-regressive models are covered in the Stan users guide. In this chapter and the next few ones, we demonstrate their use for the prediction of energy consumption. 7.2.1 Data: the ASHRAE machine learning competition In 2019, the ASHRAE great energy predictor competition was featured on Kaggle. Participants developed models of metered building energy usage: chilled water, electric, hot water, and steam meters. The purpose of the competition was to find good modelling options for M&amp;V and encourage energy-saving investments. The data comes from over 1,449 buildings over a three-year timeframe, located in 16 different North-American sites. The dataset also includes basic weather data of all sites and some metadata for each building. Only a few buildings have all four meter types. In this chapter, one of these buildings was picked, labeled Building 1298. All examples of the Times-series modelling part of this book (autoregressive models and hidden Markov models) will be illustrated with the Building 1298 data. The complete dataset of the competition is available on Kaggle, but the data for this particular building has been extracted and is shared in the books repository. The following figure shows the readings of all 4 meters of this building during the whole training dataset. You can zoom in or hide some of the lines with the bokeh toolbar on the right. Bokeh Plot We start by importing libraries and the data file, and by looking at its content with the head() function. library(rstan) library(tidyverse) library(lubridate) df &lt;- read.csv(&quot;data/building_1298.csv&quot;) head(df) ## datetime m0 m1 m2 m3 air_temperature ## 1 2016-01-01 00:00:00 416.169 1994.63 2334.99 0.0000 5.6 ## 2 2016-01-01 01:00:00 408.616 2101.56 2755.43 79.5127 5.6 ## 3 2016-01-01 02:00:00 412.072 1885.37 2564.32 0.0000 5.6 ## 4 2016-01-01 03:00:00 393.053 1909.73 2804.94 0.0000 5.6 ## 5 2016-01-01 04:00:00 404.519 1882.42 2621.65 132.9570 5.0 ## 6 2016-01-01 05:00:00 406.975 1700.60 3047.30 74.5527 4.4 ## cloud_coverage dew_temperature precip_depth_1_hr sea_level_pressure ## 1 0 -0.6 0 1019.3 ## 2 0 -0.6 0 1019.3 ## 3 4 -0.6 0 1019.4 ## 4 NA -1.1 0 1019.4 ## 5 NA -2.2 0 1019.2 ## 6 NA -2.2 0 1018.9 ## wind_direction wind_speed ## 1 300 2.6 ## 2 300 2.6 ## 3 300 2.6 ## 4 NA 1.5 ## 5 290 3.1 ## 6 300 4.1 In this pre-processed data file, the meter readings are called m0 to m3 and there is a number of weather variables to choose explanatory variables from. The time resolution is hourly, which justifies the use of time series models. The following block performs some additional data processing and selection. First, we add some separate variables to the dataframe: the date, hour, day of week of each line. Also, the fill() function replaces NaN values found in some columns. Then, two subsets of the dataframe are chosen as separate training and test dataframes df.train and df.test. In the original Kaggle competition, the whole year of readings shown above is just the training period. We chose a shorter training and testing period within this original training data so that we can later show how the model performs in prediction. df &lt;- df %&gt;% mutate(Date = as_date(datetime), hour = hour(datetime), wday = wday(Date), week.end = wday==1 | wday==7) %&gt;% fill(air_temperature, wind_speed) trainingStart &lt;- as.Date(&quot;2016-05-01&quot;, &quot;%Y-%m-%d&quot;) trainingEnd &lt;- as.Date(&quot;2016-05-31&quot;, &quot;%Y-%m-%d&quot;) testEnd &lt;- as.Date(&quot;2016-06-07&quot;, &quot;%Y-%m-%d&quot;) df.train &lt;- df %&gt;% filter(datetime &gt; trainingStart, datetime &lt; trainingEnd) df.test &lt;- df %&gt;% filter(datetime &gt; trainingStart, datetime &lt; testEnd) 7.2.2 A simple ARX model The complete equation for an AR(p)MA(q)X model is shown by Eq. (7.3). Below, we will use a slightly simpler version of this model: the moving average term is left out as it is difficult to physically justify its relationship with meter readings. The following is therefore an ARX model defined with Stan, where \\(P\\) is the autoregressive order, and there are \\(K\\) explanatory variables. At every time step, we suppose that the current and the \\(P\\) previous values of the explanatory variables can be used: this means that the parameter \\(\\theta\\) is a \\((P+1) \\times K\\) matrix. model = &quot; data { int&lt;lower=0&gt; N_train; // data points (training) int&lt;lower=0&gt; N; // data points (all) int&lt;lower=0&gt; K; // number of predictors int P; // AR order matrix[N, K] x; // predictor matrix (training and test) vector[N_train] y; // output data } parameters { real alpha; // mean coeff row_vector[P] beta; // AR parameters row_vector[P+1] theta[K]; // coefficients of explanatory variables real&lt;lower=0&gt; sigma; // error term } model { for (n in P+1:N_train) { real ex = 0; for (j in 1:K) ex += theta[j] * x[n-P:n, j]; y[n] ~ normal(alpha + beta * y[n-P:n-1] + ex, sigma); } } generated quantities { vector[N] y_new; for (n in 1:P) y_new[n] = y[n]; for (n in P+1:N_train) { real ex = 0; for (j in 1:K) ex += theta[j] * x[n-P:n, j]; y_new[n] = normal_rng(alpha + beta * y[n-P:n-1] + ex, sigma); } for (n in N_train+1:N) { real ex = 0; for (j in 1:K) ex += theta[j] * x[n-P:n, j]; y_new[n] = normal_rng(alpha + beta * y_new[n-P:n-1] + ex, sigma); } } &quot; The generated quantities block at the end of this model computes the predicted output of the model every time a sample has been generated. This is an interesting feature of Stan to easily calculate posterior probabilities of variables other than the models parameters. The y_new variable will be used to assess the prediction performance of the fitted model. The next steps are similar to the other Stan examples given in this book: A model.data list is created to map the data from the dataframes to the Stan model; Model fitting; Printing posterior distributions of parameters and saving results in a dataframe. In this example, the outdoor air temperature was picked as the only explanatory variable, and the autoregressive order is only 1. model.data &lt;- list( N_train = nrow(df.train), # number of training data points N = nrow(df.test), # total number of data points K = 1, # number of predictors P = 1, # AR order y = df.train$m0 , # dependent variable (training data) x = df.test %&gt;% select(air_temperature) # explanatory variables (all data) ) fit &lt;- stan( model_code = model, # Stan program data = model.data, # named list of data chains = 4, # number of Markov chains warmup = 1000, # number of warmup iterations per chain iter = 2000, # total number of iterations per chain cores = 2, # number of cores (could use one per chain) refresh = 1, # progress not shown ) print(fit, pars=c(&quot;alpha&quot;, &quot;beta&quot;, &quot;theta&quot;, &quot;sigma&quot;)) ## Inference for Stan model: 4800e29de0bb5622caa49441804892bc. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## alpha 195.40 0.38 14.04 167.40 186.03 195.26 205.21 222.13 1373 1 ## beta[1] 0.57 0.00 0.03 0.51 0.55 0.57 0.59 0.63 1358 1 ## theta[1,1] -5.97 0.03 1.26 -8.38 -6.84 -6.00 -5.12 -3.50 1567 1 ## theta[1,2] 5.99 0.03 1.26 3.48 5.17 6.02 6.86 8.42 1575 1 ## sigma 37.53 0.02 0.99 35.67 36.84 37.49 38.18 39.56 2344 1 ## ## Samples were drawn using NUTS(diag_e) at Wed Jul 06 14:39:39 2022. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). This model is simple enough to reach convergence without any priors, but they would quickly become necessary if the number of unknown parameters were to increase. Finally, we can look how well the fitted model predicts the dependent variable, since some test data was extracted from the original dataframe. The prediction y_new was directly calculated by Stan in the generated quantities block: each sample of the posterior comes with a predicted output. We can therefore display the quantiles of the posterior predictive distribution as follows: # Saving results in a dataframe la &lt;- rstan::extract(fit, permuted = TRUE) # Quantiles from the posterior predictive distribution y.quantiles &lt;- apply(la$y_new, 2, quantile, probs=c(0.025, 0.5, 0.975)) df.test &lt;- df.test %&gt;% mutate(pred_low = y.quantiles[1, ], pred_med = y.quantiles[2, ], pred_up = y.quantiles[3, ]) # Plot plotStart &lt;- as.Date(&quot;2016-05-23&quot;, &quot;%Y-%m-%d&quot;) plotEnd &lt;- testEnd ggplot(data = df.test %&gt;% filter(datetime &gt; plotStart, datetime &lt; plotEnd)) + geom_line(mapping = aes(x=ymd_hms(datetime), y=m0)) + geom_line(mapping = aes(x=ymd_hms(datetime), y=pred_med), color=&quot;red&quot;) + geom_ribbon(mapping = aes(x=ymd_hms(datetime), ymin=pred_low, ymax=pred_up), fill=&#39;red&#39;, alpha=0.1) This plot shows the predicted output during the last week of the training period (until the end of May) and during the test period (beginning of June). We can see that as soon as baseline data stops being available, the model reproduces some trend in the data but doesnt perform very well. References "],["hmm.html", "Chapter 8 Hidden Markov models 8.1 Principles 8.2 Tutorial (Python)", " Chapter 8 Hidden Markov models 8.1 Principles In a hidden Markov model (HMM), the sequence of \\(T\\) output variables \\({y_t}\\) is generated by a parallel sequence of latent categorical state variables \\(z_t \\in \\left\\{ 1,\\cdots , K\\right\\}\\). These hidden states are a Markov chain, so that \\(z_t\\) is conditionally independent of other variables given \\(z_{t-1}\\). Figure 8.1: Hidden Markov model. Grey nodes denote the observed output sequence; white nodes denote the hidden states. Hidden states are conditioned on each other, while each observation is only conditioned on the current state The primary use of HMMs in building energy simulation is for identifying and modelling occupancy (Candanedo, Feldheim, and Deramaix (2017)). The hidden variable \\(z_t\\) may denote a binary state of occupancy or a finite number of occupants; the observed output \\(y_t\\) may denote any measurement impacted by occupancy: environmental sensors, smart meters, cameras, infrared sensors There is a vast literature on occupancy detection and estimation from different types of data and methods. The reader is referred to the review of Chen et al (Chen, Jiang, and Xie (2018)) for a more comprehensive insight. A HMM is defined by: A sequence of hidden states \\(\\left\\{z_t\\right\\} \\in \\mathbb{N}^T\\), each of which can take a finite number of values: \\(z_t \\in \\left[1,...,K\\right]\\) An observed variable \\(\\left\\{y_t\\right\\} \\in \\mathbb{R}^T\\) An initial probability \\(\\pi_0\\) which is the likelihood of the state at time \\(t=0\\). \\(\\pi_0\\) is a \\(K\\)-simplex, i.e. a \\(K\\)-vector which components sum to 1. A \\(K\\times K\\) one-step transition probability matrix \\(\\mathbf{A}(t) = \\left(a_{ij}(t)\\right)\\) so that \\[\\begin{equation} a_{ij}(t)=p\\left(z_t=j |z_{t-1}=i\\right) \\tag{8.1} \\end{equation}\\] is the probability at time \\(t\\) for the hidden variable to switch from state \\(i\\) to state \\(j\\). Like \\(\\pi_0\\), each row of \\(\\mathbf{A}(t)\\) must sum to 1. Emission probabilities \\(b_{i}(y_t) = p(y_t | z_t=i)\\), i.e. the probability distribution of the output variable given each possible state. The transition probabilities \\(\\left(a_{ij}(t)\\right)\\) are shown here as functions of time because they can be formulated as parametric expressions of external observed variables, such as the time of the day or weather variables. The Markov chain is then called inhomogeneous. Inhomogeneous transition probability matrices can capture occupancy properties at different time instances and thus encode occupancy dynamics: there can a higher probability of people entering the building at a certain time of the day, or if the outdoor temperature is high, etc. Training an HMM means finding the set of parameters \\(\\theta\\) which best explain the observations. This can be done by in least two ways: the first option is to compute the likelihood function with the forward algorithm, explained below, and to perform its direct numerical maximization. The second option is the Baum-Welch algorithm, a special case of the expectation-maximization (EM) algorithm: it alternates between a forward-backward algorithm for the expectation step, and an updating phase for the maximization step. A trained HMM can then be used to predict states and future values of the outcome variable. The estimation of the most likely state sequence given the observations is called decoding. \\[\\begin{equation} z^*_{0:T} = \\mathrm{arg max}_z \\; p(z_{0:T} | y_{0:T}) \\tag{8.2} \\end{equation}\\] and can be done by the Viterbi algorithm described below. 8.1.1 The forward algorithm The forward algorithm computes the likelihood function \\(p(y_{1:T}|\\theta)\\) of a hidden Markov model, given the values of its parameters \\(\\theta\\). These parameters come from the parametric expressions of \\(\\pi_0\\), \\(a_{ij}(t)\\) and \\(b_{i}\\). The algorithm works by calculating the components of a \\([T \\times K]\\) matrix \\(\\alpha\\) of forward variables defined by: \\[\\begin{equation} \\alpha_{ij} = p(y_{0:i},z_i=j) \\tag{8.3} \\end{equation}\\] The forward variable \\(\\alpha_{ij}\\) is the joint probability of observations up to time \\(i\\), and of the current hidden variable \\(z_i\\) being in state \\(j\\). The \\(\\alpha\\) matrix is filled row by row, and the total likelihood of the data is the sum of its last row. Figure 8.2: Forward algorithm for computing the likelihood of the data \\(y_{1:T}\\) given the parameters of a HMM 8.1.2 The Viterbi algorithm Supposing a trained HMM with known parameters in the initial probability \\(\\pi_0\\), transition matrix \\(\\mathbf{A}(t)\\) and emission probabilities \\(\\mathrm{b}(y)\\); the Viterbi algorithm finds the best sequence of hidden states, so that their probability conditioned on the data \\(y_{0:T}\\) is at a maximum: \\[\\begin{equation} z^*_{0:T} = \\mathrm{arg max}_z \\; p(z_{0:T} | y_{0:T}) \\tag{8.4} \\end{equation}\\] This process is called global decoding, and one of the possible inferences from HMMs. The Viterbi algorithm looks similar to the backward algorithm, with an additional backtracking phase. It produces two \\([T \\times K]\\) matrices \\(\\delta\\) and \\(\\psi\\) and fills them row by row in a forward phase. Then, a backtracking phase computes \\(z^*_{0:T}\\) from the \\(\\psi\\) matrix. The algorithm is described with more detail here. Figure 8.3: The Viterbi algorithm finds the best sequence of hidden states \\(z_{0:T}\\) given observations \\(y_{0:T}\\) 8.2 Tutorial (Python) Hidden Markov models are also covered in the Stan users guide. This tutorial uses the same data as the ARMAX example from chapter 7: Building 1298 from the ASHRAE Kaggle competition. This buildings meter date is available in the books repository. Bokeh Plot We will focus again on meter 0 (electricity), but the reader is free to use the same code for any of the other three meters of this data. This tutorial implements a simple homogeneous HMM written in Python and doesnt use Stan for once. The point is to show how to code the forward algorithm and the Viterbi algorithm from scratch. Let us start by importing some libraries, reading the data file and selecting which variable will be the dependent variable of our model. # importing the three main libraries import pandas as pd import numpy as np import matplotlib.pyplot as plt # importing parts of scipy from scipy.special import logsumexp from scipy.optimize import minimize from scipy.stats import norm # Reading the data file df = pd.read_csv(&#39;data/building_1298.csv&#39;) df.set_index(pd.to_datetime(df[&#39;datetime&#39;]), inplace=True, drop=True) df.fillna(method=&#39;ffill&#39;, inplace=True) The forward algorithm described above is written here. At any of the \\(N\\) time steps, the Markov chain can take one of \\(K\\) states. We assume that the emission probabilities are Normal with means \\(\\mu_i\\) and standard deviations \\(\\sigma_i\\): \\[\\begin{equation} b_i(y_t) = p(y_t|z_t=i) = N(\\mu_i, \\sigma_i) \\tag{8.5} \\end{equation}\\] The parameters of the forward algorithm are the transition matrix \\(a\\), and the parameters of the emission probabilities \\(\\mu\\) and \\(\\sigma\\): def forward(y, a, mu, sig): &quot;&quot;&quot; Calculates the likelihood from parameters a, mu and sig Arguments: y: dependent variable [N] a: transition matrix [KxK] mu: emission means [K] sig: emission standard deviations [K] Returns: The total log-likelihood &quot;&quot;&quot; N = len(y) logalpha = np.zeros((N,K)) # log of the forward variable defined above # Initialisation pi0 = 1/K * np.ones(K) # initial probabilities. Supposed known here. logalpha[0] = np.log(pi0) + norm.logpdf(y[0], loc=mu, scale=sig) # Recursion for t in range(1, N): for j in range(K): logalpha[t,j] = logsumexp(logalpha[t-1,:] + np.log(a[:,j]) + norm.logpdf(y[t], loc=mu[j], scale=sig[j]) ) # Termination return logsumexp(logalpha[-1]) The next block uses this forward algorithm to train the HMM. Just like in the ARMAX example, we only use a one-month subset of the whole training dataset. Training is done by the scipy.minimize() function. Before using it, we need to define an objective function to minimize. This function will take a single array \\(x\\) as argument and return the value we aim to minimize, which is the negative log likelihood from the forward algorithm. # Training subset training_start = &#39;2016-01-01&#39; training_end = &#39;2016-01-31&#39; df_train = df.drop(df.index[(df.index &lt; pd.to_datetime(training_start)) |(df.index &gt; pd.to_datetime(training_end))]) # choosing meter 0 as dependent variable df_train[&#39;y&#39;] = df_train[&#39;m0&#39;] # removing some outliers #df[&#39;y&#39;][df[&#39;m0&#39;] &lt; 300] = df[&#39;m0&#39;].mean() # normalizing y between 0 and 1 df_train[&#39;y&#39;] = (df_train[&#39;y&#39;] - df_train[&#39;y&#39;].min()) / ( df_train[&#39;y&#39;].max() - df_train[&#39;y&#39;].min() ) def objective(x): # Reshaping the parameter vector x into the three variables of the forward algorithm a1 = np.reshape(x[:K*(K-1)], (K,K-1)) # Matrix a without the right column a2 = (1-a1.sum(axis=1))[:,np.newaxis] # Right column of matrix a a = np.concatenate([a1, a2],axis=1) mu = x[K*(K-1):K*(K-1)+K] sig = x[K*(K-1)+K:] # Returns the minus log likelihood return -forward(df_train[&#39;y&#39;], a, mu, sig) # Initial parameter values to be passed to scipy.minimize() K = 2 # nombre d&#39;états possibles a_init = np.array([[0.9],[0.1]]) mu_init = [0.2, 0.6] # valeurs moyennes des émissions sig_init = [0.1, 0.1] # écarts types des émissions # Parameters are assembled into a single array x with given bounds x0 = np.concatenate( [a_init.flatten(), mu_init, sig_init] ) bounds = (K*(K-1)*[(0,1)] + 2*K*[(0, None)]) # Training res = minimize(objective, x0, bounds=bounds) # Variables are recovered from the fitted x array a1 = np.reshape(res.x[:K*(K-1)], (K,K-1)) a2 = (1-a1.sum(axis=1))[:,np.newaxis] a = np.concatenate([a1, a2],axis=1) mu = res.x[K*(K-1):K*(K-1)+K] sig = res.x[K*(K-1)+K:] Now that the parameters of the HMM have been estimated, we can decode it, i.e. estimating the most likely state sequence. This is done by the Viterbi algorithm below. y = df_train[&#39;y&#39;] N = len(y) z = np.zeros(N) # hidden state to be determined best_logp = np.zeros((N, K)) # delta in the description above back_ptr = np.zeros((N, K)) # psi in the description above # Initialisation best_logp[0] = norm.logpdf(y[0], loc=mu, scale=sig) # Recursion for t in range(1, N): for k in range(K): logp = best_logp[t - 1] + np.log(a[:, k]) + norm.logpdf(y[t], loc=mu[k], scale=sig[k]) best_logp[t, k] = np.max(logp) back_ptr[t, k] = np.argmax(logp) # Backtracking z[-1] = np.argmax(best_logp[-1]) for t in range(1, N): z[-1 - t] = back_ptr[-1 - t + 1, int(z[-1 - t + 1])] We can finish by inferring the predicted output from the estimated states and parameters of the HMM y_star_mean = np.zeros(N) y_star_std = np.zeros(N) for k in range(K): y_star_mean[z == k] = mu[k] y_star_std[z == k] = sig[k] y_star = np.random.normal(loc=y_star_mean, scale=y_star_std) Bokeh Plot References "],["composite-time-series-models.html", "Chapter 9 Composite time series models 9.1 Markov switching models 9.2 Hidden Markov energy signature", " Chapter 9 Composite time series models 9.1 Markov switching models Following the definitions of autoregressive models and hidden Markov models, a natural extension is a combination of both: a time-series model where the observed variable \\(y_t\\) is explained by a hidden state \\(x_t\\) and by a regression of its own previous value \\(y_{t-1}\\). These models are called autoregressive hidden Markov models (AR-HMM) (Murphy (2002)) or Markov switching models (MSM) (Wolf et al. (2019)). Figure 9.1: Markov switching model Similar to an HMM, an MSM is defined by a matrix of transition probabilities \\(\\left(a_{ij}(t)\\right) = p\\left(z_t=j |z_{t-1}=i\\right)\\) whose terms can be conditioned on explanatory variables (time, day, weather) and by emission probabilities. Rather than being only conditioned on \\(x_t\\), the emission probability can be a function of previous observations. This is an example of AR(1) process: \\[\\begin{equation} p(y_t | z_t=j) = \\alpha_j + \\phi_j y_{t-1} + w_{t,j} \\tag{9.1} \\end{equation}\\] where the intercept \\(\\alpha_j\\), slope \\(\\phi_j\\) and noise \\(w_{t,j}\\) depend on the state \\(z_t\\), and may have as many different values as the number of possible states. In a more complicated example, one could implement a whole ARMAX model (Eq. (7.3)) into the observation probability of a Markov switching model. An MSM can be trained with the same Baum-Welch algorithm and decoded with the same Viterbi algorithm as an HMM. The only difference is in the expression of the emission probabilities, which do not change the structure of the algorithms because \\(y_t\\) is conditionally independent on \\(x_{t-1}\\) given \\(x_t\\) and \\(y_{t-1}\\). 9.2 Hidden Markov energy signature Another extension of the HMM structure was proposed by the author (Rouchier (2022)) and called hidden Markov energy signature model. It is a HMM where the emission probability functions are energy signature (ES) models (see chapter 5): The energy use \\(y_t\\) of a building at time \\(t\\) follows a different ES model for each possible occupancy state \\(z_t \\in \\left[1,...,K\\right]\\). This is how we allow the parameters of the ES model \\(\\left\\{E_0, T_1, T_2, H_1, H_2, \\sigma\\right\\}\\) to depend on the occupancy. \\[\\begin{equation} b_{i}(y_t) = p(y_t|\\theta, T_a, z_t=i) = N\\left[E_{0,i} + H_{1,i}\\left(T_{1,i}-T_a\\right)^+ + H_{2,i}\\left(T_a-T_{2,i}\\right)^+, \\sigma_i \\right] \\tag{9.2} \\end{equation}\\] The occupancy state at each time \\(t\\) is unknown, and described by a hidden Markov chain. We define a transition probability matrix for each hour of the day \\(h\\) and day of the week \\(d\\) \\[\\begin{equation} a_{ij}(h,d)=p\\left(z_{h,d}=j |z_{h-1,d}=i\\right) \\tag{9.3} \\end{equation}\\] This formulation can be described as follows: at every hour \\(h\\) and day \\(d\\), the building has a probability \\(a_{ij}(h,d)\\) to switch from the occupancy state \\(i\\) to state \\(j\\). Then, if the building is in the occupancy state \\(i\\), then its energy use follows one of \\(K\\) possible ES models \\(b_i(y_t)\\). References "],["ssmprinciple.html", "Chapter 10 Principle of SSMs 10.1 Description 10.2 Linear state-space models 10.3 The Kalman filter 10.4 Non-linear state-space models 10.5 Switching state-space models", " Chapter 10 Principle of SSMs Unless specified otherwise, the main source for the descriptions of this section is the book of Simo Särkkä: Bayesian filtering and smoothing 10.1 Description State-space models (SSM) are the continuous-state equivalent of hidden Markov models: the sequence of \\(T\\) output variables \\({y_t}\\) is generated by a parallel sequence of \\(n_x\\) latent continuous state variables \\(x_t \\in \\mathbb{R}^{n_x}\\). Since the letter \\(x\\) is used here to denote the state variable, we denote the explanatory inputs as \\(u_t \\in \\mathbb{R}^{n_u}\\). A Bayesian SSM is fully defined by four distributions: a prior distribution of the parameters \\(\\theta\\); a prior distribution of the first hidden state \\(\\mathbf{x}_0\\); a dynamic model which describes the system dynamics and its uncertainties in terms of the transition probability distribution; a measurement model which describes how the measurements depend on the current state: \\[\\begin{align} \\theta &amp; \\sim p(\\theta) \\nonumber \\\\ \\mathbf{x}_0 &amp; \\sim p(\\mathbf{x}_0) \\nonumber \\\\ \\mathbf{x}_t &amp; \\sim p(\\mathbf{x}_t | \\mathbf{x}_{t-1}, \\theta) \\nonumber \\\\ \\mathbf{y}_t &amp; \\sim p(\\mathbf{y}_t | \\mathbf{x}_t, \\theta) \\tag{10.1} \\end{align}\\] Given the measurements \\(\\mathbf{y}_{1:T}\\), we are then interested in computing: Filtering distributions, i.e. the marginal distribution of each state given the current and previous measurements: \\[ p(\\mathbf{x}_t | \\mathbf{y}_{1:t}, \\theta), \\quad t = 1, \\dots ,T \\] Prediction distributions, the marginal distributions of future states: \\[ p(\\mathbf{x}_{t+k} | \\mathbf{y}_{1:t}, \\theta), \\quad t = 1, \\dots ,T, \\quad k=1,2,\\dots \\] Smoothing distributions, the marginal distributions of each state given all measurements: \\[p(\\mathbf{x}_t | \\mathbf{y}_{1:T}, \\theta), \\quad t = 1, \\dots ,T \\] Parameter estimation, i.e. the posterior density of parameters given all measurements: \\[ p(\\theta | \\mathbf{y}_{1:T}) \\] On-line parameter estimation, i.e. the marginal distribution of parameters given the current and previous measurements: \\[ p(\\theta | \\mathbf{y}_{1:t}), \\quad t = 1, \\dots ,T \\] Filtering, prediction and smoothing are variations of state estimation. Filtering and smoothing a linear Gaussian state-space model are respectively done with the Kalman filter (KF) and the Rauch-Tung-Striebel smoother (RTSS). These methods are not applicable to non-linear and non-Gaussian models, but other alternatives are: extended Kalman filter (EKF) and unscented Kalman filter (UKF), along with their smoother counterparts (ERTSS and URTSS); sequential Monte Carlo, or particle filters and smoothers; etc. Parameter estimation is meant here as the computation of \\(p(\\theta | \\mathbf{y}_{1:T})\\), the marginal posterior of \\(\\theta\\) given all measurements. The way to do this is to perform filtering, which produces the likelihood function \\(p(\\mathbf{y}_{1:T} | \\theta)\\) as a side product. The Maximum Likelihood estimate of \\(\\theta\\) is then obtainable by numerical maximization of the likelihood, otherwise the full posterior density can be approached by one of the Markov Chain Monte Carlo (MCMC) methods. As opposed to batch estimation, on-line estimation means computing \\(p(\\theta | \\mathbf{y}_{1:t})\\) at every time step \\(t\\), i.e. the probability distribution of \\(\\theta\\) given the current and previous measurement. 10.2 Linear state-space models In the linear Gaussian state-space formulation, both the dynamic and the measurement models are linear with respect to the state \\(\\mathbf{x}_t\\) and the inputs \\(\\mathbf{u}_t\\): \\[\\begin{align} \\mathbf{x}_t &amp; = \\mathbf{A}_\\theta \\, \\mathbf{x}_{t-1} + \\mathbf{B}_\\theta \\, \\mathbf{u}_t + \\mathbf{w}_t \\tag{10.2} \\\\ \\mathbf{y}_t &amp; = \\mathbf{C}_\\theta \\, \\mathbf{x}_t + \\mathbf{D}_\\theta \\, \\mathbf{u}_t + \\mathbf{v}_t \\tag{10.3} \\end{align}\\] where the matrices \\(\\mathbf{A}_\\theta\\), \\(\\mathbf{B}_\\theta\\), \\(\\mathbf{C}_\\theta\\) and \\(\\mathbf{D}_\\theta\\) have coefficients as functions of \\(\\theta\\) and are not necessarily time-invariant. \\(\\mathbf{w}_t \\sim N\\left(0, \\mathbf{Q}\\right)\\) is the process noise and \\(\\mathbf{v}_t \\sim N\\left(0, \\mathbf{R}\\right)\\) is the measurement noise. As a practical example of linear state-space models, the simplified resistor-capacitor (RC) model structures are a popular choice for either parameter estimation or system identification. When written as a set of stochastic differential equations, they allow accounting for modelling approximations (Madsen and Holst (1995)) and offer a more reproducible parameter estimation than deterministic models that overlook modelling errors (Rouchier, Rabouille, and Oberlé (2018)). These models will be used in the next chapter for the estimation of thermal properties of building envelopes. Figure 10.1: Second order RC model Consider the example of a simple building represented by a 2-resistor, 2-capacitor model structure (2R2C) as shown here. The equations of this model are: \\[\\begin{align} C_i \\, \\mathrm{d} T_i &amp; = \\dfrac{1}{R_i}\\left(T_e-T_i\\right)\\mathrm{d}t + \\Phi_h \\, \\mathrm{d}t + A_i \\Phi_s \\mathrm{d}t + \\sigma_i \\,\\mathrm{d}\\omega_i \\tag{10.4} \\\\ C_e \\, \\mathrm{d} T_e &amp; = \\dfrac{1}{R_i}\\left(T_i-T_e\\right)\\mathrm{d}t + \\frac{1}{R_o}\\left(T_o-T_e\\right)\\mathrm{d}t + A_e \\Phi_s \\mathrm{d}t + \\sigma_e \\, \\mathrm{d}\\omega_e \\tag{10.5} \\end{align}\\] where \\(T_i\\), \\(T_e\\) and \\(T_o\\) are the indoor, envelope and outdoor temperatures. The envelope temperature is associated with the thermal mass of the opaque surfaces, and does not represent a specific coordinate within the envelope. The model has two states \\(T_e\\) (unobserved) and \\(T_i\\) (observed); \\(\\Phi_h\\) (W) is the indoor heating power; \\(\\Phi_s\\) (W/m\\(^2\\)) is the global horizontal solar irradiance. \\(R_i\\) (K/W) is the thermal resistance between the indoor air temperature and the envelope, \\(R_e\\) the resistance between the envelope and the ambient air. \\(C_i\\) and \\(C_e\\) (J/K) are the heat capacitances of the interior and the envelope, respectively, and \\(A_i\\) and \\(A_e\\) (m\\(^2\\)) are their solar gain coefficients. \\(\\{\\omega_i\\}\\) and \\(\\{\\omega_e\\}\\) are standard Wiener processes and \\(\\sigma_i^2\\) are \\(\\sigma_e^2\\) are their variances. This process noise is a way to account for modelling approximations, unrecognized inputs or noise-corrupted input measurements. Eq. (10.4) and (10.5) can be written in matrix form: \\[\\begin{equation} \\mathrm{d} \\begin{bmatrix} T_i \\\\ T_e \\end{bmatrix} = \\begin{pmatrix} -\\frac{1}{R_i \\, C_i} &amp; \\frac{1}{R_i \\, C_i} \\\\ \\frac{1}{R_i \\, C_e} &amp; -\\frac{1}{R_i \\, C_e}-\\frac{1}{R_e \\, C_e}\\end{pmatrix} \\begin{bmatrix} T_i \\\\ T_e \\end{bmatrix}\\, \\mathrm{d}t + \\begin{pmatrix} 0 &amp; \\frac{1}{C_i} &amp; \\frac{A_i}{C_i} \\\\ \\frac{1}{R_e \\, C_e} &amp; 0 &amp; \\frac{A_e}{C_e} \\end{pmatrix} \\begin{bmatrix} T_o \\\\ \\Phi_h \\\\ \\Phi_s \\end{bmatrix} \\, \\mathrm{d}t + \\mathbf{\\sigma} \\, \\mathrm{d}\\omega \\tag{10.6} \\end{equation}\\] which is the dynamic model of the following stochastic state-space model, written in continuous-discrete form: \\[\\begin{align} \\mathrm{d}\\mathbf{x}(t) &amp; = \\mathbf{A}_\\mathit{rc} \\, \\mathbf{x}(t) \\, \\mathrm{d}t + \\mathbf{B}_\\mathit{rc} \\, \\mathbf{u}(t)\\,\\mathrm{d}t + \\mathbf{\\sigma}_\\theta \\mathrm{d}\\omega \\tag{10.7} \\\\ \\mathbf{y}_t &amp; = \\mathbf{C}_\\theta \\, \\mathbf{x}_t + \\mathbf{v}_t \\tag{10.8} \\end{align}\\] The state vector \\(\\mathbf{x}\\) includes the temperatures \\(T_i\\) and \\(T_e\\) calculated by the model, and \\(\\mathbf{u}=\\left[T_o, \\Phi_h, \\Phi_s\\right]\\) is the input vector including boundary conditions and excitations. The second equation, Eq. (10.8), is the observation equation. It indicates that the measured quantity \\(y_t\\) may be different from the output of the state equation. In our case, the observed temperature is only the first component of the state vector, and is encumbered with some measurement error \\(\\mathbf{v}_t\\). In this equation, time is noted as a subscript to indicate that observations come in a discrete sequence. This model described by Eq. (10.7) must be discretized in order to specify its evolution between discrete time coordinates. Supposing a sample interval length \\(\\Delta t\\) and assume that the inputs \\(\\mathbf{u}(t)\\) are constant during each interval. Eq. (10.7) and (10.8) can be discretized into the system of Eq. (10.2) and (10.3) through the following discretization equations: \\[\\begin{align} \\mathbf{A}_\\theta &amp; = \\mathrm{exp}\\left( \\mathbf{A}_\\mathit{rc} \\, \\Delta t \\right) \\tag{10.9}\\\\ \\mathbf{B}_\\theta &amp; = \\mathbf{A}_\\mathit{rc}^{-1} \\, \\left(\\mathbf{A}_\\theta-\\mathbf{I}\\right) \\, \\mathbf{B}_\\mathit{rc} \\tag{10.10} \\\\ \\mathbf{Q} &amp; = \\int_0^{\\Delta t} \\mathrm{exp}\\left( \\mathbf{A}_\\mathit{rc} \\, \\Delta t \\right) \\, \\mathbf{\\sigma}_\\theta \\, \\mathrm{exp}\\left( \\mathbf{A}_\\mathit{rc}^T \\, \\Delta t \\right) \\mathrm{d}t \\tag{10.11} \\end{align}\\] The Kalman filter is the typical method for computing the marginal distribution of each state \\(p(\\mathbf{x}_t | \\mathbf{y}_{1:t}, \\theta)\\) and the likelihood function \\(p(\\mathbf{y}_{1:t}| \\theta)\\) from which \\(\\theta\\) can be estimated. 10.3 The Kalman filter Given a state transition probability \\(p\\left( \\mathbf{x}_{t} | \\theta, \\mathbf{x}_{t-1}, \\mathbf{u}_{t} \\right)\\) (Eq. (10.2)) and an observation probability \\(p\\left( \\mathbf{y}_{t} | \\mathbf{x}_{t}\\right)\\) (Eq. (10.3)), a Kalman filter produces \\(p\\left(\\mathbf{x}_t|\\mathbf{y}_{1:T}, \\theta \\right)\\), the probability distribution function of each state \\(\\mathbf{x}_t\\) given measurements and parameter values, and the marginal likelihood function \\(L_y(\\theta)=p\\left(\\mathbf{y}_{1:T} | \\theta \\right)\\). Filtering produces \\(p\\left(\\mathbf{x}_t|\\mathbf{y}_{1:N}, \\theta \\right)\\), the probability distribution function of each state \\(\\mathbf{x}_t\\) given measurements and parameter values. In the following, definitions adapted from the book of Shumway et al (Shumway and Stoffer (2000)) are used: \\(\\mathbf{x}_{t|s}\\) is the expected state at time \\(t\\) given observations up to time \\(s\\). \\(\\mathbf{P}_{t|s}\\) is the variance of the state \\(\\mathbf{x}_{t}\\), i.e. the mean-squared error. \\[\\begin{align} \\mathbf{x}_{t|s} &amp; = \\mathrm{E}\\left(\\mathbf{x}_t|\\mathbf{y}_{1:s}, \\theta \\right) \\\\ \\mathbf{P}_{t|s} &amp; = \\mathrm{Var}\\left(\\mathbf{x}_t|\\mathbf{y}_{1:s, \\theta}\\right)= \\mathrm{E}\\left[(\\mathbf{x}_t-\\mathbf{x}_{t|s})(\\mathbf{x}_t-\\mathbf{x}_{t|s})^T|\\mathbf{y}_{1:s}, \\theta \\right] \\end{align}\\] Figure 10.2: Schematic view of one iteration of the Kalman filter The Kalman filter algorithm is described here and illustrated by Fig. 10.2: Set the initial states \\(\\mathbf{x}_{0|0}\\) and their covariance \\(\\mathbf{P}_{0|0}\\) for \\(t=1...T\\): Prediction step: given the previous state \\(\\mathbf{x}_{t|t}\\) and its covariance \\(\\mathbf{P}_{t|t}\\), the model estimates the one-step ahead prediction. \\[\\begin{align} \\mathbf{x}_{t+1|t} &amp; = \\mathbf{A}_\\theta \\, \\mathbf{x}_{t|t} + \\mathbf{B}_\\theta \\, \\mathbf{u}_{t+1}\\\\ \\mathbf{P}_{t+1|t} &amp; = \\mathbf{A}_\\theta \\, \\mathbf{P}_{t|t} \\, \\mathbf{A}_\\theta^T + \\mathbf{Q} \\end{align}\\] Innovations (prediction error) \\(\\varepsilon_{t+1}\\) and their covariances \\(\\Sigma_{t+1}\\) are then calculated, along with the Kalman gain \\(\\mathbf{K}_{t+1}\\), by comparing measurements \\(\\mathbf{y}_{t+1}\\) with the one-step ahead prediction \\(\\mathbf{x}_{t+1|t}\\): \\[\\begin{align} \\varepsilon_{t+1} &amp; = \\mathbf{y}_{t+1} - \\mathbf{C}_\\theta \\, \\mathbf{x}_{t+1|t}\\\\ \\Sigma_{t+1} &amp; = \\mathbf{C}_\\theta \\, \\mathbf{P}_{t+1|t} \\, \\mathbf{C}_\\theta^T + \\mathbf{R} \\\\ \\mathbf{K}_{t+1} &amp; = \\mathbf{P}_{t+1|t} \\, \\mathbf{C}_\\theta^T \\, \\Sigma_{t+1}^{-1} \\end{align}\\] Updating step: the new states at time \\(t+1\\) are updated, as a compromise between the one-step ahead prediction and the measurement. \\[\\begin{align} \\mathbf{x}_{t+1|t+1} &amp; = \\mathbf{x}_{t+1|t} + \\mathbf{K}_{t+1} \\, \\mathbf{\\varepsilon}_{t+1} \\\\ \\mathbf{P}_{t+1|t+1} &amp; = \\left( \\mathbf{I}- \\mathbf{K}_{t+1} \\, \\mathbf{C}_\\theta \\right) \\, \\mathbf{P}_{t+1|t} \\end{align}\\] The total (negative) log-likelihood can be calculated up to a normalizing constant: \\[\\begin{equation} -\\ln L_y(\\theta) = \\frac{1}{2} \\sum_{t=1}^{T} \\ln \\left|\\Sigma_t(\\theta)\\right| + \\frac{1}{2} \\sum_{t=1}^{T} \\varepsilon_t(\\theta)^T \\, \\Sigma_t(\\theta)^{-1} \\, \\varepsilon_t(\\theta) \\end{equation}\\] Roughly speaking, the Kalman filter applies Bayes rule at each time step: the updated state \\(p(\\mathbf{x}_t|\\mathbf{y}_{1:t})=N(\\mathbf{x}_{t|t}, \\mathbf{P}_{t|t})\\) is a posterior distribution, obtained from a compromise between a prior output of the model \\(p(\\mathbf{x}_t|\\mathbf{y}_{1:t-1})=N(\\mathbf{x}_{t|t-1}, \\mathbf{P}_{t|t-1})\\) and the evidence brought by measurements \\(\\mathbf{y}_t\\). Their relative weight is expressed by the Kalman gain \\(\\mathbf{K}_t\\) that measures the relative confidence we put in both the model and the measurements. 10.4 Non-linear state-space models State-space models are not necessarily linear or Gaussian. In a more generic situation, the system of equations (10.1) can take the form: \\[\\begin{align} \\mathbf{x}_t &amp; = F(\\mathbf{x}_{t-1}, \\mathbf{u}_t, \\mathbf{w}_t) \\tag{10.12} \\\\ \\mathbf{y}_t &amp; = G(\\mathbf{x}_t, \\mathbf{u}_t, \\mathbf{v}_t) \\tag{10.13} \\end{align}\\] The most common extensions of the Kalman filter to non-linear systems are the extended Kalman filter (EKF) and unscented Kalman filter (UKF). The EKF approximates the non-linear and non-Gaussian measurement and dynamic models by linearization at the nominal solution. The unscented Kalman filter (UKF) approximates the propagation of densities through the non-linearities of measurement and noise processes using the unscented transform. They are both Gaussian approximations. Non-linearity in a building energy state-space model can arise in different situations: The states can include not only the temperatures simulated by an RC model. By using fictitious process equations to augment the state vector with model parameters, it is possible to perform on-line joint state-parameter estimation. To encode time-varying uncertainty in some of the model inputs: states can be augmented with additional state variables describing these varying unknown parameters. For non-linear non-Gaussian state-space models, particle filters, or sequential Monte Carlo (SMC) methods, are now a popular alternative to the EKF because of their suitability for parallel implementation and for on-line inference (Cappé, Godsill, and Moulines (2007)). SMC filters and smoothers represent the posterior distribution of states as a weighted set of Monte Carlo samples. They are applicable to joint state-parameter estimation and have already been applied to the characterization of building envelope properties (Rouchier, Jiménez, and Castaño (2019)). In a non-Bayesian framework, particle methods can approximate the likelihood function which may then be maximized for an off-line or on-line ML estimation of parameters (Kantas et al. (2015)). In Bayesian joint state-parameter estimation, the target distribution \\(p(\\theta, x_{1:T} | y_{1:T})\\) can be approached in several ways: Particle MCMC methods (Andrieu, Doucet, and Holenstein (2010)) sample from \\(\\theta\\) with a MCMC method, within which particle filters are called to approximate the marginal state distributions; the SMC\\(^2\\) algorithm (Chopin, Jacob, and Papaspiliopoulos (2013)) couples a SMC algorithm in the \\(\\theta\\)-dimension, which propagates and resamples many particle filters in the \\(x\\)-dimension. 10.5 Switching state-space models The last form of statistical model we will mention here are the state-space models with regime switching, or switching state-space models (SSSM), or switching dynamical systems. They are state-space models containing both categorical and continuous hidden variables, and thus a sort of coalition between hidden Markov models and state-space models. Figure 10.3: Switching dynamic model One of the forms of SSSMs is shown here, where the continuous state variable \\(X\\) is conditioned on a categorical latent variable \\(S\\) denoted the switch. Like in an HMM or MSM, the switch refers to one of several possible regimes the system can be in. The solutions of such models with a fixed number of modes of operation can be approximated by several methods Generalized pseudo-Bayesian methods, and the interacting multiple model (IMM) algorithm (Blom and Bar-Shalom (1988)), based on forming a mixture of Kalman filters; Expectation propagation (Zoeter and Heskes (2011)); Rao-Blackwellised particle filters (Doucet et al. (2000)) use closed form integration (Kalman filters) for some of the state variables and Monte Carlo integration for others (Särkkä (2013)). References "],["a-simple-rc-model-python.html", "Chapter 11 A simple RC model (Python) 11.1 Case study 11.2 Modelling 11.3 Deterministic formulation 11.4 Stochastic formulation", " Chapter 11 A simple RC model (Python) As a practical example of linear state-space models, the simplified resistor-capacitor (RC) model structures are a popular choice for either parameter estimation or system identification. When written as a set of stochastic differential equations, they allow accounting for modelling approximations (Madsen and Holst (1995)) and offer a more reproducible parameter estimation than deterministic models that overlook modelling errors (Rouchier, Rabouille, and Oberlé (2018)). This Python tutorial shows an RC model being used to estimate the heat resistance and capacitance of a test house. The model will be implemented twice: the first time in a deterministic formulation, without system uncertainty; the second time in a stochastic formulation, solved by the Kalman filter. 11.1 Case study The experimental test cell used in this study is called the Armadillo Box. It is a demonstration building of 42 m\\(^2\\) floor area, designed for the 2010 European Solar Decathlon by the ENSAG-GAIA-INES team. The envelope is a light wood framed construction with integrated insulation. Heating and cooling is performed by a 3 in 1 heat pump, and photovoltaic solar panels provide recharge for electric vehicles. import pandas as pd import numpy as np import matplotlib.pyplot as plt df = pd.read_csv(&#39;data/statespace.csv&#39;) The building is monitored by a variety of sensors, but the present study only uses records of indoor temperature and prescribed heating power, in addition to weather data. The indoor temperature profiles used here have been averaged over several sensors distributed in the living space. The measurement time is 4 days, during which there is a 24 hour heat input. Note from the author: I chose not to display the code that generated the figures in this page. This is not just for clarity, but mostly because matplotlib integrates poorly into Rmarkdown documents. 11.2 Modelling 11.2.1 RC model Consider the example of a simple building represented by a 2-resistor, 2-capacitor model structure (2R2C), described in Sec. 10.2. The equations of this model are: \\[\\begin{align} C_i \\, \\mathrm{d} T_i &amp; = \\dfrac{1}{R_i}\\left(T_e-T_i\\right)\\mathrm{d}t + \\Phi_h \\, \\mathrm{d}t + A_i \\Phi_s \\mathrm{d}t + \\sigma_i \\,\\mathrm{d}\\omega_i \\tag{11.1} \\\\ C_e \\, \\mathrm{d} T_e &amp; = \\dfrac{1}{R_i}\\left(T_i-T_e\\right)\\mathrm{d}t + \\frac{1}{R_o}\\left(T_o-T_e\\right)\\mathrm{d}t + A_e \\Phi_s \\mathrm{d}t + \\sigma_e \\, \\mathrm{d}\\omega_e \\tag{11.2} \\end{align}\\] where \\(T_i\\), \\(T_e\\) and \\(T_o\\) are the indoor, envelope and outdoor temperatures. The envelope temperature is associated with the thermal mass of the opaque surfaces, and does not represent a specific coordinate within the envelope. The model has two states \\(T_e\\) (unobserved) and \\(T_i\\) (observed); \\(\\Phi_h\\) (W) is the indoor heating power; \\(\\Phi_s\\) (W/m\\(^2\\)) is the global horizontal solar irradiance. \\(R_i\\) (K/W) is the thermal resistance between the indoor air temperature and the envelope, \\(R_e\\) the resistance between the envelope and the ambient air. \\(C_i\\) and \\(C_e\\) (J/K) are the heat capacitances of the interior and the envelope, respectively, and \\(A_i\\) and \\(A_e\\) (m\\(^2\\)) are their solar gain coefficients. \\(\\{\\omega_i\\}\\) and \\(\\{\\omega_e\\}\\) are standard Wiener processes and \\(\\sigma_i^2\\) are \\(\\sigma_e^2\\) are their variances. This process noise is a way to account for modelling approximations, unrecognized inputs or noise-corrupted input measurements. Despite its simplicity, this model structure is able to reproduce the thermal behaviour of a simple unoccupied building. The equations can be written in matrix form: \\[\\begin{align} \\mathrm{d} \\begin{bmatrix} T_i \\\\ T_e \\end{bmatrix} &amp; = \\begin{pmatrix} -\\frac{1}{R_i \\, C_i} &amp; \\frac{1}{R_i \\, C_i} \\\\ \\frac{1}{R_i \\, C_e} &amp; -\\frac{1}{R_i \\, C_e}-\\frac{1}{R_e \\, C_e}\\end{pmatrix} \\begin{bmatrix} T_i \\\\ T_e \\end{bmatrix}\\, \\mathrm{d}t \\\\ &amp; + \\begin{pmatrix} 0 &amp; \\frac{1}{C_i} &amp; \\frac{A_i}{C_i} \\\\ \\frac{1}{R_o \\, C_e} &amp; 0 &amp; \\frac{A_e}{C_e} \\end{pmatrix} \\begin{bmatrix} T_o \\\\ \\Phi_h \\\\ \\Phi_s \\end{bmatrix} \\, \\mathrm{d}t + \\mathbf{\\sigma} \\, \\mathrm{d}\\omega \\tag{11.3} \\end{align}\\] which is the dynamic model of the following stochastic state-space model, written in continuous-discrete form: \\[\\begin{align} \\mathrm{d}\\mathbf{x}(t) &amp; = \\mathbf{A}_\\mathit{rc} \\, \\mathbf{x}(t) \\, \\mathrm{d}t + \\mathbf{B}_\\mathit{rc} \\, \\mathbf{u}(t)\\,\\mathrm{d}t + \\mathbf{\\sigma}_\\theta \\mathrm{d}\\omega \\tag{11.4} \\\\ \\mathbf{y}_t &amp; = \\mathbf{C}_\\theta \\, \\mathbf{x}_t + \\mathbf{v}_t \\tag{11.5} \\end{align}\\] The state vector \\(\\mathbf{x}\\) includes the temperatures \\(T_i\\) and \\(T_e\\) calculated by the model, and \\(\\mathbf{u}=\\left[T_o, \\Phi_h, \\Phi_s\\right]\\) is the input vector including boundary conditions and excitations. The second equation is the observation equation. It indicates that the measured quantity \\(y_t\\) may be different from the output of the state equation. In our case, the observed temperature is only the first component of the state vector, and is encumbered with some measurement error \\(\\mathbf{v}_t\\). In this equation, time is noted as a subscript to indicate that observations come in a discrete sequence. 11.3 Deterministic formulation We first consider the most simple implementation of the model: if the modelling errors \\(\\{\\omega_i\\}\\) and \\(\\{\\omega_e\\}\\) are neglected. This is what we call a deterministic formulation, as this hypothesis is equivalent to assuming that the model is absolutely correct in reproducing the behaviour of the house. Perhaps the easiest way to solve Eq. (11.3) for the temperatures is an explicit discretisation scheme: the indoor and envelope temperatures at time \\(t+1\\) are functions of all variables at time \\(t\\): \\[\\begin{align} T_i^{(t+1)} &amp; = T_i^{(t)} + \\frac{\\mathrm{d}t}{C_i} \\left( \\dfrac{1}{R_i}\\left(T_e-T_i\\right) + \\Phi_h + A_i \\Phi_s \\right)^{(t)} \\tag{11.6} \\\\ T_e^{(t+1)} &amp; = T_e^{(t)} + \\frac{\\mathrm{d}t}{C_e} \\left( \\dfrac{1}{R_i}\\left(T_i-T_e\\right) + \\frac{1}{R_o} \\left(T_o-T_e\\right) + A_e \\Phi_s \\Phi_s\\right)^{(t)} \\tag{11.7} \\end{align}\\] In Python, we define this model in a function that takes the inputs \\(\\mathbf{u}\\) as first argument, and the model parameters as other arguments. This syntax is necessary so that we can call Scipys curve_fit method later. The function returns the indoor temperature, to be compared with the measurements. inputs = df[[&#39;Time&#39;, &#39;T_ext&#39;, &#39;P_hea&#39;, &#39;I_sol&#39;]].values output = df[&#39;T_int&#39;].values # Function that will calculate the indoor temperature for a given value of R and C def simpleRC(inputs, Ri, Ro, Ci, Ce, Ai, Ae): time = inputs[:, 0] to = inputs[:, 1] phi_h = inputs[:, 2] phi_s = inputs[:, 3] ti = np.zeros(len(time)) te = np.zeros(len(time)) # Initial temperatures ti[0] = output[0] te[0] = ( Ri*to[0] + Ro*ti[0] ) / (Ri + Ro) # Loop for calculating all temperatures for t in range(1, len(output)): dt = time[t] - time[t-1] ti[t] = ti[t-1] + dt / Ci * ( (te[t-1]-ti[t-1])/Ri + phi_h[t-1] + Ai*phi_s[t-1] ) te[t] = te[t-1] + dt / Ce * ( (ti[t-1]-te[t-1])/Ri + (to[t-1]-te[t-1])/Ro + Ae*phi_s[t-1] ) return ti We can find the optimal parameters \\(\\theta=(R_i, R_o, C_i, C_e, A_i, A_e)\\) by minimizing the sum of squared residuals between the measured indoor temperature and the output of the function we just defined. This is equivalent to what the curve_fit method of Scipy does here. The method requires initial parameter values. They can be picked by running a few simulations first until the predictions are approximately in the right range. This step is not detailed here. import scipy.optimize as so p_opt, p_cov = so.curve_fit(f=simpleRC, xdata=inputs, ydata=output, p0=(0.01, 0.01, 1e6, 1e7, 5, 5)) # Saving results into a dataframe and displaying it res1 = pd.DataFrame(index=[&#39;Ri&#39;, &#39;Ro&#39;, &#39;Ci&#39;, &#39;Ce&#39;, &#39;Ai&#39;, &#39;Ae&#39;]) res1[&#39;avg&#39;] = p_opt res1[&#39;std&#39;] = np.diag(p_cov)**0.5 print(res1) ## avg std ## Ri 4.091570e-03 0.000125 ## Ro 5.038526e-02 0.004552 ## Ci 5.344581e+06 225321.203258 ## Ce 1.761891e+07 841478.831398 ## Ai 1.861247e-01 0.055401 ## Ae -1.571828e+00 0.096530 The estimated parameters seem consistent with our expectations. We can compare the profile of measured indoor temperature with the output that the model predicts given the identified optimal parameters. 11.4 Stochastic formulation In more realistic applications, the phenomena that impact the energy balance of a building are far more complex than what such a simple model may predict. We extend the RC model structure in order to include effects of solar irradiance, ventilation, several time constants of inertia But there is a limit to how complex a model can be made, before parameter identification becomes infeasible. If a model does not fully describe the thermal behaviour of the building, and this discrepancy is not somehow explicitely accounted for, then the parameter estimates and predictions will be biased and unreliable. Stochastic state-space models are a way to account for modelling uncertainties and are now standard practice in the field of building energy performance assessment. 11.4.1 Specification The basics of linear state space models has been explained in Sec. 10.2 and the principle of the Kalman filter in Sec. 10.3. In the following block, a class RC is defined which implements these equations for any RC model. Any new RC model can then be inherited from this class, by specifying its matrices \\(\\mathbf{A}_\\mathit{rc}\\), \\(\\mathbf{A}_\\mathit{rc}\\) and \\(\\mathbf{A}_\\mathit{rc}\\), and the covariance matrices of the modelling and measurement errors \\(\\mathbf{Q}\\) and \\(\\mathbf{R}\\) (see eq. (11.4) and (11.5)). from scipy.linalg import expm from numpy.linalg import inv class RC(object): &quot;&quot;&quot;This is the generic class for any RC model structure&quot;&quot;&quot; def __init__(self): pass def discretize(self, dt): &quot;&quot;&quot; This method applies the discretisation equations shown in the previous chapter It is only function of the time step size dt &quot;&quot;&quot; n = self.N_states # Discretisation F = expm(self.Ac * dt) G = np.dot(inv(self.Ac), np.dot(F - np.eye(n), self.Bc)) H = self.Cc # System error covariance matrix (continuous Qc and discrete Q) foo = expm(np.block([[-self.Ac, self.Qc], [np.zeros(np.shape(self.Ac)), self.Ac.T]]) * dt) Q = np.dot(foo[n:2 * n, n:2 * n].T, foo[0:n, n:2 * n]) # Measurement error covariance matrix (discrete) R = self.Rc / dt return F, G, H, Q, R def prediction(self, x_0, t, u, y=None, update=False): &quot;&quot;&quot; This method predicts the indoor temperature. If update=True, the function will apply all steps of the Kalman filter and return the mean state, covariances of innovations and the likelihood If update=False, the function will only predict states and their variance, as if no measurements were available. Use this for forecasting. x_0: initial state vector t: vector of time coordinates u: input vector y: measurement vector (only required if update=True) &quot;&quot;&quot; N_time = len(t) # Initialisation of all arrays that will be used in the calculations # Mean and variance of the states calculated at the prediction step x_avg_predict = np.zeros((N_time, self.N_states)) x_var_predict = np.zeros((N_time, self.N_states, self.N_states)) x_avg_predict[0] = x_0 x_var_predict[0] = np.eye(self.N_states) x_avg_update = np.zeros((N_time, self.N_states)) x_var_update = np.zeros((N_time, self.N_states, self.N_states)) x_avg_update[0] = x_0 x_var_update[0] = np.eye(self.N_states) epsilon = np.zeros(N_time) # prediction errors (innovations) Sigma = np.zeros(N_time) # innovation covariances loglike = np.zeros(N_time) # log-likelihood to be minimized for i in range(1, N_time): # Matrices of the current time step (depend on the time step size dt) dt = t[i] - t[i - 1] F, G, H, Q, R = self.discretize(dt) # KALMAN 1: Prediction step x_avg_predict[i] = np.dot(F, x_avg_update[i-1]) + np.dot(G, u[i]) x_var_predict[i] = np.dot(F, np.dot(x_var_update[i-1], F.T)) + Q if update: # KALMAN 2: Residuals and Kalman gain epsilon[i] = y[i] - np.dot(H, x_avg_predict[i]) foo = np.dot(H, np.dot(x_var_predict[i], H.T)) + R Sigma[i] = foo K = np.dot(x_var_predict[i], np.dot(H.T, np.linalg.inv(foo))) loglike[i] = -0.5 * epsilon[i] ** 2 / Sigma[i] - 0.5 * Sigma[i] * 2 * np.pi # KALMAN 3: Update and weight x_avg_update[i] = x_avg_predict[i] + np.dot(K, y[i] - np.dot(H, x_avg_predict[i])) x_var_update[i] = x_var_predict[i] - np.dot(K, np.dot(H, x_var_predict[i])) else: x_avg_update[i] = x_avg_predict[i] x_var_update[i] = x_var_predict[i] # Returns if update: # If the Kalman update was used, return X = np.dot(H, x_avg_predict.T).flatten() S = Sigma.flatten() return X, S, loglike else: X_avg = np.dot(H, x_avg_predict.T).flatten() X_var = np.dot(H, np.dot(x_var_predict, H.T)).flatten() return X_avg, X_var The 2-resistor, 2-capacitor model drawn above is then inherited from this RC class. Upon definition, it takes its parameters as arguments. class tite(RC): def __init__(self, Ri, Ro, Ci, Ce, Ai, Ae, qi, qe, r): &quot;&quot;&quot; Model inputs: ambient temperature, heating, solar irradiance&quot;&quot;&quot; # Model parameters self.Ri = Ri # The first resistance (K/W) of the model, on the indoor side self.Ro = Ro # The second resistance (K/W) of the model, on the outdoor side self.Ci = Ci # Heat capacity (J/K) connected to the indoor temperature self.Ce = Ce # Heat capacity (J/K) connected to the envelope temperature self.Ai = Ai # Solar aperture coefficient directed to the indoor temperature self.Ae = Ae # Solar aperture coefficient directed to the envelope temperature self.qi = qi # Standard deviation of the modelling error on Ti self.qe = qe # Standard deviation of the modelling error on Ti self.r = r # Standard deviation of the measurement error # Number of states self.N_states = 2 # Matrices of the continuous system self.Ac = np.array([[-1/(Ci*Ri), 1/(Ci*Ri)], [1/(Ce*Ri), -1/(Ce*Ri) - 1/(Ce*Ro)]]) self.Bc = np.array([[0, 1 / Ci, Ai / Ci], [1/(Ce*Ro), 0, Ae/Ce]]) self.Cc = np.array([[1, 0]]) # System and measurement error covariance matrices self.Qc = np.diag([qi**2, qe**2]) self.Rc = np.array([[r ** 2]]) Notice that this model has four additional parameters compared to the deterministic formulation above: the variance of the modelling errors on each state qi and qe, and the measurement error r. 11.4.2 Training We now need to find the parameters that will offer the best fit between predictions of the 2R2C model and indoor temperature measurements. We will simply use the curve_fit method from Scipy again. In order to facilitate convergence, an initial parameter value is first defined. Then, the optimiser will act on an evaluation function that receives each parameter as argument, and returns the likelihood value. Some parameters are log-transformed in order to allow the search over larger scales. # Definition of an evaluation function that takes &quot;normalized&quot; values or each parameter def evaluation(df, Ri, Ro, Ci, Ce, Ai, Ae, qi, qe, r, xe0): # Reading the dataframe given as argument t = df[&#39;Time&#39;].values u = df[[&#39;T_ext&#39;, &#39;P_hea&#39;, &#39;I_sol&#39;]].values y = df[&#39;T_int&#39;].values # Model specification and initial condition model = tite(Ri, Ro, Ci, Ce, Ai, Ae, qi, qe, r) x_0 = [y[0], xe0] # Model prediction, without Kalman update X, S, L = model.prediction(x_0, t, u, y, update=True) # We only need the likelihood for the minimize method return X from scipy.optimize import curve_fit # Curve fitting happens here. popt and pcov are the mean and covariance of parameter estimates popt, pcov = curve_fit(evaluation, xdata = df, ydata = df[&#39;T_int&#39;], p0 = np.append(res1[&#39;avg&#39;], [1e-3, 1e-3, 1e-1, 20]) , method=&#39;lm&#39;) res2 = pd.DataFrame(index=[&#39;Ri&#39;, &#39;Ro&#39;, &#39;Ci&#39;, &#39;Ce&#39;, &#39;Ai&#39;, &#39;Ae&#39;, &#39;qi&#39;, &#39;qe&#39;, &#39;r&#39;, &#39;xe0&#39;]) res2[&#39;avg&#39;] = popt res2[&#39;std&#39;] = np.diag(pcov)**0.5 print(res2) ## avg std ## Ri 2.799801e-03 0.000125 ## Ro 1.680273e-02 0.001284 ## Ci 3.773714e+06 152372.536275 ## Ce 1.469854e+07 876101.583123 ## Ai 1.561966e-01 0.045091 ## Ae -3.933333e-02 0.124889 ## qi 8.516594e-09 0.000244 ## qe 6.275908e-06 0.008552 ## r -2.055527e-02 28.008709 ## xe0 2.992531e+01 0.562491 Notice that Ae, the solar gain coefficient on the envelope capacity, has an average value lower than its standard deviation: this parameter is likely not influential on the outcome. 11.4.3 Diagnostics and residuals analysis In order to evaluate the agreement between the fitted model and the data, let us first simply view the model output in prediction mode (without Kalman update at every time step). # Model specification with the mean of optimised parameters model_opt = tite(*res2[&#39;avg&#39;][:-1]) # Initial states t = df[&#39;Time&#39;].values u = df[[&#39;T_ext&#39;, &#39;P_hea&#39;, &#39;I_sol&#39;]].values y = df[&#39;T_int&#39;].values x_0 = [y[0], res2[&#39;avg&#39;][&#39;xe0&#39;] ] # Prediction without Kalman update, to be compared with the data x_avg, x_var = model_opt.prediction(x_0, t, u, y=None, update=False) The model fits the data well. The 95% confidence intervals are relatively narrow, indicating a good confidence in the prediction. Another important criterion on which to judge the fitted model is the autocorrelation function (ACF) of the one-step ahead prediction residuals. In order to view these residuals, the prediction function should be run with the Kalman updating switched on, so that the states it returns are one-step ahead predictions only. X, S, L = model_opt.prediction(x_0, t, u, y, update=True) # eps_ are the one-step ahead prediction residuals eps_ = y - X # Let&#39;s only take every other value, so that we have time lags of one hour. eps_ = eps_[::2] # Correlation function of two time series. Can be used for the autocorrelation of a single time series def crosscorrelation(x, u): &quot;&quot;&quot; http://stackoverflow.com/q/14297012/190597 http://en.wikipedia.org/wiki/Autocorrelation#Estimation &quot;&quot;&quot; n = len(x) x = x-x.mean() u = u-u.mean() r = np.correlate(x, u, mode = &#39;full&#39;)[-n:] return r/(x.std()*u.std()*(np.arange(n, 0, -1))) # Autocorrelation of the residuals (only keeping the first 50 lags) ACF = crosscorrelation(eps_, eps_)[0:50] lags = np.linspace(0,49,50) The residuals are low, except from one important peak at 24h lag. This suggests that an important influence occuring with a period of 24 hours has been insufficiently accounted for by the model. References "],["the-pysip-library-python.html", "Chapter 12 The pySIP library (Python)", " Chapter 12 The pySIP library (Python) The previous chapters on this series showed the theory behind linear stochastic state-space models. The theory was then applied by calibrating a simple 2R2C model on the heat dynamics of an unoccupied test house. The purpose was to give a transparent overview of the mechanics of state-space models and the Kalman filter. In practical applications however, we will wish to ensure that we obtained a robust predictive model, suitable for building energy performance assessment and prediction of energy use. Additional tools are required to assist in model selection, diagnostics of convergence, validation of calibrated models, and eventually Bayesian inference. This is why the pySIP python library for stochastic state-space inference and prediction was developed. pySIP provides a framework for infering continuous time linear stochastic state-space models. For that purpose, it is possible to chose between a frequentist and a Bayesian workflow. Each workflow allows to estimate the parameters, assess the inference and model reliability, and perform model selection. This page is an example of how pySIP can be used, with the same data and model structure as the previous notebook on state-space models. The following code is provided as an example in the pySIP repository. We are considering the same experimental building, and the same measurement period, as in the previous notebook. The building is monitored by a variety of sensors, but the present study only uses records of indoor temperature and prescribed heating power, in addition to weather data. The indoor temperature profiles used here have been averaged over several sensors distributed in the living space. An experimental sequence of four days was used in this study. Some imports first, then we load the data. import pandas as pd import numpy as np from pysip.statespace import TwTi_RoRi from pysip.regressors import FreqRegressor as Regressor # Reading the data df = pd.read_csv(&#39;data/statespace.csv&#39;).set_index(&#39;Time&#39;) df.drop(df.index[-1], axis=0, inplace=True) # pySIP&#39;s fit() method takes a dataframe df as argument # It also needs to be passed the labels of inputs and outputs in this dataframe inputs = [&#39;T_ext&#39;, &#39;P_hea&#39;] outputs = &#39;T_int&#39; # The time can be scaled on daily units instead of seconds. # This brings heat capacities to a range closer to other variables sT = 3600.0 * 24.0 df.index /= sT We imported the TwTi_RoRi model, which is similar to the 2R2C model of the previous notebook, except that solar aperture coefficients are removed. This is fine since their estimated values were very low. Parameters are specified as a list of dictionaries. Each parameter has a value in the constrained space \\(\\theta\\) and in the unconstrained space \\(\\eta\\). These two values are linked by the relation \\[ \\theta = \\mathrm{loc} + f(\\eta) . \\mathrm{scale}\\] where loc and scale are keywords of a parameters dictionary, indicating location and scaling values. A parameter may be transformed by a function \\(f\\), in order to search on a log scale for instance. More options on parameter specification are given in the documentation pySIP works with either a frequentist and a Bayesian regressor. The frequentist regressor, used in this example, applies maximum likelihood estimation by the BFGS algorithm. # Specification of parameters parameters = [ dict(name=&#39;Ro&#39;, value=1.0, scale=0.1, transform=&#39;log&#39;), dict(name=&#39;Ri&#39;, value=1.0, scale=0.01, transform=&#39;log&#39;), dict(name=&#39;Cw&#39;, value=1.0, scale=1e7 / sT, transform=&#39;log&#39;), dict(name=&#39;Ci&#39;, value=1.0, scale=1e6 / sT, transform=&#39;log&#39;), dict(name=&#39;sigw_w&#39;, value=1.0, scale=0.01 * sT ** 0.5, transform=&#39;log&#39;), dict(name=&#39;sigw_i&#39;, value=1.0, scale=0.01 * sT ** 0.5, transform=&#39;log&#39;), dict(name=&#39;sigv&#39;, value=1.0, scale=0.01, transform=&#39;log&#39;), dict(name=&#39;x0_w&#39;, value=1.0, scale=25.0, transform=&#39;log&#39;), dict(name=&#39;x0_i&#39;, value=26.7), dict(name=&#39;sigx0_w&#39;, value=1.0, transform=&#39;fixed&#39;), dict(name=&#39;sigx0_i&#39;, value=1.0, transform=&#39;fixed&#39;), ] # Definition of the regressor, and model fitting reg = Regressor(TwTi_RoRi(parameters, hold_order=1)) out = reg.fit(df=df, inputs=inputs, outputs=outputs) ## Optimization terminated successfully. ## Current function value: -179.300653 ## Iterations: 43 ## Function evaluations: 53 ## Gradient evaluations: 53 print(out[0]) ##  () pvalue |g()| |dpen()| ## Ro 1.937e-02 1.459e-03 0.000e+00 3.125e-05 2.664e-15 ## Ri 2.276e-03 9.802e-05 0.000e+00 3.569e-05 1.930e-15 ## Cw 1.671e+02 1.004e+01 0.000e+00 2.702e-05 4.798e-17 ## Ci 3.026e+01 1.409e+00 0.000e+00 1.721e-05 1.463e-17 ## sigw_w 5.120e-01 6.462e-02 2.909e-13 8.486e-06 3.296e-15 ## sigw_i 3.119e-04 3.995e-02 9.938e-01 1.056e-05 8.879e-09 ## sigv 6.697e-02 3.994e-03 0.000e+00 2.690e-05 2.229e-18 ## x0_w 3.010e+01 1.054e+00 0.000e+00 4.369e-05 6.896e-17 ## x0_i 3.027e+01 9.349e-01 0.000e+00 1.228e-06 0.000e+00 Note that the parameter names are different than on the previous notebook, but the estimation results are similar. The calculation is much faster than what we did with scipys curve_fit method, and also more reliable because of the choice of algorithm. Finally, we can compare the fitted model output with the data. "],["gaussian-process-models.html", "Chapter 13 Gaussian Process models 13.1 Principle 13.2 Gaussian Processes for prediction of energy use 13.3 Gaussian Processes for time series data 13.4 Latent Force Models", " Chapter 13 Gaussian Process models 13.1 Principle In machine learning, Gaussian Process (GP) regression is a widely used tool for solving modelling problems (Rasmussen (2003)). The appeal of GP models comes from their flexibility and ease of encoding prior information into the model. A GP is a generalization of the Gaussian probability distribution to infinite dimensions. Instead of having a mean vector and a covariance matrix, the Gaussian process \\(f(\\mathbf{x})\\) is a random function in a d-dimensional input space, characterized by a mean function \\(\\mu: \\mathbb{R}^d \\rightarrow \\mathbb{R}\\) and a covariance function \\(\\kappa: \\mathbb{R}^{d\\times d} \\rightarrow \\mathbb{R}\\) \\[\\begin{equation} f\\left(\\mathbf{x}\\right) \\sim \\mathrm{GP}(\\mu(\\mathbf{x}),\\,\\kappa(\\mathbf{x}, \\mathbf{x}^\\prime)) \\tag{13.1} \\end{equation}\\] The variable \\(\\mathbf{x}\\) is the input of the Gaussian process and not the state vector defined in the previous section. The notation of equation (13.1) implies that any finite collection of random variables \\(\\{f(\\mathbf{x}_i)\\}^n_{i=1}\\) has a multidimensional Gaussian distribution (Gaussian process prior) \\[\\begin{equation} \\left\\{f(\\mathbf{x}_1), f(\\mathbf{x}_2), \\ldots, f(\\mathbf{x}_n)\\right\\} \\sim \\mathcal{N}(\\mathbf{\\mu}, \\mathbf{K}) \\tag{13.2} \\end{equation}\\] where \\(\\mathbf{K}_{i,\\,j} = \\kappa(\\mathbf{x}_i, \\mathbf{x}_j)\\) defines the covariance matrix and \\(\\mathbf{\\mu}_i = \\mu(\\mathbf{x}_i)\\) the mean vector, for \\(i,j = 1,2,\\ldots,n\\). The mean function is often, without loss of generality, fixed to zero (e.g. \\(\\mu(\\mathbf{x}) = \\mathbf{0}\\)) if no prior information is available; assumption regarding the mean behavior of the process can be encoded into the covariance function instead (Solin et al.). Indeed, the choice of covariance function allows encoding any prior belief about the properties of the stochatic process \\(f(\\mathbf{x})\\), e.g. linearity, smoothness, periodicity, etc. New covariance functions can be formulated by combining existing covariance functions. The sum \\(\\kappa(\\mathbf{x}, \\mathbf{x}^\\prime) = \\kappa_1(\\mathbf{x}, \\mathbf{x}^\\prime) + \\kappa_2(\\mathbf{x}, \\mathbf{x}^\\prime)\\), or the product \\(\\kappa(\\mathbf{x}, \\mathbf{x}^\\prime) = \\kappa_1(\\mathbf{x}, \\mathbf{x}^\\prime) \\times \\kappa_2(\\mathbf{x}, \\mathbf{x}^\\prime)\\) of two covariance functions is a valid covariance function. The Gaussian process regression is concerned by the problem of estimating the value of an unknown function \\(f(t)\\) at arbitrary time instant \\(t\\) (i.e. test point) based on a noisy training data \\(\\mathcal{D} = \\left\\{t_k, y_k\\right\\}^n_{k=1}\\) \\[\\begin{align} f(t) &amp; \\sim \\mathrm{GP}(0, \\kappa(t, t^\\prime)) \\\\ y_k &amp; = f(t_k) + v_k \\tag{13.3} \\end{align}\\] The joint distribution between the test point \\(f(t)\\) and the training points \\(\\left(f(t_1),\\,f(t_2),\\,\\ldots,\\,f(t_n)\\right)\\) is Gaussian with known statistics. Because the measurement model in equation @ref(gp_model) is linear and Gaussian, the joint distribution between the test point \\(f(t)\\) and the measurements \\(\\left(y_1,\\,y_2,\\,\\ldots,\\,y_n\\right)\\) is Gaussian with known statistics as well. From the property of the Gaussian distribution, the conditional distribution of \\(f(t)\\) given the measurements has an analytical solution (Särkkä and Solin (2019)). \\[\\begin{equation} p\\left(f(t) \\mid \\mathbf{y}\\right) = p\\left(\\mathbb{E}[f(t)] \\mid \\mathbb{V}[f(t)]\\right) \\tag{13.4} \\end{equation}\\] with mean and variance \\[\\begin{align} \\mathbb{E}[f(t)] &amp;= \\mathbf{k}^\\text{T}\\,\\left( \\mathbf{K} + \\sigma^2_\\varepsilon\\,\\mathbf{I}\\right)^{-1}\\,\\mathbf{y} \\\\ \\mathbb{V}[f(t)] &amp;= \\kappa(t, t) - \\mathbf{k}^\\text{T}\\, \\left(\\mathbf{K} + \\sigma^2_\\varepsilon\\,\\mathbf{I}\\right)^{-1}\\,\\mathbf{k} \\tag{13.5} \\end{align}\\] where \\(\\mathbf{K}_{i,\\,j} = \\kappa(t_i, t_j)\\), \\(\\mathbf{k} = \\kappa(t, \\mathbf{t})\\) and, \\(\\mathbf{t}\\) and \\(\\mathbf{y}\\) are the time and measurement vectors from the training data \\(\\mathcal{D}\\). The estimated function model represents dependencies between function values at different inputs through the correlation structure given by the covariance function. Thus, the function values at the observed points give information also of the unobserved points. 13.2 Gaussian Processes for prediction of energy use The first application of Gaussian Processes in building energy modelling is based on the developments of Kennedy and OHagan (Kennedy and OHagan (2001)) which they called Bayesian calibration. Bayesian model calibration refers to using a GP as a surrogate model to reproduce a reference model, then training a second GP as the discrepancy function between this model and observations, then evaluating the posterior distribution of calibration parameters. In this context GPs have static inputs and are not dynamic models. \\[\\begin{equation} z_i = \\zeta(\\mathbf{x}_i) +e_i =\\rho \\, \\eta(\\mathbf{x}_i,\\theta)+\\delta(\\mathbf{x}_i)+e_i \\tag{13.6} \\end{equation}\\] where \\(\\mathbf{x}_i\\) is a series of known model inputs, \\(z_i\\) are observations, \\(\\zeta(\\mathbf{x}_i)\\) is the true value of the real process, \\(\\eta(\\mathbf{x}_i,\\theta)\\) is a computer model output with parameter \\(\\theta\\), \\(\\delta(\\mathbf{x}_i)\\) is the discrepancy function and \\(e_i \\sim N(0,\\lambda)\\) are the observation errors. In Kennedy and OHagans work, GP are used to represent prior information about both \\(\\eta(\\cdot,\\cdot)\\) and \\(\\delta(\\cdot)\\). \\(\\rho\\) and \\(\\lambda\\) are hyperparameters, to be added to the list of hyperparameters of the covariance functions into a global hyperparameter vector \\(\\phi\\). Before attempting prediction of the true phenomenon using the calibrated code, the first step is to derive the posterior distribution of the parameters \\(\\theta\\), \\(\\beta\\) (parameters of the GP mean functions) and \\(\\phi\\). Hyperparameters are estimated in two stages: \\(\\eta(\\cdot,\\cdot)\\) is estimated from a series of code outputs, and \\(\\delta(\\cdot)\\) is estimated from observations. The authors restrict their study to having analytical, tractable posterior distributions that do not require methods such as MCMC. Therefore they fix the value of some hyperparameters to make these functions tractable, and have to resort to some simplifications. The first application of this method to building energy modelling was the work of Heo et al (Heo, Choudhary, and Augenbroe (2012)). They followed the formulation of Bayesian calibration developed by Kennedy and OHagan, and used three sets of data as input: (1) monthly gas consumption values as observations \\(y(x)\\), (2) computer outputs from exploring the space of calibration parameters \\(\\eta(x,\\theta)\\), and (3) the prior PDF of calibration parameters \\(p(\\theta)\\). The model outputs \\(\\eta(x,\\theta)\\) and the bias term \\(\\delta(x)\\) are both modeled as GPs. Calibration parameters are for instance: infiltration rate, indoor temperature, \\(U\\)-values, etc. With very little data, results are posterior PDFs which are very close to the priors. GP learning scales poorly with the amount of data, which restricts its applicability to lower observation time steps. (Kristensen, Choudhary, and Petersen (2017)) studied the influence of time resolution on the predictive accuracy and showed the advantage of higher resolutions. More recently, (Chong et al. (2017)) used the NUTS algorithm for the MCMC sampling in order to accelerate learning. Later, (Chong and Menberg (2018)) gave a summary of publications using Bayesian calibration in building energy. In (Gray and Schmidt (2018)), a hybrid model was implemented. A zero mean GP is trained to learn the error between the grey-box model and the reference data. As in the previous references, both models are added to obtain the final predicted output. They are trained in sequence: the GB model has some inputs \\(\\mathbf{u}_\\mathrm{GB}\\) and is trained first; then the GP has some other inputs \\(\\mathbf{u}_\\mathrm{GP}\\) and is trained on the GB models prediction error. Results are the hyperparameters of the GP. Models trained by this method are said to have very good prediction performance, since the GP predicts the inadequacy of the GB as a function of new inputs, not included in the physical model. However, the method may not be fit for the interpretation of physical parameters. Indeed, since the GB model is first trained independently from the GP, it is biased and its parameter estimates are not interpretable. 13.3 Gaussian Processes for time series data Gaussian process are non-parametric models, which means that the latent function \\(f(t)\\) is represented by an infinite-dimensional parameter space. Unlike parametric methods, the number of parameters is not fixed, but grows with the size of the dataset \\(\\mathcal{D}\\), which is an advantage and a limitation. Non-parametric models are memory-based, which means that they can represent more complex mapping as the data set grows but in order to make predictions they have to remember the full dataset (Frigola (2015)). The computational complexities of the analytical regression equations (13.5) are cubic \\(\\mathcal{O}(N^3)\\) in the number of measurements \\(N\\), which is not suited for long time series. However, for a certain class of covariance function, temporal Gaussian process regression is equivalent to state inference problem which can be solved with Kalman filter and Rauch-Tung-Striebel smoother (Hartikainen and Särkkä (2010)). The computational complexity of these sequential methods is linear \\(\\mathcal{O}(N)\\) instead of cubic in the number of measurements \\(N\\). A stationary Gaussian process (i.e. the covariance function depends only on the time difference \\(\\kappa(t, t^\\prime) = \\kappa(\\tau)\\), with \\(\\tau=\\lvert t - t^\\prime \\rvert\\)) can be exactly represented or well approximated by a stochastic state-space model: \\[\\begin{align} \\mathrm{d}\\mathbf{f} &amp; = \\mathbf{A_{gp}} \\, \\mathbf{f} \\, \\mathrm{d}t + \\mathbf{\\sigma}_{\\mathbf{gp}} \\, \\mathrm{d}\\mathbf{w} \\\\ y_k &amp; = \\mathbf{C}_{\\mathbf{gp}} \\, \\mathbf{f}(t_k) + v_k \\tag{13.7} \\end{align}\\] where the matrices of the system are defined by the choice of covariance function. A list of widely used covariance function with this dual representation is given in (Solin et al. (2016)), (Särkkä and Solin (2019)). As example, consider the Mat'ern covariance function with decay parameter \\(\\nu=3/2\\) \\[\\begin{equation} \\kappa \\left(\\tau\\right) = \\sigma^2 \\, \\left(1 + \\frac{\\sqrt{3}\\tau}{\\ell}\\right) \\, \\exp\\left(-\\frac{\\sqrt{3}\\tau}{\\ell}\\right) \\tag{13.8} \\end{equation}\\] which has the following equivalent state-space representation \\[\\begin{equation} \\mathbf{A_{gp}} = \\begin{pmatrix} 0 &amp; 1 \\\\[0.5em] -\\lambda^2 &amp; -2\\lambda \\end{pmatrix} \\quad \\mathbf{\\sigma}_{\\mathbf{gp}} = \\begin{pmatrix} 0 &amp; 0 \\\\[0.5em] 0 &amp; 2\\lambda^{3/2}\\sigma \\end{pmatrix} \\quad \\mathbf{C}_{\\mathbf{gp}} = \\begin{pmatrix} 1 &amp; 0 \\end{pmatrix} \\tag{13.9} \\end{equation}\\] with \\(\\lambda=\\sqrt{2\\,\\nu} / \\ell\\) and where \\(\\sigma, \\ell &gt; 0\\) are the magnitude and length-scale parameters. The parameter \\(\\ell\\) controls the smoothness (i.e. how much time difference \\(\\tau\\) is required to observe a significant change in the function value) and the parameter \\(\\sigma\\) controls the overall variance of the function (i.e. the expected magnitude of function values). 13.4 Latent Force Models The stochastic part of the state-space model can accommodate for unmodelled disturbances, which do not have a significant influence on the thermal dynamics. This assumption holds if the disturbances have white noise properties and are uncorrelated accross time lags, which is seldom the case in practice (Ghosh et al. (2015)). Usually, the model complexity is increased to erase the structure in the model residuals. However, this strategy may lead to unnecessarily complex models because non-linear dynamics are often modelled by linear approximations. Increasing the model complexity often requires more prior knowledge about the underlying physical systems and additional measurements, which may not be available in practice. Another strategy is to model these unknown disturbances as Gaussian processes with certain parametrized covariance structures (Särkkä, Álvarez, and Lawrence (2018)). The resulting latent force model (Alvarez, Luengo, and Lawrence (2009)) is a combination of parametric grey-box model and non-parametric Gaussian process model. \\[\\begin{align} \\mathrm{d}\\mathbf{x} &amp; = \\left(\\mathbf{A_{rc} \\, \\mathbf{x} + \\mathbf{M_{rc}} \\, \\mathbf{C_{gp}}\\mathbf{f} + \\mathbf{B_{rc}} \\, \\mathbf{u}} \\right) \\, \\mathrm{d}t + \\mathbf{\\sigma}_{\\mathbf{rc}} \\, \\mathrm{d}\\mathbf{w} \\\\ \\mathrm{d}\\mathbf{f} &amp;= \\mathbf{A_{gp}} \\, \\mathbf{f} \\, \\mathrm{d}t + \\mathbf{\\sigma}_{\\mathbf{gp}} \\, \\mathrm{d}\\mathbf{w} \\\\ y_k &amp; = \\mathbf{C}_{\\mathbf{rc}} \\, \\mathbf{x}(t_k) + v_k \\tag{13.10} \\end{align}\\] where \\(\\mathbf{M_{rc}}\\) is the input matrix corresponding to the unknown latent forces. The augmented state-space representation of the latent force model \\[\\begin{align} \\label{lfm_ssm} \\mathrm{d}\\mathbf{z} &amp;= \\mathbf{A} \\, \\mathbf{z} \\, \\mathrm{d}t + \\mathbf{B} \\, \\mathbf{u} \\, \\mathrm{d}t + \\mathbf{\\sigma} \\, \\mathrm{d}\\mathbf{w} \\\\ y_k &amp;= \\mathbf{C} \\, \\mathbf{z}(t_k) + v_k \\tag{13.11} \\end{align}\\] is obtained by combining the grey-box model and the gaussian process model (13.7), such that \\[\\begin{equation} \\begin{alignedat}{3} \\mathbf{z}&amp;=\\begin{pmatrix} \\mathbf{x} \\\\ \\mathbf{f} \\end{pmatrix} \\quad &amp; \\mathbf{A}&amp;=\\begin{pmatrix} \\mathbf{A_{rc}} &amp; \\mathbf{M_{rc}} \\, \\mathbf{C_{gp}} \\\\ \\mathbf{0} &amp; \\mathbf{A_{gp}} \\end{pmatrix} \\quad &amp; \\mathbf{B}=\\begin{pmatrix} \\mathbf{B_{rc}} \\\\ \\mathbf{0} \\end{pmatrix} \\\\ \\mathbf{C}&amp;=\\begin{pmatrix} \\mathbf{C}_{\\mathbf{rc}} &amp; \\mathbf{0} \\end{pmatrix} \\quad &amp; \\mathbf{\\sigma}&amp;=\\begin{pmatrix} \\mathbf{\\sigma}_{\\mathbf{rc}} &amp; \\mathbf{0} \\\\ \\mathbf{0} &amp; \\mathbf{\\sigma}_{\\mathbf{gp}} \\end{pmatrix} \\end{alignedat} \\tag{13.12} \\end{equation}\\] The latent force model representation allows to incorporate prior information about the overall dynamic of the physical system, but also about the behavior of the unknown inputs. References "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
